{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study the dynamics of morphemic processing and understand if/how they relate to the computed geometries from static word-level embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "import lemminflect\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec, \\\n",
    "    prepare_state_trajectory, aggregate_state_trajectory, flatten_trajectory\n",
    "from src.datasets.speech_equivalence import SpeechEquivalenceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "base_model = \"w2v2_8\"\n",
    "model_class = \"discrim-rnn_32-mAP1\"\n",
    "model_name = \"word_broad_10frames_fixedlen25\"\n",
    "train_dataset = \"librispeech-train-clean-100\"\n",
    "model_dir = f\"outputs/models/{train_dataset}/{base_model}/{model_class}/{model_name}\"\n",
    "output_dir = f\".\"\n",
    "dataset_path = f\"outputs/preprocessed_data/{train_dataset}\"\n",
    "equivalence_path = f\"outputs/equivalence_datasets/{train_dataset}/{base_model}/{model_name}/equivalence.pkl\"\n",
    "hidden_states_path = f\"outputs/hidden_states/{train_dataset}/{base_model}/{train_dataset}.h5\"\n",
    "state_space_specs_path = f\"outputs/state_space_specs/{train_dataset}/{base_model}/state_space_specs.h5\"\n",
    "embeddings_path = f\"outputs/model_embeddings/{train_dataset}/{base_model}/{model_class}/{model_name}/{train_dataset}.npy\"\n",
    "\n",
    "seed = 1234\n",
    "\n",
    "max_samples_per_word = 100\n",
    "\n",
    "metric = \"cosine\"\n",
    "\n",
    "agg_fn = (\"mean_within_cut\", \"phoneme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embeddings_path, \"rb\") as f:\n",
    "    model_representations: np.ndarray = np.load(f)\n",
    "state_space_spec = StateSpaceAnalysisSpec.from_hdf5(state_space_specs_path, \"word\")\n",
    "assert state_space_spec.is_compatible_with(model_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space_spec = state_space_spec.subsample_instances(max_samples_per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load difference vectors for comparison\n",
    "difference_vectors = torch.load(f\"{output_dir.replace('analogy_dynamic', 'analogy')}/analogy_difference_vectors.pt\")\n",
    "nns_difference_vectors = np.concatenate([x['difference_vectors'] for x in difference_vectors if \"noun - plural_reg\" in x[\"prefix\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = prepare_state_trajectory(model_representations, state_space_spec, pad=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_agg = aggregate_state_trajectory(trajectory, state_space_spec, agg_fn, keepdims=True)\n",
    "agg_flat, agg_src = flatten_trajectory(traj_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts_df = state_space_spec.cuts.xs(\"phoneme\", level=\"level\").drop(columns=[\"onset_frame_idx\", \"offset_frame_idx\"])\n",
    "cuts_df[\"label_idx\"] = cuts_df.index.get_level_values(\"label\").map({l: i for i, l in enumerate(state_space_spec.labels)})\n",
    "cuts_df[\"frame_idx\"] = cuts_df.groupby([\"label\", \"instance_idx\"]).cumcount()\n",
    "cuts_df = cuts_df.reset_index().set_index([\"label_idx\", \"instance_idx\", \"frame_idx\"]).sort_index()\n",
    "\n",
    "agg_flat_idxs = pd.Series({tuple(agg_src_i): i for i, agg_src_i in enumerate(agg_src)})\n",
    "agg_flat_idxs.index.names = [\"label_idx\", \"instance_idx\", \"frame_idx\"]\n",
    "cuts_df = pd.merge(cuts_df, agg_flat_idxs.rename(\"traj_flat_idx\"), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts_df = cuts_df.set_index(\"label\", append=True).reorder_levels([\"label\", \"label_idx\", \"instance_idx\", \"frame_idx\"]).sort_index()\n",
    "cuts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inflection(word, all_labels, target) -> set[str]:\n",
    "    if target in (\"VBD\", \"VBZ\", \"VBG\", \"NNS\"):\n",
    "        inflections = set(lemminflect.getInflection(word, tag=target, inflect_oov=False))\n",
    "        # don't include zero-derived form\n",
    "        inflections -= {word}\n",
    "    elif target == \"NOT-latin\":\n",
    "        inflections = {\"in\" + word}\n",
    "        if word[0] == \"l\":\n",
    "            inflections |= {\"il\" + word}\n",
    "        elif word[0] in [\"p\", \"b\", \"m\"]:\n",
    "            inflections |= {\"im\" + word}\n",
    "        elif word[0] == \"r\":\n",
    "            inflections |= {\"ir\" + word}\n",
    "\n",
    "        # catch exceptional cases -- these predicted forms are attested, but don't count\n",
    "        # as a negative inflection. e.g. \"come\" -> \"income\"\n",
    "        if word in (\"come comes deed diana dies doors fancy form formation formed forming \"\n",
    "                    \"habit jury justice k l laid land most n part parted port press pressed \"\n",
    "                    \"prove proved pulse pulses side sight stead sure tend tended tending tent \"\n",
    "                    \"to trusted vent ward\"):\n",
    "            inflections = set()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown target: {target}\")\n",
    "    \n",
    "    covered_inflections = inflections & all_labels\n",
    "    return covered_inflections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_targets = [\n",
    "    (\"VBD\", \"verb_inf - Ved\"),\n",
    "    (\"VBZ\", \"verb_inf - 3pSg\"),\n",
    "    (\"VBG\", \"verb_inf - Ving\"),\n",
    "    (\"NNS\", \"noun - plural_reg\"),\n",
    "    (\"NOT-latin\", None),\n",
    "]\n",
    "labels = state_space_spec.label_counts\n",
    "labels = set(labels[labels >= 5].index)\n",
    "\n",
    "inflection_results = {target: {} for target, _ in inflection_targets}\n",
    "inflection_reverse = defaultdict(set)\n",
    "for target, _ in tqdm(inflection_targets):\n",
    "    for label in labels:\n",
    "        label_inflections = get_inflection(label, labels, target)\n",
    "        if label_inflections:\n",
    "            inflection_results[target][label] = label_inflections\n",
    "\n",
    "            for infl in label_inflections:\n",
    "                inflection_reverse[infl].add((label, target))\n",
    "\n",
    "from pprint import pprint\n",
    "pprint({target: len(v) for target, v in inflection_results.items()})\n",
    "\n",
    "ambiguous_inflected_forms = {k: v for k, v in inflection_reverse.items()\n",
    "                             if len(v) > 1}\n",
    "print(f\"Ambiguous inflected forms ({len(ambiguous_inflected_forms)} total):\")\n",
    "print(\" \".join(ambiguous_inflected_forms.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_phonemic_forms = cuts_df.groupby([\"label\", \"instance_idx\"]).description.agg(' '.join)\n",
    "cut_phonemic_forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_regular(inflection, base, inflected):\n",
    "    if inflection == \"NNS\":\n",
    "        return inflected[:len(base)] == base \\\n",
    "                or inflected[-3:] == \"ies\" and base[-1] == \"y\" \\\n",
    "                or inflected[-3:] == \"ves\" and (base[-1] == \"f\" or base[-2:] == \"fe\")\n",
    "    elif inflection == \"VBZ\":\n",
    "        return inflected == base + \"s\" \\\n",
    "                or inflected == base + \"es\" \\\n",
    "                or (base[-1] == \"y\" and inflected == base[:-1] + \"ies\")\n",
    "    elif inflection == \"VBG\":\n",
    "        return inflected == base + \"ing\" \\\n",
    "                or (base[-1] == \"e\" and inflected == base[:-1] + \"ing\") \\\n",
    "                or (base[-1] in \"bcdfghjklmnpqrstvwxz\" and inflected == base + base[-1] + \"ing\") \\\n",
    "                or (base[-2:] == \"ie\" and inflected == base[:-2] + \"ying\")\n",
    "    elif inflection == \"VBD\":\n",
    "        return inflected == base + \"ed\" \\\n",
    "                or inflected == base + \"d\" \\\n",
    "                or inflected == base + \"t\" \\\n",
    "                or (base[-1] == \"y\" and inflected == base[:-1] + \"ied\") \\\n",
    "                or (base[-2:] == \"ay\" and inflected == base[:-1] + \"id\") \\\n",
    "                or (base[-1] in \"bcdfghjklmnpqrstvwxz\" and inflected == base + base[-1] + \"ed\")\n",
    "    elif inflection == \"NOT-latin\":\n",
    "        return inflected[:2] == \"in\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown inflection {inflection}\")\n",
    "\n",
    "\n",
    "def run_inflection_study(study_inflection, study_inflection_difference_vector_prefix,\n",
    "                         smoke_test=False):\n",
    "    study_triples, study_metadata = defaultdict(list), {}\n",
    "    i = 0\n",
    "    for base_form, inflected_forms in tqdm(inflection_results[study_inflection].items()):\n",
    "        if smoke_test and i > 2:\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "        for inflected_form in inflected_forms:\n",
    "            # orthographic divergence point\n",
    "            ortho_divergence_point = sum(1 for idx in range(min(len(base_form), len(inflected_form)))\n",
    "                                        if inflected_form[:idx] == base_form[:idx])\n",
    "            \n",
    "            base_cuts = cuts_df.loc[base_form]\n",
    "            inflected_cuts = cuts_df.loc[inflected_form]\n",
    "\n",
    "            # all attested phonological forms of base\n",
    "            base_phono_forms = set(base_cuts.groupby(\"instance_idx\").apply(\n",
    "                lambda xs: tuple(xs.description)))\n",
    "\n",
    "            for instance_idx, inflected_instance in inflected_cuts.groupby(\"instance_idx\"):\n",
    "                # phonological divergence point: latest point at which the inflected form overlaps with\n",
    "                # any pronunciation of the base form\n",
    "                inflected_phones = tuple(inflected_instance.description)\n",
    "                phono_divergence_points = []\n",
    "                for base_phones in base_phono_forms:\n",
    "                    for idx in range(len(inflected_phones) + 1):\n",
    "                        if inflected_phones[:idx] != base_phones[:idx]:\n",
    "                            break\n",
    "                    phono_divergence_points.append(idx - 1)\n",
    "                phono_divergence_point = max(phono_divergence_points)\n",
    "\n",
    "                # print(f\"{base_phono_forms} -> {inflected_phones} (regular: {is_regular}, ortho_divergence: {ortho_divergence_point}, phono_divergence: {phono_divergence_point})\")\n",
    "\n",
    "                if phono_divergence_point == 0:\n",
    "                    print(f\"Ignoring {base_form} -> {inflected_form} due to phono_divergence_point == 0\")\n",
    "                    continue\n",
    "                pre_diverging_frame = inflected_instance.iloc[phono_divergence_point - 1].traj_flat_idx\n",
    "                diverging_frame = inflected_instance.iloc[phono_divergence_point].traj_flat_idx\n",
    "                final_frame = inflected_instance.iloc[-1].traj_flat_idx\n",
    "                \n",
    "                study_triples[base_form].append((pre_diverging_frame, diverging_frame, final_frame))\n",
    "                if base_form not in study_metadata:\n",
    "                    study_metadata[base_form] = {\n",
    "                        \"label\": base_form,\n",
    "                        \"is_regular\": is_regular(study_inflection, base_form, inflected_form),\n",
    "                        \"post_divergence\": Counter([inflected_phones[phono_divergence_point:]]),\n",
    "                    }\n",
    "                else:\n",
    "                    study_metadata[base_form][\"post_divergence\"].update([inflected_phones[phono_divergence_point:]])\n",
    "\n",
    "    # for each base -> inflected, get average in each of the three states\n",
    "    study_triple_means = []\n",
    "    study_triple_metadata = []\n",
    "    for label, label_triples in tqdm(study_triples.items()):\n",
    "        study_triple_means.append(agg_flat[label_triples].mean(axis=0))\n",
    "\n",
    "        label_metadata = study_metadata[label]\n",
    "        # get most common post-divergence phonological form\n",
    "        label_metadata[\"post_divergence\"] = \" \".join(label_metadata[\"post_divergence\"].most_common(1)[0][0])\n",
    "        study_triple_metadata.append(label_metadata)\n",
    "\n",
    "    if not study_triple_means:\n",
    "        return\n",
    "\n",
    "    study_triple_metadata = pd.DataFrame(study_triple_metadata)\n",
    "\n",
    "    # get num_lemmata * 3 * model_dim representations\n",
    "    inflected_states = np.stack(study_triple_means)\n",
    "\n",
    "    assert len(study_triple_metadata) == len(inflected_states)\n",
    "\n",
    "    #######\n",
    "\n",
    "    inflection_updates = inflected_states[:, 2, :] - inflected_states[:, 0, :]\n",
    "\n",
    "    ########\n",
    "    # fetch control state updates. for each observed divergence point, attempt to find word tokens\n",
    "    # with the same phonemic content, but which words don't correspond to the inflection being studied\n",
    "    control_divergence_lookup = study_triple_metadata.post_divergence.unique()\n",
    "\n",
    "    # regular_inflected_states = inflected_states[study_triple_metadata[study_triple_metadata[\"is_regular\"]].index]\n",
    "    # regular_inflection_updates = regular_inflected_states[:, 2, :] - regular_inflected_states[:, 0, :]\n",
    "\n",
    "    if study_inflection_difference_vector_prefix is None:\n",
    "        reference_difference_vectors = None\n",
    "        counterfactual_difference_vectors = None\n",
    "    else:\n",
    "        reference_difference_vectors = np.stack([x[\"difference_vectors\"].mean(0) for x in difference_vectors\n",
    "                                                 if study_inflection_difference_vector_prefix in x[\"prefix\"]])\n",
    "        counterfactual_difference_vectors = np.stack([x['difference_vectors'].mean(0) for x in difference_vectors\n",
    "                                                      if study_inflection_difference_vector_prefix not in x[\"prefix\"]])\n",
    "\n",
    "    return inflection_updates, \\\n",
    "        reference_difference_vectors, counterfactual_difference_vectors, \\\n",
    "        study_triple_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_results = {}\n",
    "study_results_df = {}\n",
    "infl_updates = {}\n",
    "\n",
    "for target, prefix in tqdm(inflection_targets):\n",
    "    ret = run_inflection_study(target, prefix)\n",
    "    if ret is None:\n",
    "        continue\n",
    "    infl_updates[target], reference_diff_vectors, reference_diff_vectors_counterfactual, metadata = ret\n",
    "\n",
    "    has_irregulars = (~metadata.is_regular).sum() > 0\n",
    "    study_results_df[target] = {\n",
    "        \"within_inflection\": cdist(infl_updates[target], infl_updates[target], metric=metric).mean(),\n",
    "        \"within_inflection_reg-reg\": cdist(infl_updates[target][metadata.is_regular],\n",
    "                                           infl_updates[target][metadata.is_regular], metric=metric).mean(),\n",
    "        \"within_inflection_irreg-irreg\": np.nan if not has_irregulars else \\\n",
    "            cdist(infl_updates[target][~metadata.is_regular],\n",
    "                  infl_updates[target][~metadata.is_regular], metric=metric).mean(),\n",
    "        \"within_inflection_reg-irreg\": np.nan if not has_irregulars else \\\n",
    "            cdist(infl_updates[target][metadata.is_regular],\n",
    "                  infl_updates[target][~metadata.is_regular], metric=metric).mean(),\n",
    "        \"reference_diff\": cdist(infl_updates[target], reference_diff_vectors, metric=metric).mean()\n",
    "                          if reference_diff_vectors is not None else None,\n",
    "        \"reference_diff_counterfactual\": cdist(infl_updates[target], reference_diff_vectors_counterfactual, metric=metric).mean()\n",
    "                                         if reference_diff_vectors_counterfactual is not None else None,\n",
    "    }\n",
    "\n",
    "    study_results[target] = {\n",
    "        \"infl_updates\": infl_updates[target],\n",
    "        \"metadata\": metadata,\n",
    "\n",
    "        \"reference_diff_vectors\": reference_diff_vectors,\n",
    "        \"reference_diff_vectors_counterfactual\": reference_diff_vectors_counterfactual,\n",
    "    }\n",
    "\n",
    "for target in study_results_df:\n",
    "    study_results_df[target][\"counterfactual\"] = cdist(infl_updates[target], np.concatenate([infl_updates[t] for t in study_results if t != target]), metric=metric).mean()\n",
    "    study_results[target][\"vectors_counterfactual\"] = np.concatenate([infl_updates[t] for t in study_results if t != target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have vectors from all inflections computed, estimate between-inflection distances\n",
    "for target in study_results_df:\n",
    "    study_results_df[target][\"between_inflection\"] = cdist(infl_updates[target], np.concatenate([infl_updates[t] for t in study_results if t != target]), metric=metric).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_results_df = pd.DataFrame.from_dict(study_results_df, orient=\"index\")\n",
    "study_results_df.to_csv(f\"{output_dir}/inflection_study.csv\")\n",
    "study_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=study_results_df.reset_index().melt(id_vars=[\"index\"]),\n",
    "            x=\"index\", y=\"value\", hue=\"variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNS sub-study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_controlled_study(target, compare_divergence_categories, smoke_test=False):\n",
    "    updates = study_results[target][\"infl_updates\"]\n",
    "    metadata = study_results[target][\"metadata\"]\n",
    "    # reference_diff_vectors = study_results[target][\"reference_diff_vectors_counterfactual\"]\n",
    "    reference_diff_vectors = study_results[target][\"vectors_counterfactual\"]\n",
    "\n",
    "    assert set(compare_divergence_categories) <= set(metadata.post_divergence)\n",
    "    compare_divergence_categories += [\"<irreg>\", \"<counterfactual>\"]\n",
    "\n",
    "    category_updates = {category: updates[metadata[metadata.post_divergence == category].index]\n",
    "                        for category in compare_divergence_categories}\n",
    "    category_updates[\"<irreg>\"] = updates[~metadata.is_regular]\n",
    "    category_updates[\"<counterfactual>\"] = reference_diff_vectors\n",
    "\n",
    "    distances = {}\n",
    "    distance_averages = np.zeros((len(compare_divergence_categories), len(compare_divergence_categories)))\n",
    "    for cat1, cat1_updates in category_updates.items():\n",
    "        for cat2, cat2_updates in category_updates.items():\n",
    "            distances_ij = cdist(cat1_updates, cat2_updates, metric=metric)\n",
    "            if cat1 == cat2:\n",
    "                distances_ij = distances_ij[np.triu_indices_from(distances_ij, k=1)]\n",
    "            else:\n",
    "                distances_ij = distances_ij[np.triu_indices_from(distances_ij)]\n",
    "\n",
    "            distances[cat1, cat2] = distances_ij\n",
    "            distance_averages[compare_divergence_categories.index(cat1), compare_divergence_categories.index(cat2)] = \\\n",
    "                distances[cat1, cat2].mean()\n",
    "            \n",
    "    distance_df = pd.concat([\n",
    "        pd.Series(distances_i, name=\"distance\").to_frame().assign(**{\"from\": cat1, \"to\": cat2})\n",
    "        for (cat1, cat2), distances_i in distances.items()\n",
    "    ])\n",
    "    distance_df.loc[distance_df[\"from\"] == distance_df[\"to\"], \"type\"] = \"within\"\n",
    "    distance_df.loc[distance_df[\"from\"] != distance_df[\"to\"], \"type\"] = \"between\"\n",
    "    distance_df.loc[distance_df[\"to\"] == \"<irreg>\", \"type\"] = \"irreg\"\n",
    "    distance_df.loc[distance_df[\"to\"] == \"<counterfactual>\", \"type\"] = \"counterfactual\"\n",
    "\n",
    "    # drop irrelevant counterfactual sources\n",
    "    distance_df = distance_df[~(distance_df[\"from\"] == \"<counterfactual>\")]\n",
    "\n",
    "    ####\n",
    "\n",
    "    nrows = 4\n",
    "    f, axs = plt.subplots(nrows, 1, figsize=(10, 5 * nrows))\n",
    "\n",
    "    sns.heatmap(distance_averages, annot=True,\n",
    "                xticklabels=compare_divergence_categories,\n",
    "                yticklabels=compare_divergence_categories,\n",
    "                ax=axs[0])\n",
    "\n",
    "    sns.barplot(data=distance_df, x=\"from\", hue=\"to\", y=\"distance\",\n",
    "                errorbar=None if smoke_test else (\"ci\", 95), ax=axs[1])\n",
    "    \n",
    "    sns.barplot(data=distance_df, x=\"from\", hue=\"type\", y=\"distance\",\n",
    "                errorbar=None if smoke_test else (\"ci\", 95), ax=axs[2])\n",
    "    \n",
    "    sns.barplot(data=distance_df, x=\"type\", y=\"distance\",\n",
    "                errorbar=None if smoke_test else (\"ci\", 95), ax=axs[3])\n",
    "    \n",
    "    f.tight_layout()\n",
    "\n",
    "    return distance_df, distance_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nns_distance_df, _ = run_controlled_study(\"NNS\", [\"S\", \"Z\", \"IH Z\", \"IH N\"])\n",
    "nns_distance_df.to_csv(f\"{output_dir}/nns_distance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbd_distance_df, _ = run_controlled_study(\"VBD\", [\"D\", \"T\", \"IH D\", \"UW\"])\n",
    "vbd_distance_df.to_csv(f\"{output_dir}/vbd_distance.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
