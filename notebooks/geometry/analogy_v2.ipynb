{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import functools\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from fastdist import fastdist\n",
    "import lemminflect\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import transforms\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.analysis import analogy\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec, \\\n",
    "    prepare_state_trajectory, aggregate_state_trajectory, flatten_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"w2v2_8\"\n",
    "\n",
    "model_class = \"discrim-rnn_32-mAP1\"\n",
    "model_name = \"word_broad_10frames_fixedlen25\"\n",
    "\n",
    "# model_class = \"ff_32\"\n",
    "# model_name = \"word_broad_10frames\"\n",
    "\n",
    "train_dataset = \"librispeech-train-clean-100\"\n",
    "model_dir = f\"outputs/models/{train_dataset}/{base_model}/{model_class}/{model_name}\"\n",
    "output_dir = f\".\"\n",
    "dataset_path = f\"outputs/preprocessed_data/{train_dataset}\"\n",
    "equivalence_path = f\"outputs/equivalence_datasets/{train_dataset}/{base_model}/{model_name}/equivalence.pkl\"\n",
    "hidden_states_path = f\"outputs/hidden_states/{train_dataset}/{base_model}/{train_dataset}.h5\"\n",
    "state_space_specs_path = f\"outputs/state_space_specs/{train_dataset}/{base_model}/state_space_specs.h5\"\n",
    "embeddings_path = f\"outputs/model_embeddings/{train_dataset}/{base_model}/{model_class}/{model_name}/{train_dataset}.npy\"\n",
    "\n",
    "pos_counts_path = \"data/pos_counts.pkl\"\n",
    "\n",
    "seed = 1234\n",
    "\n",
    "max_samples_per_word = 100\n",
    "\n",
    "metric = \"cosine\"\n",
    "\n",
    "agg_fns = [\n",
    "    \"mean\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embeddings_path, \"rb\") as f:\n",
    "    model_representations: np.ndarray = np.load(f)\n",
    "state_space_spec = StateSpaceAnalysisSpec.from_hdf5(state_space_specs_path, \"word\")\n",
    "assert state_space_spec.is_compatible_with(model_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_representations /= np.linalg.norm(model_representations, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space_spec = state_space_spec.subsample_instances(max_samples_per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = prepare_state_trajectory(model_representations, state_space_spec, pad=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_aggs = {agg_fn: aggregate_state_trajectory(trajectory, state_space_spec, agg_fn, keepdims=True)\n",
    "                   for agg_fn in tqdm(agg_fns)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_aggs_flat = {k: flatten_trajectory(v) for k, v in trajectory_aggs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg, agg_src = trajectory_aggs_flat[\"mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts_df = state_space_spec.cuts.xs(\"phoneme\", level=\"level\").drop(columns=[\"onset_frame_idx\", \"offset_frame_idx\"])\n",
    "cuts_df[\"label_idx\"] = cuts_df.index.get_level_values(\"label\").map({l: i for i, l in enumerate(state_space_spec.labels)})\n",
    "cuts_df[\"frame_idx\"] = cuts_df.groupby([\"label\", \"instance_idx\"]).cumcount()\n",
    "cuts_df = cuts_df.reset_index().set_index([\"label\", \"instance_idx\", \"frame_idx\"]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_phonemic_forms = cuts_df.groupby([\"label\", \"instance_idx\"]).description.agg(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_df = pd.read_csv(\"data/WorldLex_Eng_US.Freq.2.txt\", sep=\"\\t\", index_col=\"Word\")\n",
    "# compute weighted average frequency across domains\n",
    "word_freq_df[\"BlogFreq_rel\"] = word_freq_df.BlogFreq / word_freq_df.BlogFreq.sum()\n",
    "word_freq_df[\"TwitterFreq_rel\"] = word_freq_df.TwitterFreq / word_freq_df.TwitterFreq.sum()\n",
    "word_freq_df[\"NewsFreq_rel\"] = word_freq_df.NewsFreq / word_freq_df.NewsFreq.sum()\n",
    "word_freq_df[\"Freq\"] = word_freq_df[[\"BlogFreq\", \"TwitterFreq\", \"NewsFreq\"]].mean(axis=1) \\\n",
    "    * word_freq_df[[\"BlogFreq\", \"TwitterFreq\", \"NewsFreq\"]].sum().mean()\n",
    "word_freq_df[\"LogFreq\"] = np.log10(word_freq_df.Freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-compute other auxiliary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token-level features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-divergence analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_instance_df = pd.DataFrame(inflection_instances)\n",
    "\n",
    "# Now merge with type-level information.\n",
    "inflection_instance_df = pd.merge(inflection_instance_df,\n",
    "                                  inflection_results_df.reset_index(),\n",
    "                                  how=\"left\",\n",
    "                                  on=[\"inflection\", \"base\", \"inflected\"])\n",
    "inflection_instance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return most-common-allomorph information to main df\n",
    "# NB this may collapse across different orthographic inflected forms\n",
    "most_common_allomorphs = inflection_instance_df.groupby([\"inflection\", \"base\"]).post_divergence \\\n",
    "    .apply(lambda xs: xs.value_counts().idxmax()) \\\n",
    "    .rename(\"most_common_allomorph\").reset_index()\n",
    "pd.merge(inflection_results_df, most_common_allomorphs,\n",
    "         on=[\"inflection\", \"base\"], validate=\"m:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_cross_instances = []\n",
    "base_cross_instances = []\n",
    "\n",
    "for inflection, row in tqdm(inflection_results_df.iterrows(), total=len(inflection_results_df)):\n",
    "    inflected_flat_idxs = np.nonzero(agg_src[:, 0] == row.inflected_idx)[0]\n",
    "    inflected_forms = cut_phonemic_forms.loc[row.inflected]\n",
    "    for inflected_flat_idx in inflected_flat_idxs:\n",
    "        inflected_instance_idx = agg_src[inflected_flat_idx, 1]\n",
    "        inflection_cross_instances.append({\n",
    "            \"inflection\": inflection,\n",
    "            \"base\": row.base,\n",
    "            \"inflected\": row.inflected,\n",
    "            \"inflected_instance_idx\": inflected_instance_idx,\n",
    "            \"inflected_phones\": inflected_forms.loc[inflected_instance_idx]\n",
    "        })\n",
    "\n",
    "    base_flat_idxs = np.nonzero(agg_src[:, 0] == row.base_idx)[0]\n",
    "    base_forms = cut_phonemic_forms.loc[row.base]\n",
    "    for base_flat_idx in base_flat_idxs:\n",
    "        base_instance_idx = agg_src[base_flat_idx, 1]\n",
    "        base_cross_instances.append({\n",
    "            \"inflection\": inflection,\n",
    "            \"base\": row.base,\n",
    "            \"inflected\": row.inflected,\n",
    "            \"base_instance_idx\": base_instance_idx,\n",
    "            \"base_phones\": base_forms.loc[base_instance_idx]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in post-divergence information\n",
    "inflection_cross_instances_df = pd.DataFrame(inflection_cross_instances)\n",
    "merge_on = [\"inflection\", \"base\", \"inflected\", \"inflected_instance_idx\"]\n",
    "inflection_cross_instances_df = pd.merge(inflection_cross_instances_df,\n",
    "                                         inflection_instance_df[merge_on + [\"post_divergence\"]],\n",
    "                                         on=merge_on)\n",
    "\n",
    "all_cross_instances = pd.merge(pd.DataFrame(base_cross_instances),\n",
    "         inflection_cross_instances_df,\n",
    "         on=[\"inflection\", \"base\", \"inflected\"],\n",
    "         how=\"outer\")\n",
    "\n",
    "# Now merge with type-level information.\n",
    "all_cross_instances = pd.merge(inflection_results_df.reset_index(),\n",
    "                               all_cross_instances,\n",
    "                               on=[\"inflection\", \"base\", \"inflected\"],\n",
    "                               validate=\"1:m\")\n",
    "\n",
    "all_cross_instances[\"exclude_main\"] = False\n",
    "all_cross_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add false friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_false_friends():\n",
    "    false_friends_dfs = {}\n",
    "    inflection_allomorph_grouper = most_common_allomorphs \\\n",
    "        [~most_common_allomorphs.inflection.isin((\"random\", \"NOT-latin\"))] \\\n",
    "        .groupby(\"inflection\").most_common_allomorph \\\n",
    "        .apply(lambda xs: xs.value_counts()[:3]).index\n",
    "    for inflection, post_divergence in tqdm(inflection_allomorph_grouper):\n",
    "        avoid_inflections = {\"POS\", inflection}\n",
    "        if inflection == \"NNS\":\n",
    "            avoid_inflections.add(\"VBZ\")\n",
    "        elif inflection == \"VBZ\":\n",
    "            avoid_inflections.add(\"NNS\")\n",
    "        avoid_inflections = list(avoid_inflections)\n",
    "\n",
    "        try:\n",
    "            false_friends_dfs[inflection, post_divergence] = \\\n",
    "                analogy.prepare_false_friends(\n",
    "                    inflection_results_df,\n",
    "                    inflection_instance_df,\n",
    "                    cut_phonemic_forms,\n",
    "                    post_divergence,\n",
    "                    avoid_inflections=avoid_inflections)\n",
    "        except:\n",
    "            print(\"Failed for\", inflection, post_divergence)\n",
    "            continue\n",
    "\n",
    "    return false_friends_dfs\n",
    "\n",
    "false_friends_cache_path = Path(output_dir) / \"false_friends.pt\"\n",
    "if false_friends_cache_path.exists():\n",
    "    false_friends_dfs = torch.load(false_friends_cache_path)\n",
    "else:\n",
    "    false_friends_dfs = compute_false_friends()\n",
    "    torch.save(false_friends_dfs, false_friends_cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_friends_df = pd.concat(false_friends_dfs, names=[\"inflection\", \"post_divergence\"]).droplevel(-1)\n",
    "\n",
    "# manually exclude some cases that don't get filtered out, often just because they're too\n",
    "# low frequency for both true base and inflected form to appear\n",
    "\n",
    "# share exclusion list for NNS and VBZ since we have experiments relating these two\n",
    "# so this is any false-friend for which their is a phonologically identical \"base\"\n",
    "# that could instantiate a VBZ or NNS inflection\n",
    "exclude_NNS_VBZ = (\"adds americans arabs assyrians berries carlyle's childs christians \"\n",
    "                   \"counties cruise dares dealings delawares europeans excellencies \"\n",
    "                   \"fins fours galleries gaze germans indians isles maids mary's negroes \"\n",
    "                   \"nuns peas phrase pyes reflections rodgers romans russians simpsons \"\n",
    "                   \"spaniards sundays vickers weeds wigwams williams \"\n",
    "                   \"jews odds news hose dis yes ice cease peace s us \"\n",
    "                    \n",
    "                   \"greeks lapse mix philips trunks its \"\n",
    "                    \n",
    "                   \"breeches occurrences personages\").split()\n",
    "false_friends_manual_exclude = {\n",
    "    \"NNS\": exclude_NNS_VBZ,\n",
    "    \"VBZ\": exclude_NNS_VBZ,\n",
    "    \"VBD\": (\"armored bald bard counseled crude dared enquired healed knowed legged \"\n",
    "            \"mourned natured renowned rude second ward wild willed withered \"\n",
    "\n",
    "            \"tract wrapped fitted hearted heralded intrusted knitted wretched\").split(),\n",
    "    \"VBG\": (\"ceiling daring fleeting morning roaming wasting weaving weighing \"\n",
    "            \"whining willing chuckling kneeling sparkling startling\").split()\n",
    "}\n",
    "\n",
    "false_friends_df = false_friends_df.groupby(\"inflection\", as_index=False).apply(\n",
    "    lambda xs: xs[~xs.inflected.isin(false_friends_manual_exclude.get(xs.name, []))]).droplevel(0)\n",
    "\n",
    "# exclude the (quite interesting) cases where the \"base\" and \"inflected\" form are\n",
    "# actually orthographically matched, and we're seeing the divergence due to a pronunciation\n",
    "# variant (e.g. don't as D OW N vs D O WN T)\n",
    "false_friends_df = false_friends_df[false_friends_df.base != false_friends_df.inflected]\n",
    "\n",
    "false_friends_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sets or lists for final-phoneme checks\n",
    "SIBILANTS = {\"S\", \"Z\", \"SH\", \"CH\", \"JH\", \"ZH\"}\n",
    "VOICELESS = {\"P\", \"T\", \"K\", \"F\", \"TH\"}  # Could add others as needed\n",
    "\n",
    "def guess_nns_vbz_allomorph(base_phones):\n",
    "    \"\"\"\n",
    "    Given a list of CMUDICT phones for a base form, \n",
    "    return the 'expected' post-divergence allomorph \n",
    "    (S, Z, or IH Z, etc.) for the English plural / 3sg verb.\n",
    "    \"\"\"\n",
    "    last_phone = base_phones[-1]\n",
    "    \n",
    "    if last_phone in SIBILANTS:\n",
    "        # e.g., 'CH' -> \"IH Z\"\n",
    "        return \"IH Z\"\n",
    "    elif last_phone in VOICELESS:\n",
    "        # e.g., 'K', 'P', 'T' -> \"S\"\n",
    "        return \"S\"\n",
    "    else:\n",
    "        # default to voiced => \"Z\"\n",
    "        return \"Z\"\n",
    "    \n",
    "\n",
    "ALVEOLAR_STOPS = {\"T\", \"D\"}\n",
    "# Example set of voiceless consonants (non-exhaustive—adjust as needed).\n",
    "VOICELESS = {\"P\", \"F\", \"K\", \"S\", \"SH\", \"CH\", \"TH\"}  # Typically would also have /ʃ/, etc.\n",
    "\n",
    "def guess_past_allomorph(base_phones):\n",
    "    \"\"\"\n",
    "    Given a list of CMUDICT phones for a base form,\n",
    "    return the 'expected' post-divergence allomorph\n",
    "    (T, D, or IH D) for the English past tense.\n",
    "    \"\"\"\n",
    "    last_phone = base_phones[-1]\n",
    "    \n",
    "    if last_phone in ALVEOLAR_STOPS:\n",
    "        # E.g., \"want\" -> \"wanted\" => \"AH0 D\"\n",
    "        return \"IH D\"\n",
    "    elif last_phone in VOICELESS:\n",
    "        # E.g., \"jump\" -> \"jumped\" => \"T\"\n",
    "        return \"T\"\n",
    "    else:\n",
    "        # default to voiced => \"D\"\n",
    "        return \"D\"\n",
    "\n",
    "\n",
    "false_friends_df.loc[[\"NNS\", \"VBZ\"], \"strong_expected\"] = false_friends_df.loc[[\"NNS\", \"VBZ\"]].apply(lambda xs: guess_nns_vbz_allomorph(xs.base_form.split(\" \")), axis=1)\n",
    "false_friends_df.loc[[\"VBD\"], \"strong_expected\"] = false_friends_df.loc[[\"VBD\"]].apply(lambda xs: guess_past_allomorph(xs.base_form.split(\" \")), axis=1)\n",
    "false_friends_df[\"strong\"] = false_friends_df.index.get_level_values(\"post_divergence\") == false_friends_df.strong_expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_friends_df.groupby([\"inflection\", \"post_divergence\", \"strong\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_friends_df.query(\"inflection == 'VBZ' and post_divergence == 'S'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_friends_df.groupby([\"inflection\", \"strong\", \"strong_expected\"]).sample(frac=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_false_friends_df = pd.merge(false_friends_df.reset_index(),\n",
    "         cut_phonemic_forms.reset_index().rename(\n",
    "             columns={\"label\": \"base\", \"description\": \"base_form\",\n",
    "                      \"instance_idx\": \"base_instance_idx\"}),\n",
    "         on=[\"base\", \"base_form\"], how=\"left\")\n",
    "cross_false_friends_df = pd.merge(cross_false_friends_df,\n",
    "         cut_phonemic_forms.reset_index().rename(\n",
    "             columns={\"label\": \"inflected\", \"description\": \"inflected_form\",\n",
    "                      \"instance_idx\": \"inflected_instance_idx\"}),\n",
    "         on=[\"inflected\", \"inflected_form\"], how=\"left\")\n",
    "\n",
    "# update to match all_cross_instances schema\n",
    "cross_false_friends_df = cross_false_friends_df.rename(\n",
    "    columns={\"base_form\": \"base_phones\",\n",
    "             \"inflected_form\": \"inflected_phones\"})\n",
    "cross_false_friends_df[\"base_idx\"] = cross_false_friends_df.base.map({l: i for i, l in enumerate(state_space_spec.labels)})\n",
    "cross_false_friends_df[\"inflected_idx\"] = cross_false_friends_df.inflected.map({l: i for i, l in enumerate(state_space_spec.labels)})\n",
    "cross_false_friends_df[\"is_regular\"] = True\n",
    "\n",
    "cross_false_friends_df[\"inflection\"] = (cross_false_friends_df.inflection + \"-FF-\").str.cat(cross_false_friends_df.post_divergence, sep=\"\")\n",
    "cross_false_friends_df[\"exclude_main\"] = True\n",
    "cross_false_friends_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cross_instances = pd.concat([all_cross_instances, cross_false_friends_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavioral tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general queries for all experiments to exclude special edge cases;\n",
    "# logic doesn't make sense in most experiments\n",
    "all_query = \"not exclude_main\"\n",
    "\n",
    "experiments = {\n",
    "    \"basic\": {\n",
    "        \"group_by\": [\"inflection\"],\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    \"regular\": {\n",
    "        \"group_by\": [\"inflection\", \"is_regular\"],\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    # \"NNS_to_VBZ\": {\n",
    "    #     \"base_query\": \"inflection == 'NNS' and is_regular\",\n",
    "    #     \"inflected_query\": \"inflection == 'VBZ' and is_regular\",\n",
    "    # },\n",
    "    # \"VBZ_to_NNS\": {\n",
    "    #     \"base_query\": \"inflection == 'VBZ' and is_regular\",\n",
    "    #     \"inflected_query\": \"inflection == 'NNS' and is_regular\",\n",
    "    # },\n",
    "    \"regular_to_irregular\": {\n",
    "        \"group_by\": [\"inflection\"],\n",
    "        \"base_query\": \"is_regular\",\n",
    "        \"inflected_query\": \"not is_regular\",\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    \"irregular_to_regular\": {\n",
    "        \"group_by\": [\"inflection\"],\n",
    "        \"base_query\": \"not is_regular\",\n",
    "        \"inflected_query\": \"is_regular\",\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    \"nn_vb_ambiguous\": {\n",
    "        \"group_by\": [\"inflection\", \"base_ambig_NN_VB\"],\n",
    "        \"base_query\": \"is_regular\",\n",
    "        \"inflected_query\": \"is_regular\",\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    \"random_to_NNS\": {\n",
    "        \"base_query\": \"inflection == 'random'\",\n",
    "        \"inflected_query\": \"inflection == 'NNS'\",\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    \"random_to_VBZ\": {\n",
    "        \"base_query\": \"inflection == 'random'\",\n",
    "        \"inflected_query\": \"inflection == 'VBZ'\",\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    \"false_friends\": {\n",
    "        \"all_query\": \"inflection.str.contains('FF')\",\n",
    "        \"group_by\": [\"inflection\"],\n",
    "        \"equivalence_keys\": [\"base\", \"inflected\", \"post_divergence\"],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate experiments testing transfer from each of top allomorphs in NNS, VBZ\n",
    "# to each other\n",
    "transfer_allomorphs = most_common_allomorphs.groupby(\"inflection\").most_common_allomorph.apply(lambda xs: xs.value_counts().head(3).index.tolist()).to_dict()\n",
    "study_unambiguous_transfer = [\"NNS\", \"VBZ\"]\n",
    "for infl1, infl2 in itertools.product(study_unambiguous_transfer, repeat=2):\n",
    "    for allomorph1 in transfer_allomorphs[infl1]:\n",
    "        for allomorph2 in transfer_allomorphs[infl2]:\n",
    "            experiments[f\"unambiguous-{infl1}_{allomorph1}_to_{infl2}_{allomorph2}\"] = {\n",
    "                \"base_query\": f\"inflection == '{infl1}' and is_regular and base_ambig_NN_VB == False and post_divergence == '{allomorph1}'\",\n",
    "                \"inflected_query\": f\"inflection == '{infl2}' and is_regular and base_ambig_NN_VB == False and post_divergence == '{allomorph2}'\",\n",
    "                \"all_query\": all_query,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate experiments testing transfer from\n",
    "# 1. false friend allomorph to matching inflection allomorph\n",
    "# 2. false friend allomorph to non-matching inflection allomorph\n",
    "# 3. inflection allomorph to matching false friend allomorph\n",
    "# 4. inflection allomorph to non-matching false friend allomorph\n",
    "transfer_allomorphs = most_common_allomorphs.groupby(\"inflection\").most_common_allomorph.apply(lambda xs: xs.value_counts().head(3).index.tolist()).to_dict()\n",
    "study_false_friends = [\"NNS\", \"VBZ\"]\n",
    "for (inflection, post_divergence), _ in false_friends_df.groupby([\"inflection\", \"post_divergence\"]):\n",
    "    if inflection not in study_false_friends:\n",
    "        continue\n",
    "    for transfer_allomorph in transfer_allomorphs[inflection]:\n",
    "        experiments[f\"{inflection}-FF-{post_divergence}-to-{inflection}_{transfer_allomorph}\"] = {\n",
    "            \"base_query\": f\"inflection == '{inflection}-FF-{post_divergence}'\",\n",
    "            \"inflected_query\": f\"inflection == '{inflection}' and is_regular and base_ambig_NN_VB == False and post_divergence == '{transfer_allomorph}'\",\n",
    "        }\n",
    "        experiments[f\"{inflection}_{transfer_allomorph}-to-{inflection}-FF-{post_divergence}\"] = {\n",
    "            \"base_query\": f\"inflection == '{inflection}' and is_regular and base_ambig_NN_VB == False and post_divergence == '{transfer_allomorph}'\",\n",
    "            \"inflected_query\": f\"inflection == '{inflection}-FF-{post_divergence}'\",\n",
    "        }\n",
    "\n",
    "for inflection in study_false_friends:\n",
    "    for t1, t2 in itertools.combinations(transfer_allomorphs[inflection], 2):\n",
    "        experiments[f\"{inflection}-FF-{t1}-to-{inflection}-FF-{t2}\"] = {\n",
    "            \"base_query\": f\"inflection == '{inflection}-FF-{t1}'\",\n",
    "            \"inflected_query\": f\"inflection == '{inflection}-FF-{t2}'\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = pd.concat({\n",
    "    experiment: analogy.run_experiment_equiv_level(\n",
    "        experiment, config,\n",
    "        state_space_spec, all_cross_instances,\n",
    "        agg, agg_src,\n",
    "        num_samples=1000,\n",
    "        seed=42,\n",
    "        device=\"cuda:2\")\n",
    "    for experiment, config in tqdm(experiments.items(), unit=\"experiment\")\n",
    "}, names=[\"experiment\"])\n",
    "experiment_results[\"correct\"] = experiment_results.predicted_label == experiment_results.gt_label\n",
    "experiment_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"inflection_results_df\": inflection_results_df,\n",
    "    \"inflection_instance_df\": inflection_instance_df,\n",
    "    \"all_cross_instances\": all_cross_instances,\n",
    "    \"experiment_results\": experiment_results,\n",
    "}, f\"{output_dir}/analogy_data_20250210.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = torch.load(f\"{output_dir}/analogy_data_20250210.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_results_df = ser[\"inflection_results_df\"]\n",
    "inflection_instance_df = ser[\"inflection_instance_df\"]\n",
    "all_cross_instances = ser[\"all_cross_instances\"]\n",
    "experiment_results = ser[\"experiment_results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results.groupby([\"experiment\", \"group\"]).correct.mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results.groupby([\"experiment\", \"group\"]).gt_label_rank.mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results.groupby([\"experiment\", \"group\"]).gt_distance.mean().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_results_df.groupby([\"inflection\", \"is_regular\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regular_inflections = inflection_results_df.groupby([\"inflection\", \"is_regular\"]).size().unstack().fillna(0)\n",
    "plot_regular_inflections = plot_regular_inflections[plot_regular_inflections.min(1) > 0]\n",
    "plot_regular_inflections = sorted(plot_regular_inflections.index) + [\"VBG\", \"random\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, len(plot_regular_inflections), figsize=(3 * len(plot_regular_inflections), 2.5),\n",
    "                     squeeze=True)\n",
    "\n",
    "regular_df = experiment_results.loc[\"regular\"]\n",
    "regular_df = pd.concat([regular_df, pd.DataFrame(regular_df.group.tolist()).add_prefix(\"group\")], axis=1)\n",
    "regular_transfer_df = experiment_results.loc[\"regular_to_irregular\"]\n",
    "regular_transfer_df[\"group\"] = regular_transfer_df.group.str[0]\n",
    "irregular_transfer_df = experiment_results.loc[\"irregular_to_regular\"]\n",
    "irregular_transfer_df[\"group\"] = irregular_transfer_df.group.str[0]\n",
    "regular_results = {}\n",
    "for inflection in plot_regular_inflections:\n",
    "    regular_results[inflection] = np.array([\n",
    "        [regular_df.query(\"group0 == @inflection and group1 == True\").correct.mean(),\n",
    "         regular_transfer_df.query(\"group == @inflection\").correct.mean()],\n",
    "        [irregular_transfer_df.query(\"group == @inflection\").correct.mean(),\n",
    "         regular_df.query(\"group0 == @inflection and group1 == False\").correct.mean()],\n",
    "    ])\n",
    "\n",
    "vmin = min(v.min() for v in regular_results.values())\n",
    "vmax = max(v.max() for v in regular_results.values())\n",
    "for ax, inflection in zip(ax, plot_regular_inflections):\n",
    "    sns.heatmap(regular_results[inflection], annot=True, fmt=\".2f\", ax=ax,\n",
    "                vmin=vmin, vmax=vmax, cbar=True,\n",
    "                xticklabels=[\"Regular\", \"Irregular\"],\n",
    "                yticklabels=[\"Regular\", \"Irregular\"])\n",
    "    ax.set_title(inflection)\n",
    "    ax.set_xlabel(\"Test\")\n",
    "    ax.set_ylabel(\"Train\")\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Root NN/VB ambiguity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_results_df.groupby([\"inflection\", \"base_ambig_NN_VB\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_results_df.groupby([\"inflection\", \"base_ambig_NN_VB\"]).sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_instance_df.groupby([\"inflection\", \"base_ambig_NN_VB\", \"post_divergence\"]).size().sort_values(ascending=False).head(10).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_instance_df.query(\"base_ambig_NN_VB == False\").groupby([\"inflection\", \"base\"]).head(1).groupby([\"inflection\", \"post_divergence\"]).size() \\\n",
    "    .loc[[(\"NNS\", \"S\"), (\"NNS\", \"Z\"), (\"NNS\", \"IH Z\"), (\"VBZ\", \"Z\"), (\"VBZ\", \"S\"), (\"VBZ\", \"IH Z\")]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_nnvb_results = []\n",
    "\n",
    "nnvb_expts = experiment_results.index.get_level_values(\"experiment\").unique()\n",
    "nnvb_expts = nnvb_expts[nnvb_expts.str.contains(\"unambiguous-\")]\n",
    "\n",
    "for expt in nnvb_expts:\n",
    "    inflection_from, allomorph_from, inflection_to, allomorph_to = \\\n",
    "        re.findall(r\"unambiguous-(\\w+)_([\\w\\s]+)_to_(\\w+)_([\\w\\s]+)\", expt)[0]\n",
    "    expt_df = experiment_results.loc[expt].copy()\n",
    "\n",
    "    num_seen_words = min(len(expt_df.base_from.unique()), len(expt_df.base_to.unique()))\n",
    "    if num_seen_words < 10:\n",
    "        print(f\"Skipping {expt} due to only {num_seen_words} seen words\")\n",
    "        continue\n",
    "\n",
    "    expt_df[\"inflection_from\"] = inflection_from\n",
    "    expt_df[\"allomorph_from\"] = allomorph_from\n",
    "    expt_df[\"inflection_to\"] = inflection_to\n",
    "    expt_df[\"allomorph_to\"] = allomorph_to\n",
    "\n",
    "    agg_nnvb_results.append(expt_df)\n",
    "\n",
    "all_nnvb_results = pd.concat(agg_nnvb_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnvb_results_summary = all_nnvb_results.groupby([\"inflection_from\", \"inflection_to\",\n",
    "                                                 \"allomorph_from\", \"allomorph_to\"]) \\\n",
    "    .correct.agg([\"count\", \"mean\"]) \\\n",
    "    .query(\"count >= 0\") \\\n",
    "    .reset_index()\n",
    "\n",
    "nnvb_results_summary[\"source_label\"] = nnvb_results_summary.inflection_from + \" \" + nnvb_results_summary.allomorph_from\n",
    "nnvb_results_summary[\"target_label\"] = nnvb_results_summary.inflection_to + \" \" + nnvb_results_summary.allomorph_to\n",
    "\n",
    "nnvb_results_summary[\"transfer_label\"] = nnvb_results_summary.inflection_from + \" -> \" + nnvb_results_summary.inflection_to\n",
    "nnvb_results_summary[\"phon_label\"] = nnvb_results_summary.allomorph_from + \" \" + nnvb_results_summary.allomorph_to\n",
    "\n",
    "# only retain cases where we have data in both transfer directions from source <-> target within inflection\n",
    "nnvb_results_summary[\"complement_exists\"] = nnvb_results_summary.apply(lambda row: len(nnvb_results_summary.query(\"source_label == @row.target_label and target_label == @row.source_label\")), axis=1)\n",
    "nnvb_results_summary = nnvb_results_summary.query(\"complement_exists > 0\").drop(columns=[\"complement_exists\"])\n",
    "\n",
    "nnvb_results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(nnvb_results_summary.set_index([\"source_label\", \"target_label\"]).sort_index()[\"mean\"].unstack(\"target_label\"), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.catplot(data=nnvb_results_summary, x=\"transfer_label\", y=\"mean\", hue=\"phon_label\", kind=\"swarm\", aspect=2)\n",
    "order = nnvb_results_summary.groupby(\"transfer_label\")[\"mean\"].mean().sort_values(ascending=False).index\n",
    "sns.catplot(data=nnvb_results_summary, x=\"transfer_label\", y=\"mean\", kind=\"box\", order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_nnvb_results[\"transfer_label\"] = all_nnvb_results.inflection_from + \" -> \" + all_nnvb_results.inflection_to\n",
    "# plot_df = all_nnvb_results.groupby([\"transfer_label\", \"base_to\"]).correct.mean().reset_index()\n",
    "# order = plot_df.groupby(\"transfer_label\")[\"correct\"].mean().sort_values(ascending=False).index\n",
    "# sns.catplot(data=plot_df, x=\"transfer_label\", y=\"correct\", kind=\"box\", aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnvb_results_summary2 = all_nnvb_results.groupby([\"inflection_from\", \"inflection_to\"]) \\\n",
    "    [[\"correct\", \"gt_label_rank\", \"gt_distance\"]].mean().reset_index()\n",
    "\n",
    "nnvb_results_summary2[\"transfer_label\"] = nnvb_results_summary2.inflection_from + \" -> \" + nnvb_results_summary2.inflection_to\n",
    "nnvb_results_summary2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(nnvb_results_summary2.set_index([\"inflection_from\", \"inflection_to\"]).correct.unstack(\"inflection_to\"),\n",
    "            annot=True)\n",
    "ax.set_xlabel(\"Test\")\n",
    "ax.set_ylabel(\"Train\")\n",
    "ax.set_title(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(nnvb_results_summary2.set_index([\"inflection_from\", \"inflection_to\"]).gt_label_rank.unstack(\"inflection_to\"),\n",
    "            annot=True)\n",
    "ax.set_xlabel(\"Test\")\n",
    "ax.set_ylabel(\"Train\")\n",
    "ax.set_title(\"Mean rank of GT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(nnvb_results_summary2.set_index([\"inflection_from\", \"inflection_to\"]).gt_distance.unstack(\"inflection_to\"),\n",
    "            annot=True)\n",
    "ax.set_xlabel(\"Test\")\n",
    "ax.set_ylabel(\"Train\")\n",
    "ax.set_title(\"Median distance to GT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### false friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_friend_expts = experiment_results.index.get_level_values(\"experiment\").unique()\n",
    "false_friend_expts = false_friend_expts[false_friend_expts.str.contains(\"FF\")]\n",
    "# false_friend_expts = false_friend_expts.tolist() + [\"false_friends\"]\n",
    "sorted(false_friend_expts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ff_results = []\n",
    "\n",
    "for expt_name in false_friend_expts:\n",
    "    expt_df = experiment_results.loc[expt_name].copy()\n",
    "    num_seen_words = min(len(expt_df.base_from.unique()), len(expt_df.base_to.unique()))\n",
    "    if num_seen_words < 10:\n",
    "        print(f\"Skipping {expt} due to only {num_seen_words} seen words\")\n",
    "        continue\n",
    "\n",
    "    if expt_name.count(\"-FF-\") == 2:\n",
    "        allomorph_from, allomorph_to = re.findall(r\"-FF-([\\w\\s]+)-to-.+FF-([\\w\\s]+)\", expt_name)[0]\n",
    "        ff_from, ff_to = True, True\n",
    "    else:\n",
    "        try:\n",
    "            allomorph_from, allomorph_to = re.findall(r\"_([\\w\\s]+)-to-.+FF-([\\w\\s]+)\", expt_name)[0]\n",
    "            # is the false friend on the \"from\" side?\n",
    "            ff_from, ff_to = False, True\n",
    "        except:\n",
    "            allomorph_from, allomorph_to = re.findall(r\".+FF-([\\w\\s]+)-to-.+_([\\w\\s]+)\", expt_name)[0]\n",
    "            ff_from, ff_to = True, False\n",
    "\n",
    "    expt_df[\"allomorph_from\"] = allomorph_from\n",
    "    expt_df[\"allomorph_to\"] = allomorph_to\n",
    "\n",
    "    if ff_from:\n",
    "        expt_df[\"inflection_from\"] = expt_df.inflection_from.str.replace(\"-FF-.+\", \"-FF\", regex=True)\n",
    "    if ff_to:\n",
    "        expt_df[\"inflection_to\"] = expt_df.inflection_to.str.replace(\"-FF-.+\", \"-FF\", regex=True)\n",
    "\n",
    "    all_ff_results.append(expt_df)\n",
    "\n",
    "all_ff_results = pd.concat(all_ff_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_df = experiment_results.loc[\"false_friends\"].copy()\n",
    "expt_df[\"allomorph_from\"] = expt_df.inflection_from.str.extract(r\"FF-(.+)$\")\n",
    "expt_df[\"allomorph_to\"] = expt_df.inflection_to.str.extract(r\"FF-(.+)$\")\n",
    "expt_df[\"inflection_from\"] = expt_df.inflection_from.str.replace(\"-FF-.+\", \"-FF\", regex=True)\n",
    "expt_df[\"inflection_to\"] = expt_df.inflection_to.str.replace(\"-FF-.+\", \"-FF\", regex=True)\n",
    "\n",
    "expt_df = expt_df[expt_df.inflection_from.isin(all_ff_results.inflection_from.unique())]\n",
    "\n",
    "all_ff_results = pd.concat([all_ff_results, expt_df])\n",
    "\n",
    "all_ff_results = pd.merge(all_ff_results, word_freq_df[\"LogFreq\"].rename(\"from_freq\"), left_on=\"base_from\", right_index=True)\n",
    "all_ff_results = pd.merge(all_ff_results, word_freq_df[\"LogFreq\"].rename(\"to_freq\"), left_on=\"base_to\", right_index=True)\n",
    "\n",
    "all_ff_results[\"from_freq_bin\"] = pd.qcut(all_ff_results.from_freq, 5, labels=[f\"Q{i}\" for i in range(1, 6)])\n",
    "all_ff_results[\"to_freq_bin\"] = pd.qcut(all_ff_results.to_freq, 5, labels=[f\"Q{i}\" for i in range(1, 6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ff_results.loc[all_ff_results.inflection_from.str.contains(\"-FF\"), \"is_strong_ff\"] = \\\n",
    "    pd.merge(all_ff_results.loc[all_ff_results.inflection_from.str.contains(\"-FF\")].assign(inflection_from_base=lambda xs: xs.inflection_from.str.replace(\"-FF\", \"\")),\n",
    "         false_friends_df.reset_index()[[\"inflection\", \"base\", \"strong_expected\", \"strong\"]]\n",
    "            .rename(columns={\n",
    "                \"inflection\": \"inflection_from_base\",\n",
    "                \"base\": \"base_from\",\n",
    "                \"strong_expected\": \"strong_ff_allomorph\",\n",
    "                \"strong\": \"is_strong_ff\"}),\n",
    "         on=[\"inflection_from_base\", \"base_from\"]).is_strong_ff\n",
    "\n",
    "all_ff_results.loc[all_ff_results.inflection_to.str.contains(\"-FF\"), \"is_strong_ff\"] = \\\n",
    "    pd.merge(all_ff_results.loc[all_ff_results.inflection_to.str.contains(\"-FF\")]\n",
    "                .assign(inflection_to_base=lambda xs: xs.inflection_to.str.replace(\"-FF\", \"\"))\n",
    "                .drop(columns=[\"is_strong_ff\"]),\n",
    "            false_friends_df.reset_index()[[\"inflection\", \"base\", \"strong_expected\", \"strong\"]]\n",
    "                .rename(columns={\n",
    "                    \"inflection\": \"inflection_to_base\",\n",
    "                    \"base\": \"base_to\",\n",
    "                    \"strong_expected\": \"strong_ff_allomorph\",\n",
    "                    \"strong\": \"is_strong_ff\"}),\n",
    "            on=[\"inflection_to_base\", \"base_to\"]).is_strong_ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY STRONG\n",
    "all_ff_results = all_ff_results[all_ff_results.is_strong_ff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_friends_df.groupby([\"inflection\", \"post_divergence\", \"strong\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_k(rows, k):\n",
    "#     return rows.sample(k, replace=True) if len(rows) > k else rows\n",
    "# false_friends_df.query(\"inflection == 'NNS'\").groupby(\"post_divergence\").apply(sample_k, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ff_results.groupby([\"inflection_from\", \"inflection_to\"]).apply(lambda xs: xs[[\"base_from\", \"base_to\", \"allomorph_from\", \"allomorph_to\"]].sample(3)).droplevel(-1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_results_summary = all_ff_results.groupby([\"inflection_from\", \"inflection_to\",\n",
    "                                             \"allomorph_from\", \"allomorph_to\"]) \\\n",
    "        .correct.agg([\"count\", \"mean\"]) \\\n",
    "        .query(\"count >= 0\") \\\n",
    "        .reset_index()\n",
    "\n",
    "ff_results_summary[\"source_label\"] = ff_results_summary.inflection_from + \" \" + ff_results_summary.allomorph_from\n",
    "ff_results_summary[\"target_label\"] = ff_results_summary.inflection_to + \" \" + ff_results_summary.allomorph_to\n",
    "\n",
    "ff_results_summary[\"transfer_label\"] = ff_results_summary.inflection_from + \" -> \" + ff_results_summary.inflection_to\n",
    "ff_results_summary[\"phon_label\"] = ff_results_summary.allomorph_from + \" \" + ff_results_summary.allomorph_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For FF1->FF2 results, fill out the upper triangle\n",
    "extra_rows = []\n",
    "for _, row in ff_results_summary[ff_results_summary.inflection_from.str.contains(\"-FF\") & ff_results_summary.inflection_to.str.contains(\"-FF\")].iterrows():\n",
    "    if row.allomorph_to == row.allomorph_from:\n",
    "        continue\n",
    "    extra_rows.append(row.copy())\n",
    "    extra_rows[-1].inflection_from, extra_rows[-1].inflection_to = extra_rows[-1].inflection_to, extra_rows[-1].inflection_from\n",
    "    extra_rows[-1].allomorph_from, extra_rows[-1].allomorph_to = extra_rows[-1].allomorph_to, extra_rows[-1].allomorph_from\n",
    "    extra_rows[-1].source_label, extra_rows[-1].target_label = extra_rows[-1].target_label, extra_rows[-1].source_label\n",
    "    extra_rows[-1].transfer_label = extra_rows[-1].transfer_label.replace(\" -> \", \" <- \")\n",
    "\n",
    "ff_results_summary = pd.concat([ff_results_summary, pd.DataFrame(extra_rows)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_results_summary.sort_values([\"source_label\", \"target_label\"])#sort_values(\"mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(pd.concat([nnvb_results_summary, ff_results_summary]).set_index([\"source_label\", \"target_label\"]).sort_index()[\"mean\"].unstack(\"target_label\"), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_results_summary[\"ff_allomorph\"] = ff_results_summary.apply(\n",
    "    lambda row: row.allomorph_from if row.inflection_from.endswith(\"-FF\") else row.allomorph_to, axis=1)\n",
    "ff_results_summary[\"inflection_base\"] = ff_results_summary.inflection_from.str.replace(\"-FF\", \"\")\n",
    "ff_results_summary[\"ff_direction\"] = ff_results_summary.inflection_from.str.contains(\"-FF\").map({True: \"from\", False: \"to\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = ff_results_summary.groupby(\"ff_allomorph\")[\"mean\"].mean().sort_values(ascending=False).index\n",
    "g = sns.catplot(data=ff_results_summary, x=\"ff_allomorph\", y=\"mean\", hue=\"inflection_base\", col=\"ff_direction\",\n",
    "                order=order, kind=\"point\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = sns.catplot(data=nnvb_results_summary.query(\"inflection_from == inflection_to\"),\n",
    "                 x=\"allomorph_from\", y=\"mean\", hue=\"inflection_from\", kind=\"point\")\n",
    "# g2.axes.flat[0].set_ylim(g.axes.flat[0].get_ylim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_results_summary2 = all_ff_results.groupby([\"inflection_from\", \"inflection_to\"]) \\\n",
    "    [[\"correct\", \"gt_label_rank\", \"gt_distance\"]].mean().reset_index()\n",
    "\n",
    "ff_results_summary2[\"transfer_label\"] = ff_results_summary2.inflection_from + \" -> \" + ff_results_summary2.inflection_to\n",
    "\n",
    "# add in data for NNS->NNS and VBZ->VBZ\n",
    "ff_results_summary2 = pd.concat([ff_results_summary2, nnvb_results_summary2.query(\"inflection_from == inflection_to\")], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(ff_results_summary2.set_index([\"inflection_from\", \"inflection_to\"]).correct.unstack(\"inflection_to\"),\n",
    "            annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Frequency effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ff_results[\"transfer_label\"] = all_ff_results.inflection_from + \" -> \" + all_ff_results.inflection_to\n",
    "g = sns.catplot(data=all_ff_results, x=\"from_freq_bin\", y=\"correct\", kind=\"point\", \n",
    "                col=\"transfer_label\", col_wrap=2,\n",
    "                height=3, aspect=1.5)\n",
    "\n",
    "# Add twin axes and histograms\n",
    "for ax in g.axes.flat:\n",
    "    twin_ax = ax.twinx()  # Create a twin y-axis\n",
    "    col_name = ax.get_title().replace(\"transfer_label = \", \"\")  # Extract facet label\n",
    "\n",
    "    # Get subset of data for this facet\n",
    "    subset = all_ff_results[all_ff_results[\"transfer_label\"] == col_name]\n",
    "\n",
    "    # Plot histogram on twin axis\n",
    "    sns.barplot(data=subset.groupby(\"from_freq_bin\").apply(lambda xs: xs.base_from.nunique()),\n",
    "                ax=twin_ax, alpha=0.3, color=\"blue\")\n",
    "\n",
    "    twin_ax.set_ylabel(\"Count\")  # Label twin axis\n",
    "    twin_ax.grid(False)  # Remove extra gridlines for clarity\n",
    "    for spine in twin_ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "g.figure.suptitle(\"Frequency effect of source word\")\n",
    "g.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ff_results[\"transfer_label\"] = all_ff_results.inflection_from + \" -> \" + all_ff_results.inflection_to\n",
    "g = sns.catplot(data=all_ff_results, x=\"to_freq_bin\", y=\"correct\", kind=\"point\", \n",
    "                col=\"transfer_label\", col_wrap=2,\n",
    "                height=3, aspect=1.5)\n",
    "\n",
    "# Add twin axes and histograms\n",
    "for ax in g.axes.flat:\n",
    "    twin_ax = ax.twinx()  # Create a twin y-axis\n",
    "    col_name = ax.get_title().replace(\"transfer_label = \", \"\")  # Extract facet label\n",
    "\n",
    "    # Get subset of data for this facet\n",
    "    subset = all_ff_results[all_ff_results[\"transfer_label\"] == col_name]\n",
    "\n",
    "    # Plot histogram on twin axis\n",
    "    sns.barplot(data=subset.groupby(\"to_freq_bin\").apply(lambda xs: xs.base_to.nunique()),\n",
    "                ax=twin_ax, alpha=0.3, color=\"blue\")\n",
    "\n",
    "    twin_ax.set_ylabel(\"Count\")  # Label twin axis\n",
    "    twin_ax.grid(False)  # Remove extra gridlines for clarity\n",
    "    for spine in twin_ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "g.figure.suptitle(\"Frequency effect of target word\")\n",
    "g.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping by \"to\" word\n",
    "plot_df = all_ff_results.groupby([\"inflection_from\", \"inflection_to\", \"base_to\"]).correct.mean().reset_index()\n",
    "plot_df[\"transfer_label\"] = plot_df.inflection_from + \" -> \" + plot_df.inflection_to\n",
    "order = plot_df.groupby(\"transfer_label\")[\"correct\"].mean().sort_values(ascending=False).index\n",
    "sns.catplot(data=plot_df, x=\"transfer_label\", y=\"correct\", kind=\"box\", order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_ff_scores = all_ff_results[all_ff_results.inflection_from.str.contains(\"-FF\") & ~all_ff_results.inflection_to.str.contains(\"-FF\")] \\\n",
    "    .groupby([\"allomorph_from\", \"allomorph_to\", \"base_from\"]).correct.mean() \\\n",
    "    .sort_values(ascending=False).reset_index()\n",
    "from_ff_scores = pd.merge(from_ff_scores, word_freq_df[\"LogFreq\"], left_on=\"base_from\", right_index=True)\n",
    "from_ff_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(data=from_ff_scores, x=\"LogFreq\", y=\"correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_ff_scores = all_ff_results[~all_ff_results.inflection_from.str.contains(\"-FF\") & all_ff_results.inflection_to.str.contains(\"-FF\")] \\\n",
    "    .groupby([\"allomorph_from\", \"allomorph_to\", \"base_to\"]).correct.mean() \\\n",
    "    .sort_values(ascending=False).reset_index()\n",
    "to_ff_scores = pd.merge(to_ff_scores, word_freq_df[\"LogFreq\"], left_on=\"base_to\", right_index=True)\n",
    "to_ff_scores.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(data=to_ff_scores, x=\"LogFreq\", y=\"correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping over transfer allomorphs\n",
    "order = ff_results_summary.groupby(\"transfer_label\")[\"mean\"].median().sort_values(ascending=False).index\n",
    "sns.catplot(data=ff_results_summary, x=\"transfer_label\", y=\"mean\", kind=\"box\", order=order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Frequency analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nnvb_results = pd.merge(all_nnvb_results, word_freq_df.LogFreq.rename(\"from_freq\"),\n",
    "                              left_on=\"base_from\", right_index=True)\n",
    "all_nnvb_results = pd.merge(all_nnvb_results, word_freq_df.LogFreq.rename(\"to_freq\"),\n",
    "                              left_on=\"base_to\", right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nnvb_results[\"to_freq_bin\"] = pd.cut(all_nnvb_results.to_freq, bins=5, labels=[f\"Q{i}\" for i in range(1, 6)])\n",
    "all_nnvb_results[\"from_freq_bin\"] = pd.cut(all_nnvb_results.from_freq, bins=5, labels=[f\"Q{i}\" for i in range(1, 6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nnvb_results.groupby([\"inflection_from\", \"inflection_to\", \"from_freq_bin\", \"base_from\"]).correct.mean().dropna().reset_index().groupby([\"inflection_from\", \"inflection_to\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nnvb_results.groupby([\"inflection_from\", \"inflection_to\", \"from_freq_bin\"]).correct.mean().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(data=all_nnvb_results, x=\"from_freq_bin\", y=\"correct\", kind=\"point\",\n",
    "                row=\"inflection_from\", col=\"inflection_to\", height=3, aspect=1.5)\n",
    "\n",
    "# Add twin axes and histograms\n",
    "for ax in g.axes.flat:\n",
    "    twin_ax = ax.twinx()  # Create a twin y-axis\n",
    "    inflection_from, inflection_to = re.findall(r\"inflection_from = (.+) \\| inflection_to = (.+)\", ax.get_title())[0]\n",
    "\n",
    "    # Get subset of data for this facet\n",
    "    subset = all_nnvb_results.query(\"inflection_from == @inflection_from and inflection_to == @inflection_to\")\n",
    "\n",
    "    # Plot histogram on twin axis\n",
    "    sns.barplot(data=subset.groupby(\"from_freq_bin\").apply(lambda xs: xs.base_from.nunique()),\n",
    "                ax=twin_ax, alpha=0.3, color=\"blue\")\n",
    "\n",
    "    twin_ax.set_ylabel(\"Count\")  # Label twin axis\n",
    "    twin_ax.grid(False)  # Remove extra gridlines for clarity\n",
    "    for spine in twin_ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "g.figure.suptitle(\"Frequency effect of source word\")\n",
    "g.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(data=all_nnvb_results, x=\"to_freq_bin\", y=\"correct\", kind=\"point\",\n",
    "                row=\"inflection_from\", col=\"inflection_to\", height=3, aspect=1.5)\n",
    "\n",
    "# Add twin axes and histograms\n",
    "for ax in g.axes.flat:\n",
    "    twin_ax = ax.twinx()  # Create a twin y-axis\n",
    "    inflection_from, inflection_to = re.findall(r\"inflection_from = (.+) \\| inflection_to = (.+)\", ax.get_title())[0]\n",
    "\n",
    "    # Get subset of data for this facet\n",
    "    subset = all_nnvb_results.query(\"inflection_from == @inflection_from and inflection_to == @inflection_to\")\n",
    "\n",
    "    # Plot histogram on twin axis\n",
    "    sns.barplot(data=subset.groupby(\"to_freq_bin\").apply(lambda xs: xs.base_to.nunique()),\n",
    "                ax=twin_ax, alpha=0.3, color=\"blue\")\n",
    "\n",
    "    twin_ax.set_ylabel(\"Count\")  # Label twin axis\n",
    "    twin_ax.grid(False)  # Remove extra gridlines for clarity\n",
    "    for spine in twin_ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "g.figure.suptitle(\"Frequency effect of target word\")\n",
    "g.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representational analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_spec_df = experiment_results.loc[\"nn_vb_ambiguous\"]\n",
    "rep_spec_df[\"inflection\"] = rep_spec_df.group.str[0]\n",
    "rep_spec_df[\"ambiguous\"] = rep_spec_df.group.str[1]\n",
    "rep_spec_df = rep_spec_df[~rep_spec_df.ambiguous]\n",
    "rep_spec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inflection_vectors(results_df, correct_only=True, max_num_vector_samples=250):\n",
    "    ret = {}\n",
    "    if correct_only:\n",
    "        results_df = results_df[results_df.correct]\n",
    "\n",
    "    for inflection, group in results_df.groupby(\"inflection\"):\n",
    "        ret[inflection] = {}\n",
    "        for base_word in group.base_from.unique():\n",
    "            # HACK look up inflected form which we didn't save\n",
    "            inflected_word = inflection_results_df.loc[inflection].query(\"base == @base_word\").inflected.iloc[0]\n",
    "\n",
    "            base_flat_idxs = np.where(agg_src[:, 0] == state_space_spec.labels.index(base_word))[0]\n",
    "            inflected_flat_idxs = np.where(agg_src[:, 0] == state_space_spec.labels.index(inflected_word))[0]\n",
    "\n",
    "            if len(base_flat_idxs) > max_num_vector_samples:\n",
    "                base_flat_idxs = np.random.choice(base_flat_idxs, size=max_num_vector_samples, replace=False)\n",
    "            elif len(base_flat_idxs) < max_num_vector_samples:\n",
    "                base_flat_idxs = np.random.choice(base_flat_idxs, size=max_num_vector_samples, replace=True)\n",
    "            if len(inflected_flat_idxs) > max_num_vector_samples:\n",
    "                inflected_flat_idxs = np.random.choice(inflected_flat_idxs, size=max_num_vector_samples, replace=False)\n",
    "            elif len(inflected_flat_idxs) < max_num_vector_samples:\n",
    "                inflected_flat_idxs = np.random.choice(inflected_flat_idxs, size=max_num_vector_samples, replace=True)\n",
    "\n",
    "            difference_vectors = (agg[inflected_flat_idxs] - agg[base_flat_idxs]).mean(0)\n",
    "            ret[inflection][base_word] = difference_vectors\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infl_vectors = get_inflection_vectors(rep_spec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_infl_types = sorted(infl_vectors.keys())\n",
    "infl_distances = np.zeros((len(analyze_infl_types), len(analyze_infl_types)))\n",
    "infl_distances_all = []\n",
    "for i, inflection_i in enumerate(analyze_infl_types):\n",
    "    for j, inflection_j in enumerate(analyze_infl_types):\n",
    "        distances = 1 - fastdist.cosine_matrix_to_matrix(\n",
    "            np.stack([v for v in infl_vectors[inflection_i].values()]),\n",
    "            np.stack([v for v in infl_vectors[inflection_j].values()])\n",
    "        )\n",
    "        infl_distances[i, j] = distances.mean()\n",
    "        infl_distances_all.extend((inflection_i, inflection_j, d) for d in distances.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infl_distances_all = pd.DataFrame(infl_distances_all, columns=[\"inflection_i\", \"inflection_j\", \"distance\"])\n",
    "infl_distances_all[\"relationship\"] = infl_distances_all.inflection_i + \" to \" + infl_distances_all.inflection_j\n",
    "g = sns.catplot(data=infl_distances_all, x=\"relationship\", y=\"distance\", kind=\"bar\")\n",
    "g.axes.flat[0].set_ylabel(\"Cosine distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_ind(infl_distances_all.query(\"relationship == 'NNS to NNS'\").distance,\n",
    "          infl_distances_all.query(\"relationship == 'VBZ to VBZ'\").distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(pd.DataFrame(infl_distances, index=analyze_infl_types, columns=analyze_infl_types),\n",
    "            annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lil PCA thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2).fit(agg)\n",
    "agg_pca = pca.transform(agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, ax = plt.subplots(figsize=(12, 12))\n",
    "# plot_words = rep_spec_df.groupby(\"inflection\").sample(3)\n",
    "# cmap = sns.color_palette(\"tab10\", len(plot_words))\n",
    "\n",
    "# max_plot_points = 20\n",
    "\n",
    "# for i, (_, row) in enumerate(plot_words.iterrows()):\n",
    "#     base_flat_idxs = np.where(agg_src[:, 0] == state_space_spec.labels.index(row.base_from))[0]\n",
    "#     base_reps = agg_pca[base_flat_idxs]\n",
    "#     if len(base_reps) > max_plot_points:\n",
    "#         base_reps = base_reps[np.random.choice(len(base_reps), size=max_plot_points, replace=False)]\n",
    "#     ax.scatter(*base_reps.T, color=cmap[i], label=row.base_from)\n",
    "\n",
    "#     if row.inflection == \"NNS\":\n",
    "#         # get target inflected form\n",
    "#         nns_form = inflection_results_df.loc[\"NNS\"].query(\"base == @row.base_from\").inflected.iloc[0]\n",
    "#         inflected_flat_idxs = np.where(agg_src[:, 0] == state_space_spec.labels.index(nns_form))[0]\n",
    "#         inflected_reps = agg_pca[inflected_flat_idxs]\n",
    "#         if len(inflected_reps) > max_plot_points:\n",
    "#             inflected_reps = inflected_reps[np.random.choice(len(inflected_reps), size=max_plot_points, replace=False)]\n",
    "        \n",
    "#         ax.scatter(*inflected_reps.T, color=cmap[i], marker=\"x\")\n",
    "#     elif row.inflection == \"VBZ\":\n",
    "#         # get VBZ inflected form\n",
    "#         vbz_form = inflection_results_df.loc[\"VBZ\"].query(\"base == @row.base_from\").inflected.iloc[0]\n",
    "#         inflected_flat_idxs = np.where(agg_src[:, 0] == state_space_spec.labels.index(vbz_form))[0]\n",
    "#         inflected_reps = agg_pca[inflected_flat_idxs]\n",
    "#         if len(inflected_reps) > max_plot_points:\n",
    "#             inflected_reps = inflected_reps[np.random.choice(len(inflected_reps), size=max_plot_points, replace=False)]\n",
    "#         ax.scatter(*inflected_reps.T, color=cmap[i], marker=\"x\")\n",
    "\n",
    "#         # get VBD inflected form\n",
    "#         vbd_form = inflection_results_df.loc[\"VBD\"].query(\"base == @row.base_from\").inflected.iloc[0]\n",
    "#         inflected_flat_idxs = np.where(agg_src[:, 0] == state_space_spec.labels.index(vbd_form))[0]\n",
    "#         inflected_reps = agg_pca[inflected_flat_idxs]\n",
    "#         if len(inflected_reps) > max_plot_points:\n",
    "#             inflected_reps = inflected_reps[np.random.choice(len(inflected_reps), size=max_plot_points, replace=False)]\n",
    "#         ax.scatter(*inflected_reps.T, color=cmap[i], marker=\"*\")\n",
    "\n",
    "#         # get VBG inflected form\n",
    "#         vbg_form = inflection_results_df.loc[\"VBG\"].query(\"base == @row.base_from\").inflected.iloc[0]\n",
    "#         inflected_flat_idxs = np.where(agg_src[:, 0] == state_space_spec.labels.index(vbg_form))[0]\n",
    "#         inflected_reps = agg_pca[inflected_flat_idxs]\n",
    "#         if len(inflected_reps) > max_plot_points:\n",
    "#             inflected_reps = inflected_reps[np.random.choice(len(inflected_reps), size=max_plot_points, replace=False)]\n",
    "#         ax.scatter(*inflected_reps.T, color=cmap[i], marker=\"^\")\n",
    "\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Collect the vectors to be visualized\n",
    "def draw(xs):\n",
    "    xs = xs.drop_duplicates(\"base_from\")\n",
    "    xs = xs.sample(min(100, len(xs)))\n",
    "    return xs\n",
    "plot_words = rep_spec_df.query(\"correct\").groupby(\"inflection\").apply(draw)\n",
    "plot_means = True\n",
    "\n",
    "# Extract indices of relevant embeddings\n",
    "all_indices = []\n",
    "for _, row in plot_words.iterrows():\n",
    "    base_idx = np.where(agg_src[:, 0] == state_space_spec.labels.index(row.base_from))[0]\n",
    "    all_indices.extend(base_idx.tolist())\n",
    "\n",
    "    if row.inflection == \"NNS\":\n",
    "        nns_form = inflection_results_df.loc[\"NNS\"].query(\"base == @row.base_from\").inflected.iloc[0]\n",
    "        nns_idx = np.where(agg_src[:, 0] == state_space_spec.labels.index(nns_form))[0]\n",
    "        all_indices.extend(nns_idx.tolist())\n",
    "\n",
    "    elif row.inflection == \"VBZ\":\n",
    "        vbz_form = inflection_results_df.loc[\"VBZ\"].query(\"base == @row.base_from\").inflected.iloc[0]\n",
    "        vbz_idx = np.where(agg_src[:, 0] == state_space_spec.labels.index(vbz_form))[0]\n",
    "        all_indices.extend(vbz_idx.tolist())\n",
    "\n",
    "        try:\n",
    "            vbd_form = inflection_results_df.loc[\"VBD\"].query(\"base == @row.base_from\").inflected.iloc[0]\n",
    "            vbd_idx = np.where(agg_src[:, 0] == state_space_spec.labels.index(vbd_form))[0]\n",
    "            all_indices.extend(vbd_idx.tolist())\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            vbg_form = inflection_results_df.loc[\"VBG\"].query(\"base == @row.base_from\").inflected.iloc[0]\n",
    "            vbg_idx = np.where(agg_src[:, 0] == state_space_spec.labels.index(vbg_form))[0]\n",
    "            all_indices.extend(vbg_idx.tolist())\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Ensure unique indices and extract embeddings\n",
    "all_indices = np.unique(all_indices)\n",
    "agg_centered = (agg - agg.mean(0)) / agg.std(0)\n",
    "selected_vectors = agg_centered[all_indices]  # Use raw embeddings before PCA\n",
    "\n",
    "# Run PCA on the selected embeddings\n",
    "pca = PCA(n_components=2)\n",
    "# pca.fit(agg_centered)\n",
    "pca.fit(selected_vectors)\n",
    "pca_transformed = pca.transform(selected_vectors)\n",
    "\n",
    "# Map back to original indices\n",
    "pca_dict = {idx: pca_transformed[i] for i, idx in enumerate(all_indices)}\n",
    "\n",
    "# Set up plot\n",
    "plot_inflections = sorted(plot_words.inflection.unique())\n",
    "n_plot_inflections = len(plot_inflections)\n",
    "f, axs = plt.subplots(figsize=(12, 12 * n_plot_inflections), nrows=n_plot_inflections)\n",
    "# cmap = sns.color_palette(\"tab10\", len(plot_words))\n",
    "cmap = sns.color_palette(\"tab10\", 2)\n",
    "cmap = {i: cmap[0 if row.inflection == \"NNS\" else 1] for i, (_, row) in enumerate(plot_words.iterrows())}\n",
    "\n",
    "max_plot_points = 20\n",
    "\n",
    "# Plot points after PCA transformation\n",
    "for i, (_, row) in enumerate(plot_words.iterrows()):\n",
    "    ax = axs.flat[plot_inflections.index(row.inflection)]\n",
    "\n",
    "    base_flat_idxs = np.where(agg_src[:, 0] == state_space_spec.labels.index(row.base_from))[0]\n",
    "    base_reps = np.array([pca_dict[idx] for idx in base_flat_idxs if idx in pca_dict])\n",
    "    if plot_means:\n",
    "        base_reps = base_reps.mean(0, keepdims=True)\n",
    "    if len(base_reps) > max_plot_points:\n",
    "        base_reps = base_reps[np.random.choice(len(base_reps), size=max_plot_points, replace=False)]\n",
    "    ax.scatter(*base_reps.T, color=cmap[i], label=row.base_from, alpha=0.5, s=100)\n",
    "\n",
    "    if row.inflection == \"NNS\":\n",
    "        nns_form = inflection_results_df.loc[\"NNS\"].query(\"base == @row.base_from\").inflected.iloc[0]\n",
    "        inflected_flat_idxs = np.where(agg_src[:, 0] == state_space_spec.labels.index(nns_form))[0]\n",
    "        inflected_reps = np.array([pca_dict[idx] for idx in inflected_flat_idxs if idx in pca_dict])\n",
    "        if plot_means:\n",
    "            inflected_reps = inflected_reps.mean(0, keepdims=True)\n",
    "        if len(inflected_reps) > max_plot_points:\n",
    "            inflected_reps = inflected_reps[np.random.choice(len(inflected_reps), size=max_plot_points, replace=False)]\n",
    "        ax.scatter(*inflected_reps.T, color=cmap[i], marker=\"x\", alpha=0.5)\n",
    "        if plot_means: \n",
    "            # plot a link between base and here\n",
    "            ax.plot([base_reps[0, 0], inflected_reps[0, 0]], [base_reps[0, 1], inflected_reps[0, 1]], color=cmap[i], alpha=0.5)\n",
    "\n",
    "    elif row.inflection == \"VBZ\":\n",
    "        vbz_form = inflection_results_df.loc[\"VBZ\"].query(\"base == @row.base_from\").inflected.iloc[0]\n",
    "        inflected_flat_idxs = np.where(agg_src[:, 0] == state_space_spec.labels.index(vbz_form))[0]\n",
    "        inflected_reps = np.array([pca_dict[idx] for idx in inflected_flat_idxs if idx in pca_dict])\n",
    "        if plot_means:\n",
    "            inflected_reps = inflected_reps.mean(0, keepdims=True)\n",
    "        if len(inflected_reps) > max_plot_points:\n",
    "            inflected_reps = inflected_reps[np.random.choice(len(inflected_reps), size=max_plot_points, replace=False)]\n",
    "        ax.scatter(*inflected_reps.T, color=cmap[i], marker=\"x\", alpha=0.5)\n",
    "        if plot_means:\n",
    "            # plot a link between base and here\n",
    "            ax.plot([base_reps[0, 0], inflected_reps[0, 0]], [base_reps[0, 1], inflected_reps[0, 1]], color=cmap[i], alpha=0.5)\n",
    "\n",
    "        try:\n",
    "            vbd_form = inflection_results_df.loc[\"VBD\"].query(\"base == @row.base_from\").inflected.iloc[0]\n",
    "            inflected_flat_idxs = np.where(agg_src[:, 0] == state_space_spec.labels.index(vbd_form))[0]\n",
    "            inflected_reps = np.array([pca_dict[idx] for idx in inflected_flat_idxs if idx in pca_dict])\n",
    "            if plot_means:\n",
    "                inflected_reps = inflected_reps.mean(0, keepdims=True)\n",
    "            if len(inflected_reps) > max_plot_points:\n",
    "                inflected_reps = inflected_reps[np.random.choice(len(inflected_reps), size=max_plot_points, replace=False)]\n",
    "            ax.scatter(*inflected_reps.T, color=cmap[i], marker=\"*\", alpha=0.5)\n",
    "            if plot_means:\n",
    "                # plot a link between base and here\n",
    "                ax.plot([base_reps[0, 0], inflected_reps[0, 0]], [base_reps[0, 1], inflected_reps[0, 1]], color=cmap[i], alpha=0.5)\n",
    "        except: pass\n",
    "\n",
    "        try:\n",
    "            vbg_form = inflection_results_df.loc[\"VBG\"].query(\"base == @row.base_from\").inflected.iloc[0]\n",
    "            inflected_flat_idxs = np.where(agg_src[:, 0] == state_space_spec.labels.index(vbg_form))[0]\n",
    "            inflected_reps = np.array([pca_dict[idx] for idx in inflected_flat_idxs if idx in pca_dict])\n",
    "            if plot_means:\n",
    "                inflected_reps = inflected_reps.mean(0, keepdims=True)\n",
    "            if len(inflected_reps) > max_plot_points:\n",
    "                inflected_reps = inflected_reps[np.random.choice(len(inflected_reps), size=max_plot_points, replace=False)]\n",
    "            ax.scatter(*inflected_reps.T, color=cmap[i], marker=\"^\", alpha=0.5)\n",
    "            if plot_means:\n",
    "                # plot a link between base and here\n",
    "                ax.plot([base_reps[0, 0], inflected_reps[0, 0]], [base_reps[0, 1], inflected_reps[0, 1]], color=cmap[i], alpha=0.5)\n",
    "        except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
