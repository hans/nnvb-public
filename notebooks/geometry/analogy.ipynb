{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import duckdb\n",
    "import lemminflect\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import transforms\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec, \\\n",
    "    prepare_state_trajectory, aggregate_state_trajectory, flatten_trajectory\n",
    "from src.datasets.speech_equivalence import SpeechEquivalenceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "base_model = \"w2v2_8\"\n",
    "model_class = \"discrim-rnn_32-mAP1\"\n",
    "model_name = \"word_broad_10frames_fixedlen25\"\n",
    "train_dataset = \"librispeech-train-clean-100\"\n",
    "model_dir = f\"outputs/models/{train_dataset}/{base_model}/{model_class}/{model_name}\"\n",
    "output_dir = f\".\"\n",
    "dataset_path = f\"outputs/preprocessed_data/{train_dataset}\"\n",
    "equivalence_path = f\"outputs/equivalence_datasets/{train_dataset}/{base_model}/{model_name}/equivalence.pkl\"\n",
    "hidden_states_path = f\"outputs/hidden_states/{train_dataset}/{base_model}/{train_dataset}.h5\"\n",
    "state_space_specs_path = f\"outputs/state_space_specs/{train_dataset}/{base_model}/state_space_specs.h5\"\n",
    "embeddings_path = f\"outputs/model_embeddings/{train_dataset}/{base_model}/{model_class}/{model_name}/{train_dataset}.npy\"\n",
    "\n",
    "seed = 1234\n",
    "\n",
    "max_samples_per_word = 100\n",
    "\n",
    "metric = \"cosine\"\n",
    "\n",
    "agg_fns = [\n",
    "    \"mean\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embeddings_path, \"rb\") as f:\n",
    "    model_representations: np.ndarray = np.load(f)\n",
    "state_space_spec = StateSpaceAnalysisSpec.from_hdf5(state_space_specs_path, \"word\")\n",
    "assert state_space_spec.is_compatible_with(model_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space_spec = state_space_spec.subsample_instances(max_samples_per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = prepare_state_trajectory(model_representations, state_space_spec, pad=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_aggs = {agg_fn: aggregate_state_trajectory(trajectory, state_space_spec, agg_fn, keepdims=True)\n",
    "                   for agg_fn in tqdm(agg_fns)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_aggs_flat = {k: flatten_trajectory(v) for k, v in trajectory_aggs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts_df = state_space_spec.cuts.xs(\"phoneme\", level=\"level\").drop(columns=[\"onset_frame_idx\", \"offset_frame_idx\"])\n",
    "cuts_df[\"label_idx\"] = cuts_df.index.get_level_values(\"label\").map({l: i for i, l in enumerate(state_space_spec.labels)})\n",
    "cuts_df[\"frame_idx\"] = cuts_df.groupby([\"label\", \"instance_idx\"]).cumcount()\n",
    "cuts_df = cuts_df.reset_index().set_index([\"label\", \"instance_idx\", \"frame_idx\"]).sort_index()\n",
    "cuts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_phonemic_forms = cuts_df.groupby([\"label\", \"instance_idx\"]).description.agg(' '.join)\n",
    "cut_phonemic_forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_analogy(triple, agg_method=\"mean\", num_samples=50, k=20, verbose=False):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    - result_df: a df describing the k nearest neighbors to the analogy vector in each sample\n",
    "    - difference_vectors: the difference vectors used in the analogy for each sample\n",
    "    - analogy_vectors: the analogy vectors for each sample\n",
    "    \"\"\"\n",
    "\n",
    "    word_a, word_b, word_c, expected = triple\n",
    "    assert word_a in state_space_spec.labels\n",
    "    assert word_b in state_space_spec.labels\n",
    "    assert word_c in state_space_spec.labels\n",
    "\n",
    "    # if the expected word isn't in the vocabulary, this isn't really interpretable\n",
    "    assert expected in state_space_spec.labels\n",
    "\n",
    "    word_a_idx = state_space_spec.labels.index(word_a)\n",
    "    word_b_idx = state_space_spec.labels.index(word_b)\n",
    "    word_c_idx = state_space_spec.labels.index(word_c)\n",
    "\n",
    "    # collect results of a - b\n",
    "    difference_vectors = []\n",
    "    # collect results of a - b + c\n",
    "    analogy_vectors = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        word_a_instance = np.random.choice(min(max_samples_per_word, len(state_space_spec.target_frame_spans[word_a_idx])))\n",
    "        word_b_instance = np.random.choice(min(max_samples_per_word, len(state_space_spec.target_frame_spans[word_b_idx])))\n",
    "        word_c_instance = np.random.choice(min(max_samples_per_word, len(state_space_spec.target_frame_spans[word_c_idx])))\n",
    "\n",
    "        word_a_traj = trajectory_aggs[agg_method][word_a_idx][word_a_instance].squeeze()\n",
    "        word_b_traj = trajectory_aggs[agg_method][word_b_idx][word_b_instance].squeeze()\n",
    "        word_c_traj = trajectory_aggs[agg_method][word_c_idx][word_c_instance].squeeze()\n",
    "\n",
    "        difference_vector = word_a_traj - word_b_traj\n",
    "        analogy_vector = difference_vector + word_c_traj\n",
    "\n",
    "        difference_vectors.append(difference_vector)\n",
    "        analogy_vectors.append(analogy_vector)\n",
    "\n",
    "    difference_vectors = np.array(difference_vectors)\n",
    "    analogy_vectors = np.array(analogy_vectors)\n",
    "\n",
    "    references, references_src = trajectory_aggs_flat[agg_method]\n",
    "    dists = cdist(analogy_vectors, references, metric=metric).mean(axis=0)\n",
    "    ranks = dists.argsort()\n",
    "\n",
    "    if verbose:\n",
    "        for dist, (label_idx, instance_idx, _) in zip(dists[ranks[:k]], references_src[ranks[:k]]):\n",
    "            print(dist, state_space_spec.labels[label_idx])\n",
    "\n",
    "    ret = pd.DataFrame(references_src[ranks[:k]], columns=[\"label_idx\", \"instance_idx\", \"frame_idx\"])\n",
    "    ret[\"distance\"] = dists[ranks[:k]]\n",
    "    ret[\"label\"] = [state_space_spec.labels[label_idx] for label_idx in ret[\"label_idx\"]]\n",
    "    return ret, difference_vectors, analogy_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial BATS study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analogy_dataset = datasets.load_dataset(\"relbert/analogy_questions\", \"bats\") \\\n",
    "#     [\"test\"].filter(lambda x: \"morphology\" in x[\"prefix\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference_vectors = []\n",
    "# prediction_results = []\n",
    "# k = 20\n",
    "# for item in tqdm(analogy_dataset):\n",
    "#     b, a = item[\"stem\"]\n",
    "#     c, d = item[\"choice\"][item[\"answer\"]]\n",
    "    \n",
    "#     try:\n",
    "#         ret, difference_vectors_i, _ = estimate_analogy((a, b, c, d), num_samples=100, k=k, verbose=False)\n",
    "#     except AssertionError:\n",
    "#         continue\n",
    "    \n",
    "#     nearest_neighbor = ret.iloc[0].label\n",
    "#     prediction_results.append(\n",
    "#         {\"nearest_neighbor\": nearest_neighbor,\n",
    "#          \"expected\": d,\n",
    "#          \"correct\": nearest_neighbor == d,\n",
    "#          \"correct_topk\": d in ret.iloc[:k].label.tolist(),\n",
    "#          \"correct_position\": ret[ret.label == d].index[0] if d in ret.label.values else None,\n",
    "#          **item})\n",
    "    \n",
    "#     difference_vectors.append({\"a\": a, \"b\": b, \"prefix\": item[\"prefix\"],\n",
    "#                                \"difference_vectors\": difference_vectors_i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = pd.DataFrame(prediction_results).drop(columns=[\"choice\"])\n",
    "# results_df.to_csv(Path(output_dir) / \"analogy_results.csv\", index=False)\n",
    "# results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_df = results_df.groupby(\"prefix\").correct.agg([\"count\", \"mean\"]).sort_values(\"mean\")\n",
    "# summary_df.to_csv(Path(output_dir) / \"analogy_summary.csv\")\n",
    "# summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(difference_vectors, Path(output_dir) / \"analogy_difference_vectors.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homegrown study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inflection(word, all_labels, target) -> set[str]:\n",
    "    if target in (\"VBD\", \"VBZ\", \"VBG\", \"NNS\"):\n",
    "        inflections = set(lemminflect.getInflection(word, tag=target, inflect_oov=False))\n",
    "        # don't include zero-derived form\n",
    "        inflections -= {word}\n",
    "    elif target == \"NOT-latin\":\n",
    "        inflections = {\"in\" + word}\n",
    "        if word[0] == \"l\":\n",
    "            inflections |= {\"il\" + word}\n",
    "        elif word[0] in [\"p\", \"b\", \"m\"]:\n",
    "            inflections |= {\"im\" + word}\n",
    "        elif word[0] == \"r\":\n",
    "            inflections |= {\"ir\" + word}\n",
    "\n",
    "        # catch exceptional cases -- these predicted forms are attested, but don't count\n",
    "        # as a negative inflection. e.g. \"come\" -> \"income\"\n",
    "        if word in (\"come comes deed diana dies doors fancy form formation formed forming \"\n",
    "                    \"habit jury justice k l laid land most n part parted port press pressed \"\n",
    "                    \"prove proved pulse pulses side sight stead sure tend tended tending tent \"\n",
    "                    \"to trusted vent ward\"):\n",
    "            inflections = set()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown target: {target}\")\n",
    "    \n",
    "    covered_inflections = inflections & all_labels\n",
    "    return covered_inflections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_targets = [\n",
    "    (\"VBD\", \"verb_inf - Ved\"),\n",
    "    (\"VBZ\", \"verb_inf - 3pSg\"),\n",
    "    (\"VBG\", \"verb_inf - Ving\"),\n",
    "    (\"NNS\", \"noun - plural_reg\"),\n",
    "    (\"NOT-latin\", None),\n",
    "]\n",
    "labels = state_space_spec.label_counts\n",
    "labels = set(labels[labels > 5].index)\n",
    "\n",
    "inflection_results = {target: {} for target, _ in inflection_targets}\n",
    "inflection_reverse = defaultdict(set)\n",
    "for target, _ in tqdm(inflection_targets):\n",
    "    for label in labels:\n",
    "        label_inflections = get_inflection(label, labels, target)\n",
    "        if label_inflections:\n",
    "            inflection_results[target][label] = label_inflections\n",
    "\n",
    "            for infl in label_inflections:\n",
    "                inflection_reverse[infl].add((label, target))\n",
    "\n",
    "from pprint import pprint\n",
    "pprint({target: len(v) for target, v in inflection_results.items()})\n",
    "\n",
    "ambiguous_inflected_forms = {k: v for k, v in inflection_reverse.items()\n",
    "                             if len(v) > 1}\n",
    "print(f\"Ambiguous inflected forms ({len(ambiguous_inflected_forms)} total):\")\n",
    "print(\" \".join(ambiguous_inflected_forms.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_num_pairs = 400\n",
    "# DEV\n",
    "total_num_pairs = 10\n",
    "k = 5\n",
    "all_prediction_results = {}\n",
    "all_difference_vectors = {}\n",
    "for study_inflection, study_inflection_prefix in tqdm(inflection_targets):\n",
    "    # generate random pairs for analogies\n",
    "    candidate_bases = list(inflection_results[study_inflection].keys())\n",
    "    candidate_pairs = list(itertools.combinations(candidate_bases, 2))\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(candidate_pairs)\n",
    "    candidate_pairs = candidate_pairs[:total_num_pairs]\n",
    "\n",
    "    difference_vectors = []\n",
    "    prediction_results = []\n",
    "    for (word1, word2) in tqdm(candidate_pairs, leave=False):\n",
    "        a = next(iter(inflection_results[study_inflection][word1]))\n",
    "        b = word1\n",
    "        c = word2\n",
    "        d = next(iter(inflection_results[study_inflection][word2]))\n",
    "        print(a, b, c, d)\n",
    "        \n",
    "        try:\n",
    "            ret, difference_vectors_i, _ = estimate_analogy((a, b, c, d), num_samples=100, k=k, verbose=False)\n",
    "        except AssertionError:\n",
    "            continue\n",
    "        \n",
    "        nearest_neighbor = ret.iloc[0].label\n",
    "        prediction_results.append(\n",
    "            {\"a\": a, \"b\": b, \"c\": c, \"d\": d,\n",
    "            \"d_pred\": nearest_neighbor,\n",
    "            \"correct\": nearest_neighbor == d,\n",
    "            \"correct_topk\": d in ret.iloc[:k].label.tolist(),\n",
    "            \"correct_position\": ret[ret.label == d].index[0] if d in ret.label.values else None})\n",
    "        \n",
    "        difference_vectors.append({\"a\": a, \"b\": b, \"c\": c, \"prefix\": study_inflection_prefix,\n",
    "                                   \"difference_vectors\": difference_vectors_i})\n",
    "\n",
    "    all_prediction_results[study_inflection] = pd.DataFrame(prediction_results)\n",
    "    all_difference_vectors[study_inflection] = difference_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "broad_inflection_results = pd.concat(all_prediction_results, names=[\"inflection\"])\n",
    "broad_inflection_results.to_csv(f\"{output_dir}/broad_inflection_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(all_difference_vectors, Path(output_dir) / \"broad_inflection_difference_vectors.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broad_inflection_results = pd.read_csv(f\"{output_dir}/broad_inflection_results.csv\", index_col=[0, 1])\n",
    "broad_inflection_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_difference_vectors = torch.load(Path(output_dir) / \"broad_inflection_difference_vectors.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broad_inflection_results.groupby(\"inflection\").correct.mean().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broad_inflection_results.groupby(\"inflection\").correct_topk.mean().rename(\"correct\").to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_study = broad_inflection_results.loc[\"NOT-latin\"]\n",
    "not_study[\"map\"] = not_study.a.str[:2] + \"->\" + not_study.d.str[:2]\n",
    "not_study.groupby(\"map\").correct_topk.agg([\"count\", \"mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_vectors_flat = defaultdict(list)\n",
    "for inflection, difference_vectors in all_difference_vectors.items():\n",
    "    for dv in difference_vectors:\n",
    "        difference_vectors_flat[inflection].append(dv[\"difference_vectors\"].mean(0))\n",
    "difference_vectors_flat = {k: np.array(v) for k, v in difference_vectors_flat.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_difference_mat = np.zeros((len(inflection_targets), len(inflection_targets)))\n",
    "for i, (inflection1, _) in enumerate(inflection_targets):\n",
    "    for j, (inflection2, _) in enumerate(inflection_targets):\n",
    "        difference_difference_mat[i, j] = cdist(difference_vectors_flat[inflection1],\n",
    "                                                difference_vectors_flat[inflection2],\n",
    "                                                metric=metric).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(difference_difference_mat, annot=True, xticklabels=[t for t, _ in inflection_targets],\n",
    "            yticklabels=[t for t, _ in inflection_targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob = []\n",
    "def run_counterfactual_study(study_inflection, num_divergence_contents=5,\n",
    "                             num_word_samples=100,\n",
    "                             num_instance_samples=50,\n",
    "                             ignore_tags=(\"VBD\", \"VBN\",),\n",
    "                             agg_method=\"mean\"):\n",
    "    infl_results = broad_inflection_results.loc[study_inflection]\n",
    "    infl_results = infl_results[[\"a\", \"b\"]]\n",
    "    post_divergence_contents = Counter()\n",
    "    post_divergence_map = {}\n",
    "    for _, (inflected_form, base_form) in infl_results.iterrows():\n",
    "        base_cuts = cuts_df.loc[base_form]\n",
    "        inflected_cuts = cuts_df.loc[inflected_form]\n",
    "\n",
    "        # all attested phonological forms of base\n",
    "        base_phono_forms = set(base_cuts.groupby(\"instance_idx\").apply(\n",
    "            lambda xs: tuple(xs.description)))\n",
    "\n",
    "        post_divergence_i = Counter()\n",
    "        for _, inflected_instance in inflected_cuts.groupby(\"instance_idx\"):\n",
    "            # phonological divergence point: latest point at which the inflected form overlaps with\n",
    "            # any pronunciation of the base form\n",
    "            inflected_phones = tuple(inflected_instance.description)\n",
    "            phono_divergence_points = []\n",
    "            for base_phones in base_phono_forms:\n",
    "                for idx in range(len(inflected_phones) + 1):\n",
    "                    if inflected_phones[:idx] != base_phones[:idx]:\n",
    "                        break\n",
    "                phono_divergence_points.append(idx - 1)\n",
    "            phono_divergence_point = max(phono_divergence_points)\n",
    "\n",
    "            if phono_divergence_point == 0:\n",
    "                continue\n",
    "            post_divergence_i[inflected_phones[phono_divergence_point:]] += 1\n",
    "\n",
    "        if len(post_divergence_i) == 0:\n",
    "            continue\n",
    "        post_divergence_map[base_form] = post_divergence_i\n",
    "        post_divergence_contents[post_divergence_i.most_common()[0][0]] += 1\n",
    "\n",
    "    print(f\"Post-divergence contents for {study_inflection}:\")\n",
    "    pprint(post_divergence_contents.most_common(10))\n",
    "\n",
    "    # collect vectors by divergence content\n",
    "    study_vectors = all_difference_vectors[study_inflection]\n",
    "    study_vectors_flat = defaultdict(list)\n",
    "    for dv in study_vectors:\n",
    "        try:\n",
    "            divergence, _ = post_divergence_map[dv[\"b\"]].most_common()[0]\n",
    "        except KeyError:\n",
    "            # no divergence data was stored\n",
    "            continue\n",
    "        study_vectors_flat[divergence].extend(dv[\"difference_vectors\"])\n",
    "\n",
    "    study_vectors_flat = {k: np.array(v) for k, v in study_vectors_flat.items()}\n",
    "\n",
    "    #### counterfactual study\n",
    "\n",
    "    start_labels = state_space_spec.label_counts[state_space_spec.label_counts > 10].index\n",
    "\n",
    "    exclude_base_forms = \"a b bo co de d eh ha la ni r re pa y ye\".split()\n",
    "    start_labels = start_labels[~start_labels.isin(exclude_base_forms)]\n",
    "\n",
    "    # prepare to look up attested phonemic forms with minimum frequency\n",
    "    filtered_phonemic_forms = set(cut_phonemic_forms.loc[start_labels])\n",
    "    instances_by_phonemic_form = cut_phonemic_forms.reset_index().set_index(\"description\")\n",
    "    instances_by_phonemic_form[\"label_idx\"] = instances_by_phonemic_form.label.map({l: i for i, l in enumerate(state_space_spec.labels)})\n",
    "\n",
    "    # prepare counterfactual vectors for each divergence content\n",
    "    counterfactual_divergence_vectors = defaultdict(list)\n",
    "    for post_divergence, _ in tqdm(post_divergence_contents.most_common(num_divergence_contents)):\n",
    "        post_divergence_str = \" \".join(post_divergence)\n",
    "\n",
    "        # some good number of examples\n",
    "        step0 = cut_phonemic_forms.loc[start_labels]\n",
    "        # whole form has the same as the post-divergence content\n",
    "        step1 = step0.loc[(step0.str[-len(post_divergence_str):] == post_divergence_str).groupby(\"label\").transform(\"any\")]\n",
    "        # remove real inflection bases\n",
    "        step2 = step1.loc[~step1.index.get_level_values(\"label\").isin(inflection_results[study_inflection].keys())]\n",
    "        # remove real inflections\n",
    "        step3 = step2.loc[~step2.index.get_level_values(\"label\").isin(list(itertools.chain.from_iterable(inflection_results[study_inflection].values())))]\n",
    "        # and if you strip off the post-divergence content, it's still attested\n",
    "        step4 = step3.loc[step3.str[:-len(post_divergence_str)].str.strip().isin(filtered_phonemic_forms)]\n",
    "        # these shouldn't be attested within the same orthographic form\n",
    "        # e.g. \"second\" has pronunciations as both \"S EH K AH N D\" and \"S EH K AH N\",\n",
    "        # this shouldn't count\n",
    "        step5 = step4.loc[step4.groupby(\"label\").transform(\n",
    "            lambda xs: len(np.intersect1d(xs.str[:-len(post_divergence_str)].str.strip(),\n",
    "                                        cut_phonemic_forms.loc[xs.name])) == 0)]\n",
    "        # exclude anything whose dominant POS is in the exclude list\n",
    "        step5_labels = step5.index.get_level_values(\"label\").unique()\n",
    "        step5_exclude = {label: nltk.pos_tag(label.split())[0][1] in ignore_tags for label in step5_labels}\n",
    "        step5_exclude = pd.Series(step5_exclude)\n",
    "\n",
    "        # TODO exclude base--inflected counterfactual relations which are homophonic with\n",
    "        # a real inflection\n",
    "\n",
    "        # manual fixes\n",
    "        if study_inflection == \"NNS\":\n",
    "            ok_forms, bad_forms = [], []\n",
    "            if post_divergence == (\"Z\",):\n",
    "                # these forms are fine; they don't have the plural relationship with their base\n",
    "                ok_forms = (\"knows wise choose depends yours regards belongs gaze towards \"\n",
    "                            \"seize theirs raise hers grows afterwards ease enters happens \"\n",
    "                            \"begins contains sees surroundings feels opens becomes \"\n",
    "                            \"backwards ours upwards hangs williams praise deserves \"\n",
    "                            \"summons myles adams delawares\").split()\n",
    "                \n",
    "                step5_exclude[step5_exclude.index.str.endswith(\"'s\")] = False\n",
    "                bad_forms = (\"follows\").split()\n",
    "            elif post_divergence == (\"S\",):\n",
    "                print(\"TODO\")\n",
    "            elif post_divergence == (\"IH\", \"Z\"):\n",
    "                ok_forms = [\"missus\", \"produces\", \"riches\"]\n",
    "            elif post_divergence == (\"AH\", \"Z\"):\n",
    "                ok_forms = [\"produces\", \"riches\"]\n",
    "\n",
    "            step5_exclude.loc[ok_forms] = False\n",
    "            step5_exclude.loc[bad_forms] = True\n",
    "        elif study_inflection == \"VBD\":\n",
    "            ok_forms, bad_forms = [], []\n",
    "            if post_divergence == (\"D\",):\n",
    "                # bad_forms = (\"tied\").split()\n",
    "                pass\n",
    "            elif post_divergence == (\"T\",):\n",
    "                bad_forms = (\"missed passed massed guessed\").split()\n",
    "\n",
    "            try:\n",
    "                step5_exclude.loc[ok_forms] = False\n",
    "            except KeyError: pass\n",
    "\n",
    "            try:\n",
    "                step5_exclude.loc[bad_forms] = True\n",
    "            except KeyError: pass\n",
    "\n",
    "        step6 = step5.loc[~step5.index.get_level_values(\"label\").isin(step5_exclude[step5_exclude].index)]\n",
    "        print(post_divergence, len(step1), len(step2), len(step3), len(step4), len(step5), len(step6))\n",
    "        glob.append((study_inflection, post_divergence_str, step5, step5_exclude))\n",
    "\n",
    "        # Sample inflected forms\n",
    "        sampled_inflected_forms: pd.Series[str] = pd.Series(step6.unique())  # type: ignore\n",
    "        sampled_inflected_forms = sampled_inflected_forms.sample(min(num_word_samples, len(sampled_inflected_forms)))\n",
    "\n",
    "        for form in sampled_inflected_forms:\n",
    "            form_instances = step6[step6 == form].reset_index()\n",
    "            form_instances[\"label_idx\"] = form_instances.label.map({l: i for i, l in enumerate(state_space_spec.labels)})\n",
    "            \n",
    "            # Draw base forms\n",
    "            base_form = form[:-len(post_divergence_str)].strip()\n",
    "            base_instances = instances_by_phonemic_form.loc[[base_form]]\n",
    "\n",
    "            # Sample instances\n",
    "            num_instances = min(num_instance_samples, len(base_instances), len(form_instances))\n",
    "            base_instances = base_instances.sample(num_instances)\n",
    "            form_instances = form_instances.sample(num_instances)\n",
    "\n",
    "            all_samples = []\n",
    "            for (_, (base_row)), (_, (form_row)) in zip(base_instances.iterrows(), form_instances.iterrows()):\n",
    "                base_sample = trajectory_aggs[agg_method][base_row.label_idx][base_row.instance_idx].squeeze()\n",
    "                form_sample = trajectory_aggs[agg_method][form_row.label_idx][form_row.instance_idx].squeeze()\n",
    "\n",
    "                print(base_row.label, base_form, \"--\", form_row.label, form)\n",
    "                \n",
    "                # take difference matching earlier evaluation\n",
    "                all_samples.append(form_sample - base_sample)\n",
    "\n",
    "            counterfactual_divergence_vectors[post_divergence].append(np.mean(all_samples, axis=0))\n",
    "\n",
    "    counterfactual_divergence_vectors = {k: np.array(v) for k, v in counterfactual_divergence_vectors.items()}\n",
    "\n",
    "    study_divergence_contents = [k for k, _ in post_divergence_contents.most_common(num_divergence_contents)]\n",
    "    study_difference_difference_mat = np.zeros((len(study_divergence_contents), len(study_divergence_contents)))\n",
    "    for i, div1 in enumerate(study_divergence_contents):\n",
    "        for j, div2 in enumerate(study_divergence_contents):\n",
    "            study_difference_difference_mat[i, j] = cdist(study_vectors_flat[div1],\n",
    "                                                        study_vectors_flat[div2],\n",
    "                                                        metric=metric).mean()\n",
    "            \n",
    "    sns.heatmap(study_difference_difference_mat, annot=True,\n",
    "                xticklabels=study_divergence_contents, yticklabels=study_divergence_contents)\n",
    "    \n",
    "    control_study_results = []\n",
    "    for div in study_divergence_contents:\n",
    "        if div not in counterfactual_divergence_vectors:\n",
    "            # no counterfactual examples\n",
    "            continue\n",
    "        within_div = cdist(study_vectors_flat[div], study_vectors_flat[div], metric=metric).mean()\n",
    "        counterfactual_vectors = counterfactual_divergence_vectors[div]\n",
    "        between_div = cdist(study_vectors_flat[div], counterfactual_vectors, metric=metric).mean()\n",
    "\n",
    "        control_study_results.append({\"divergence\": \" \".join(div), \"within_div\": within_div, \"between_div\": between_div})\n",
    "\n",
    "    control_study_results_df = pd.DataFrame(control_study_results)\n",
    "    return control_study_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbd_control_df = run_counterfactual_study(\"VBD\", num_divergence_contents=5, ignore_tags=(\"VBD\", \"VBN\"),\n",
    "                         # DEV\n",
    "                         # num_word_samples=2, num_instance_samples=2\n",
    "                         num_word_samples=100, num_instance_samples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(data=vbd_control_df.melt(id_vars=[\"divergence\"]).query(\"divergence != 'OW Z'\"), x=\"divergence\", hue=\"variable\", y=\"value\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.set_xlabel(\"Inflection\")\n",
    "ax.set_ylabel(\"Cosine distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nns_control_df = run_counterfactual_study(\"NNS\", num_divergence_contents=4, ignore_tags=(\"NN\", \"NNS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(data=nns_control_df.melt(id_vars=[\"divergence\"]).query(\"divergence != 'OW Z'\"), x=\"divergence\", hue=\"variable\", y=\"value\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.set_xlabel(\"Inflection\")\n",
    "ax.set_ylabel(\"Cosine distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_inflection = \"VBD\"\n",
    "infl_results = broad_inflection_results.loc[study_inflection]\n",
    "infl_results = infl_results[[\"a\", \"b\"]]\n",
    "# infl_results = pd.concat([infl_results[[\"a\", \"b\"]],\n",
    "#                           infl_results[[\"c\", \"d\"]].rename(columns={\"c\": \"b\", \"d\": \"a\"})], axis=0)\n",
    "post_divergence_contents = Counter()\n",
    "post_divergence_map = {}\n",
    "for _, (inflected_form, base_form) in infl_results.iterrows():\n",
    "    base_cuts = cuts_df.loc[base_form]\n",
    "    inflected_cuts = cuts_df.loc[inflected_form]\n",
    "\n",
    "    # all attested phonological forms of base\n",
    "    base_phono_forms = set(base_cuts.groupby(\"instance_idx\").apply(\n",
    "        lambda xs: tuple(xs.description)))\n",
    "\n",
    "    post_divergence_i = Counter()\n",
    "    for _, inflected_instance in inflected_cuts.groupby(\"instance_idx\"):\n",
    "        # phonological divergence point: latest point at which the inflected form overlaps with\n",
    "        # any pronunciation of the base form\n",
    "        inflected_phones = tuple(inflected_instance.description)\n",
    "        phono_divergence_points = []\n",
    "        for base_phones in base_phono_forms:\n",
    "            for idx in range(len(inflected_phones) + 1):\n",
    "                if inflected_phones[:idx] != base_phones[:idx]:\n",
    "                    break\n",
    "            phono_divergence_points.append(idx - 1)\n",
    "        phono_divergence_point = max(phono_divergence_points)\n",
    "\n",
    "        if phono_divergence_point == 0:\n",
    "            continue\n",
    "        post_divergence_i[inflected_phones[phono_divergence_point:]] += 1\n",
    "\n",
    "    if len(post_divergence_i) == 0:\n",
    "        continue\n",
    "    post_divergence_map[base_form] = post_divergence_i\n",
    "    post_divergence_contents[post_divergence_i.most_common()[0][0]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_vectors = all_difference_vectors[study_inflection]\n",
    "study_vectors_flat = defaultdict(list)\n",
    "for dv in study_vectors:\n",
    "    try:\n",
    "        divergence, _ = post_divergence_map[dv[\"b\"]].most_common()[0]\n",
    "    except KeyError:\n",
    "        # no divergence data was stored\n",
    "        continue\n",
    "    study_vectors_flat[divergence].extend(dv[\"difference_vectors\"])\n",
    "\n",
    "study_vectors_flat = {k: np.array(v) for k, v in study_vectors_flat.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_divergence_contents.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_phonemic_forms = set(cut_phonemic_forms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_divergence_contents = 5\n",
    "num_word_samples = 50\n",
    "num_instance_samples = 3\n",
    "agg_method = \"mean\"\n",
    "\n",
    "counterfactual_divergence_vectors = defaultdict(list)\n",
    "for post_divergence, _ in tqdm(post_divergence_contents.most_common(num_divergence_contents)):\n",
    "    post_divergence_str = \" \".join(post_divergence)\n",
    "    # Find items which exist with the same relationship -D and +D\n",
    "    matching_ending = (cut_phonemic_forms.str[-len(post_divergence_str):] == post_divergence_str).groupby(\"label\").any()\n",
    "\n",
    "    # some good number of examples\n",
    "    start_labels = state_space_spec.label_counts[state_space_spec.label_counts > 10].index\n",
    "    step0 = cut_phonemic_forms.loc[start_labels]\n",
    "    # whole form has the same as the post-divergence content\n",
    "    step1 = step0.loc[(step0.str[-len(post_divergence_str):] == post_divergence_str).groupby(\"label\").transform(\"any\")]\n",
    "    # remove real VBD bases\n",
    "    step2 = step1.loc[~step1.index.get_level_values(\"label\").isin(inflection_results[\"VBD\"].keys())]\n",
    "    # remove real VBD inflections\n",
    "    step3 = step2.loc[~step2.index.get_level_values(\"label\").isin(list(itertools.chain.from_iterable(inflection_results[\"VBD\"].values())))]\n",
    "    # and if you strip off the post-divergence content, it's still attested\n",
    "    step4 = step3.loc[step3.str[:-len(post_divergence_str)].str.strip().isin(all_phonemic_forms)]\n",
    "    # these shouldn't be attested within the same orthographic form\n",
    "    # e.g. \"second\" has pronunciations as both \"S EH K AH N D\" and \"S EH K AH N\",\n",
    "    # this shouldn't count\n",
    "    step5 = step4.loc[step4.groupby(\"label\").transform(\n",
    "        lambda xs: len(np.intersect1d(xs.str[:-len(post_divergence_str)].str.strip(),\n",
    "                                    cut_phonemic_forms.loc[xs.name])) == 0)]\n",
    "    # exclude anything whose dominant POS is VBx\n",
    "    step5_labels = step5.index.get_level_values(\"label\").unique()\n",
    "    step5_is_vb = {label: nltk.pos_tag(label.split())[0][1].startswith(\"VB\") for label in step5_labels}\n",
    "    step5_is_vb = pd.Series(step5_is_vb)\n",
    "    step6 = step5.loc[~step5.index.get_level_values(\"label\").isin(step5_is_vb[step5_is_vb].index)]\n",
    "    print(post_divergence, len(step1), len(step2), len(step3), len(step4), len(step5), len(step6))\n",
    "\n",
    "    sampled_words = step6.index.get_level_values(\"label\").value_counts().head(num_word_samples).index\n",
    "    print(\"\\t\", \" \".join(sampled_words))\n",
    "    sampled_word_idxs = [state_space_spec.labels.index(w) for w in sampled_words]\n",
    "\n",
    "    for word in sampled_words:\n",
    "        word_idx = state_space_spec.labels.index(word)\n",
    "        word_instance_idxs = np.random.choice(min(max_samples_per_word, len(state_space_spec.target_frame_spans[word_idx])),\n",
    "                                              num_instance_samples, replace=False)\n",
    "        word_samples = [trajectory_aggs[agg_method][word_idx][instance_idx].squeeze()\n",
    "                        for instance_idx in word_instance_idxs]\n",
    "        counterfactual_divergence_vectors[post_divergence].extend(word_samples)\n",
    "\n",
    "counterfactual_divergence_vectors = {k: np.array(v) for k, v in counterfactual_divergence_vectors.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_divergence_contents = [k for k, _ in post_divergence_contents.most_common(5)]\n",
    "study_difference_difference_mat = np.zeros((len(study_divergence_contents), len(study_divergence_contents)))\n",
    "for i, div1 in enumerate(study_divergence_contents):\n",
    "    for j, div2 in enumerate(study_divergence_contents):\n",
    "        study_difference_difference_mat[i, j] = cdist(study_vectors_flat[div1],\n",
    "                                                      study_vectors_flat[div2],\n",
    "                                                      metric=metric).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(study_difference_difference_mat, annot=True,\n",
    "            xticklabels=study_divergence_contents, yticklabels=study_divergence_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_study_results = []\n",
    "for div in study_divergence_contents:\n",
    "    within_div = cdist(study_vectors_flat[div], study_vectors_flat[div], metric=metric).mean()\n",
    "    counterfactual_vectors = counterfactual_divergence_vectors[div]\n",
    "    between_div = cdist(study_vectors_flat[div], counterfactual_vectors, metric=metric).mean()\n",
    "\n",
    "    control_study_results.append({\"divergence\": div, \"within_div\": within_div, \"between_div\": between_div})\n",
    "\n",
    "control_study_results_df = pd.DataFrame(control_study_results)\n",
    "control_study_results_df.to_csv(f\"{output_dir}/control_study_results.csv\")\n",
    "control_study_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
