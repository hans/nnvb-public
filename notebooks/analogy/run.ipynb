{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.analysis import analogy\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec, \\\n",
    "    prepare_state_trajectory, flatten_trajectory\n",
    "from src.datasets.speech_equivalence import SpeechHiddenStateDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "base_model = \"w2v2_8\"\n",
    "\n",
    "model_class = \"discrim-rnn_32-mAP1\"\n",
    "model_name = \"word_broad_10frames_fixedlen25\"\n",
    "\n",
    "inflection_results_path = \"inflection_results.parquet\"\n",
    "# all_cross_instances_path = \"all_cross_instances.parquet\"\n",
    "all_cross_instances_path = \"outputs/analogy/inputs/librispeech-train-clean-100/w2v2/all_cross_instances.parquet\"\n",
    "most_common_allomorphs_path = \"most_common_allomorphs.csv\"\n",
    "false_friends_path = \"false_friends.csv\"\n",
    "\n",
    "train_dataset = \"librispeech-train-clean-100\"\n",
    "hidden_states_path = f\"outputs/hidden_states/{base_model}/{train_dataset}.h5\"\n",
    "state_space_specs_path = f\"state_space_spec.h5\"\n",
    "embeddings_path = f\"outputs/model_embeddings/{train_dataset}/{base_model}/{model_class}/{model_name}/{train_dataset}.npy\"\n",
    "\n",
    "output_dir = f\".\"\n",
    "\n",
    "pos_counts_path = \"data/pos_counts.pkl\"\n",
    "\n",
    "seed = 42\n",
    "\n",
    "metric = \"cosine\"\n",
    "\n",
    "agg_fns = [\n",
    "    \"mean\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general queries for all experiments to exclude special edge cases;\n",
    "# logic doesn't make sense in most experiments\n",
    "all_query = \"not exclude_main\"\n",
    "\n",
    "experiments = {\n",
    "    \"basic\": {\n",
    "        \"group_by\": [\"inflection\"],\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    \"regular\": {\n",
    "        \"group_by\": [\"inflection\", \"is_regular\"],\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    # \"NNS_to_VBZ\": {\n",
    "    #     \"base_query\": \"inflection == 'NNS' and is_regular\",\n",
    "    #     \"inflected_query\": \"inflection == 'VBZ' and is_regular\",\n",
    "    # },\n",
    "    # \"VBZ_to_NNS\": {\n",
    "    #     \"base_query\": \"inflection == 'VBZ' and is_regular\",\n",
    "    #     \"inflected_query\": \"inflection == 'NNS' and is_regular\",\n",
    "    # },\n",
    "    # \"regular_to_irregular\": {\n",
    "    #     \"group_by\": [\"inflection\"],\n",
    "    #     \"base_query\": \"is_regular == True\",\n",
    "    #     \"inflected_query\": \"is_regular == False\",\n",
    "    #     \"all_query\": all_query,\n",
    "    # },\n",
    "    # \"irregular_to_regular\": {\n",
    "    #     \"group_by\": [\"inflection\"],\n",
    "    #     \"base_query\": \"is_regular == False\",\n",
    "    #     \"inflected_query\": \"is_regular == True\",\n",
    "    #     \"all_query\": all_query,\n",
    "    # },\n",
    "    \"nn_vb_ambiguous\": {\n",
    "        \"group_by\": [\"inflection\", \"base_ambig_NN_VB\"],\n",
    "        \"base_query\": \"is_regular == True\",\n",
    "        \"inflected_query\": \"is_regular == True\",\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    \"random_to_NNS\": {\n",
    "        \"base_query\": \"inflection == 'random'\",\n",
    "        \"inflected_query\": \"inflection == 'NNS'\",\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    \"random_to_VBZ\": {\n",
    "        \"base_query\": \"inflection == 'random'\",\n",
    "        \"inflected_query\": \"inflection == 'VBZ'\",\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    \"false_friends\": {\n",
    "        \"all_query\": \"inflection.str.contains('FF')\",\n",
    "        \"group_by\": [\"inflection\"],\n",
    "        \"equivalence_keys\": [\"base\", \"inflected\", \"post_divergence\"],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO document\n",
    "study_unambiguous_transfer = [\"NNS\", \"VBZ\"]\n",
    "study_false_friends = [\"NNS\", \"VBZ\", \"VBD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_experiments = {\n",
    "    (\"Z\", \"S\"): {\n",
    "        \"source_inflections\": [\"VBZ\", \"NNS\"],\n",
    "    },\n",
    "    (\"D\", \"T\"): {\n",
    "        \"source_inflections\": [\"VBD\"],\n",
    "    },\n",
    "    (\"D\", \"IH D\"): {\n",
    "        \"source_inflections\": [\"VBD\"],\n",
    "    },\n",
    "    (\"T\", \"IH D\"): {\n",
    "        \"source_inflections\": [\"VBD\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare model representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if embeddings_path == \"ID\":\n",
    "    model_representations = SpeechHiddenStateDataset.from_hdf5(hidden_states_path).states\n",
    "else:\n",
    "    with open(embeddings_path, \"rb\") as f:\n",
    "        model_representations: np.ndarray = np.load(f)\n",
    "state_space_spec = StateSpaceAnalysisSpec.from_hdf5(state_space_specs_path)\n",
    "assert state_space_spec.is_compatible_with(model_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_agg = prepare_state_trajectory(model_representations, state_space_spec, \n",
    "                                          agg_fn_spec=\"mean\", agg_fn_dimension=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg, agg_src = flatten_trajectory(trajectory_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts_df = state_space_spec.cuts.xs(\"phoneme\", level=\"level\").drop(columns=[\"onset_frame_idx\", \"offset_frame_idx\"])\n",
    "cuts_df[\"label_idx\"] = cuts_df.index.get_level_values(\"label\").map({l: i for i, l in enumerate(state_space_spec.labels)})\n",
    "cuts_df[\"frame_idx\"] = cuts_df.groupby([\"label\", \"instance_idx\"]).cumcount()\n",
    "cuts_df = cuts_df.reset_index().set_index([\"label\", \"instance_idx\", \"frame_idx\"]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_phonemic_forms = cuts_df.groupby([\"label\", \"instance_idx\"]).description.agg(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_df = pd.read_csv(\"data/WorldLex_Eng_US.Freq.2.txt\", sep=\"\\t\", index_col=\"Word\")\n",
    "# compute weighted average frequency across domains\n",
    "word_freq_df[\"BlogFreq_rel\"] = word_freq_df.BlogFreq / word_freq_df.BlogFreq.sum()\n",
    "word_freq_df[\"TwitterFreq_rel\"] = word_freq_df.TwitterFreq / word_freq_df.TwitterFreq.sum()\n",
    "word_freq_df[\"NewsFreq_rel\"] = word_freq_df.NewsFreq / word_freq_df.NewsFreq.sum()\n",
    "word_freq_df[\"Freq\"] = word_freq_df[[\"BlogFreq\", \"TwitterFreq\", \"NewsFreq\"]].mean(axis=1) \\\n",
    "    * word_freq_df[[\"BlogFreq\", \"TwitterFreq\", \"NewsFreq\"]].sum().mean()\n",
    "word_freq_df[\"LogFreq\"] = np.log10(word_freq_df.Freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cross_instances = pd.read_parquet(all_cross_instances_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_results_df = pd.read_parquet(inflection_results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_allomorphs = pd.read_csv(most_common_allomorphs_path)\n",
    "false_friends_df = pd.read_csv(false_friends_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homophone preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "pron2label = defaultdict(set)\n",
    "for label, rows in cut_phonemic_forms.groupby(\"label\"):\n",
    "    for pron in set(rows):\n",
    "        pron2label[pron].add(label)\n",
    "\n",
    "homophone_map = defaultdict(set)\n",
    "for label_idx, label in enumerate(state_space_spec.labels):\n",
    "    for pron in set(cut_phonemic_forms.loc[label]):\n",
    "        homophone_map[label] |= pron2label[pron]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to exclude predictions of homophones from analogy evaluations.\n",
    "# create a map from label idx -> all label idxs which should be ignored.\n",
    "homophone_map = {state_space_spec.labels.index(label): {state_space_spec.labels.index(hom) for hom in homs}\n",
    "                 for label, homs in homophone_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavioral tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study 3 most frequent allomorphs of each inflection\n",
    "transfer_allomorphs = most_common_allomorphs.groupby(\"inflection\").most_common_allomorph.apply(lambda xs: xs.value_counts().head(3).index.tolist()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate experiments testing transfer from each of top allomorphs in NNS, VBZ\n",
    "# to each other\n",
    "for infl1, infl2 in itertools.product(study_unambiguous_transfer, repeat=2):\n",
    "    for allomorph1 in transfer_allomorphs[infl1]:\n",
    "        for allomorph2 in transfer_allomorphs[infl2]:\n",
    "            experiments[f\"unambiguous-{infl1}_{allomorph1}_to_{infl2}_{allomorph2}\"] = {\n",
    "                \"base_query\": f\"inflection == '{infl1}' and is_regular == True and base_ambig_NN_VB == False and post_divergence == '{allomorph1}'\",\n",
    "                \"inflected_query\": f\"inflection == '{infl2}' and is_regular == True and base_ambig_NN_VB == False and post_divergence == '{allomorph2}'\",\n",
    "                \"all_query\": all_query,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate experiments testing transfer from\n",
    "# 1. false friend allomorph to matching inflection allomorph\n",
    "# 2. false friend allomorph to non-matching inflection allomorph\n",
    "# 3. inflection allomorph to matching false friend allomorph\n",
    "# 4. inflection allomorph to non-matching false friend allomorph\n",
    "for (inflection, post_divergence), _ in false_friends_df.groupby([\"inflection\", \"post_divergence\"]):\n",
    "    if inflection not in study_false_friends:\n",
    "        continue\n",
    "    for transfer_allomorph in transfer_allomorphs[inflection]:\n",
    "        if inflection in [\"NNS\", \"VBZ\"]:\n",
    "            ambig_clause = \"base_ambig_NN_VB == {ambig} and \"\n",
    "        else:\n",
    "            ambig_clause = \"\"\n",
    "\n",
    "        ambig_positive = ambig_clause.format(ambig=\"True\")\n",
    "        ambig_negative = ambig_clause.format(ambig=\"False\")\n",
    "        experiments[f\"{inflection}-FF-{post_divergence}-to-{inflection}_{transfer_allomorph}\"] = {\n",
    "            \"base_query\": f\"inflection == '{inflection}-FF-{post_divergence}'\",\n",
    "            \"inflected_query\": f\"inflection == '{inflection}' and is_regular == True and {ambig_negative} post_divergence == '{transfer_allomorph}'\",\n",
    "        }\n",
    "        experiments[f\"{inflection}_{transfer_allomorph}-to-{inflection}-FF-{post_divergence}\"] = {\n",
    "            \"base_query\": f\"inflection == '{inflection}' and is_regular == True and {ambig_negative} post_divergence == '{transfer_allomorph}'\",\n",
    "            \"inflected_query\": f\"inflection == '{inflection}-FF-{post_divergence}'\",\n",
    "        }\n",
    "\n",
    "for inflection in study_false_friends:\n",
    "    for t1, t2 in itertools.combinations(transfer_allomorphs[inflection], 2):\n",
    "        experiments[f\"{inflection}-FF-{t1}-to-{inflection}-FF-{t2}\"] = {\n",
    "            \"base_query\": f\"inflection == '{inflection}-FF-{t1}'\",\n",
    "            \"inflected_query\": f\"inflection == '{inflection}-FF-{t2}'\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate experiments for forced-choice analysis\n",
    "fc_types = [infl for infl in all_cross_instances.inflection.unique() if infl.startswith(\"FC\")]\n",
    "fc_types = set(re.findall(r\"FC-([\\w\\s]+)_([\\w\\s]+)\", infl)[0] for infl in fc_types)\n",
    "\n",
    "for fc_pair, config in fc_experiments.items():\n",
    "    fc_pair_name = \"_\".join(fc_pair)\n",
    "    if fc_pair not in fc_types:\n",
    "        raise ValueError(f\"FC pair {fc_pair} not found in FC stimuli\")\n",
    "    \n",
    "    for source_inflection in config[\"source_inflections\"]:\n",
    "        for source_allomorph in transfer_allomorphs[source_inflection]:\n",
    "            experiments[f\"FC-{fc_pair_name}-from_{source_inflection}-{source_allomorph}\"] = {\n",
    "                \"base_query\": f\"inflection == '{source_inflection}' and post_divergence == '{source_allomorph}'\",\n",
    "                \"inflected_query\": f\"inflection == 'FC-{fc_pair_name}'\",\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment = \"unambiguous-NNS_Z_to_NNS_Z\"\n",
    "# config = experiments[experiment]\n",
    "# ret = analogy.run_experiment_equiv_level(\n",
    "#     experiment, config, state_space_spec, all_cross_instances,\n",
    "#     agg, agg_src,\n",
    "#     num_samples=20,\n",
    "#     device=\"cpu\",\n",
    "#     include_idxs_in_predictions=homophone_map,\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = pd.concat({\n",
    "    experiment: analogy.run_experiment_equiv_level(\n",
    "        experiment, config,\n",
    "        state_space_spec, all_cross_instances,\n",
    "        agg, agg_src,\n",
    "        num_samples=1000,\n",
    "        seed=seed,\n",
    "        device=\"cuda\")\n",
    "    for experiment, config in tqdm(experiments.items(), unit=\"experiment\")\n",
    "}, names=[\"experiment\"])\n",
    "experiment_results[\"correct\"] = experiment_results.predicted_label == experiment_results.gt_label\n",
    "experiment_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results.to_csv(f\"{output_dir}/experiment_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
