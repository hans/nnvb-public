{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare stimuli shared across all analogy evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import functools\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec\n",
    "from src.analysis import analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "state_space_specs_path = f\"outputs/state_space_specs/librispeech-train-clean-100/w2v2_8/state_space_specs.h5\"\n",
    "\n",
    "pos_counts_path = \"data/pos_counts.pkl\"\n",
    "output_dir = \".\"\n",
    "\n",
    "seed = 1234\n",
    "\n",
    "min_samples_per_word = 5\n",
    "max_samples_per_word = 100\n",
    "\n",
    "inflection_targets = [\n",
    "    \"VBD\",\n",
    "    \"VBZ\",\n",
    "    \"VBG\",\n",
    "    \"NNS\",\n",
    "    \"NOT-latin\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space_spec = StateSpaceAnalysisSpec.from_hdf5(state_space_specs_path, \"word\")\n",
    "state_space_spec = state_space_spec.subsample_instances(max_samples_per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pos_counts_path, \"rb\") as f:\n",
    "    pos_counts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts_df = state_space_spec.cuts.xs(\"phoneme\", level=\"level\").drop(columns=[\"onset_frame_idx\", \"offset_frame_idx\"])\n",
    "cuts_df[\"label_idx\"] = cuts_df.index.get_level_values(\"label\").map({l: i for i, l in enumerate(state_space_spec.labels)})\n",
    "cuts_df[\"frame_idx\"] = cuts_df.groupby([\"label\", \"instance_idx\"]).cumcount()\n",
    "cuts_df = cuts_df.reset_index().set_index([\"label\", \"instance_idx\", \"frame_idx\"]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_phonemic_forms = cuts_df.groupby([\"label\", \"instance_idx\"]).description.agg(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_spans = state_space_spec.target_frame_spans_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_nns_vbz_allomorph(base_phones):\n",
    "    \"\"\"\n",
    "    Given a list of CMUDICT phones for a base form, \n",
    "    return the 'expected' post-divergence allomorph \n",
    "    (S, Z, or IH Z, etc.) for the English plural / 3sg verb.\n",
    "    \"\"\"\n",
    "    last_phone = base_phones[-1]\n",
    "\n",
    "    # Define sets or lists for final-phoneme checks\n",
    "    SIBILANTS = {\"S\", \"Z\", \"SH\", \"CH\", \"JH\", \"ZH\"}\n",
    "    VOICELESS = {\"P\", \"T\", \"K\", \"F\", \"TH\"}  # Could add others as needed\n",
    "    \n",
    "    if last_phone in SIBILANTS:\n",
    "        # e.g., 'CH' -> \"IH Z\"\n",
    "        return \"IH Z\"\n",
    "    elif last_phone in VOICELESS:\n",
    "        # e.g., 'K', 'P', 'T' -> \"S\"\n",
    "        return \"S\"\n",
    "    else:\n",
    "        # default to voiced => \"Z\"\n",
    "        return \"Z\"\n",
    "\n",
    "\n",
    "def guess_past_allomorph(base_phones):\n",
    "    \"\"\"\n",
    "    Given a list of CMUDICT phones for a base form,\n",
    "    return the 'expected' post-divergence allomorph\n",
    "    (T, D, or IH D) for the English past tense.\n",
    "    \"\"\"\n",
    "    last_phone = base_phones[-1]\n",
    "\n",
    "    ALVEOLAR_STOPS = {\"T\", \"D\"}\n",
    "    # Example set of voiceless consonants (non-exhaustiveâ€”adjust as needed).\n",
    "    VOICELESS = {\"P\", \"F\", \"K\", \"S\", \"SH\", \"CH\", \"TH\"}\n",
    "    \n",
    "    if last_phone in ALVEOLAR_STOPS:\n",
    "        # E.g., \"want\" -> \"wanted\" => \"AH0 D\"\n",
    "        return \"IH D\"\n",
    "    elif last_phone in VOICELESS:\n",
    "        # E.g., \"jump\" -> \"jumped\" => \"T\"\n",
    "        return \"T\"\n",
    "    else:\n",
    "        # default to voiced => \"D\"\n",
    "        return \"D\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up main stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = state_space_spec.label_counts\n",
    "labels = set(labels[labels > min_samples_per_word].index)\n",
    "\n",
    "inflection_results_df = analogy.get_inflection_df(\n",
    "    inflection_targets, labels)\n",
    "inflection_results_df[\"base_idx\"] = inflection_results_df.base.map({l: i for i, l in enumerate(state_space_spec.labels)})\n",
    "inflection_results_df[\"inflected_idx\"] = inflection_results_df.inflected.map({l: i for i, l in enumerate(state_space_spec.labels)})\n",
    "inflection_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add on random word pair baseline\n",
    "num_random_word_pairs = inflection_results_df.groupby(\"inflection\").size().max()\n",
    "random_word_pairs = np.random.choice(len(list(labels)), size=(num_random_word_pairs, 2))\n",
    "random_word_pairs = pd.DataFrame(random_word_pairs, columns=[\"base_idx\", \"inflected_idx\"])\n",
    "random_word_pairs[\"base\"] = random_word_pairs.base_idx.map({i: l for i, l in enumerate(state_space_spec.labels)})\n",
    "random_word_pairs[\"inflected\"] = random_word_pairs.inflected_idx.map({i: l for i, l in enumerate(state_space_spec.labels)})\n",
    "random_word_pairs[\"is_regular\"] = False\n",
    "random_word_pairs[\"inflection\"] = \"random\"\n",
    "random_word_pairs = random_word_pairs.set_index(\"inflection\")\n",
    "random_word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_results_df = pd.concat([inflection_results_df, random_word_pairs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare token-level features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNS/VBZ ambiguity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_noun_ambiguous(row):\n",
    "    attested_pos = set(pos_counts[row.base].keys()) | set(pos_counts[row.inflected].keys())\n",
    "    return len(attested_pos & {\"VERB\"}) > 0\n",
    "inflection_results_df.loc[\"NNS\", \"base_ambig_NN_VB\"] = inflection_results_df.loc[\"NNS\"].apply(is_noun_ambiguous, axis=1)\n",
    "# inflection_results_df.loc[\"NNS\"].groupby(\"base_ambig_NN_VB\").sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_verb_ambiguous(row):\n",
    "    attested_pos = set(pos_counts[row.base].keys()) | set(pos_counts[row.inflected].keys())\n",
    "    return len(attested_pos & {\"NOUN\"}) > 0\n",
    "inflection_results_df.loc[\"VBZ\", \"base_ambig_NN_VB\"] = inflection_results_df.loc[\"VBZ\"].apply(is_verb_ambiguous, axis=1)\n",
    "# inflection_results_df.loc[\"VBZ\"].groupby(\"base_ambig_NN_VB\").sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-divergence analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache\n",
    "def _get_base_forms(base_label: str) -> frozenset[tuple[str, ...]]:\n",
    "    base_cuts = cuts_df.loc[base_label]\n",
    "    base_phon_forms = frozenset(base_cuts.groupby(\"instance_idx\").apply(\n",
    "        lambda xs: tuple(xs.description)))\n",
    "    return base_phon_forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache\n",
    "def _get_phonological_divergence(base_forms: frozenset[tuple[str, ...]],\n",
    "                                 inflected_form: tuple[str, ...]) -> tuple[int, tuple[str, ...]]:\n",
    "    phono_divergence_points = []\n",
    "    for base_phones in base_forms:\n",
    "        for idx in range(len(inflected_form) + 1):\n",
    "            if inflected_form[:idx] != base_phones[:idx]:\n",
    "                break\n",
    "        phono_divergence_points.append(idx - 1)\n",
    "    phono_divergence_point = max(phono_divergence_points)\n",
    "\n",
    "    post_divergence = inflected_form[phono_divergence_point:]\n",
    "    return phono_divergence_point, post_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phonological_divergence(base_label, inflected_label, inflected_instance_idx):\n",
    "    try:\n",
    "        base_phon_forms = _get_base_forms(base_label)\n",
    "        inflected_phones = tuple(cuts_df.loc[inflected_label].loc[inflected_instance_idx].description)\n",
    "    except KeyError:\n",
    "        return Counter()\n",
    "\n",
    "    div_point, div_content = _get_phonological_divergence(base_phon_forms, inflected_phones)\n",
    "    return inflected_phones, div_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_instances = []\n",
    "\n",
    "for inflection, row in tqdm(inflection_results_df.iterrows(), total=len(inflection_results_df)):\n",
    "    inflected_instance_idxs = ss_spans.query(f\"label == @row.inflected\").instance_idx\n",
    "    for inflected_instance_idx in inflected_instance_idxs:\n",
    "        inflected_phones, post_divergence = \\\n",
    "            get_phonological_divergence(row.base, row.inflected, inflected_instance_idx)\n",
    "        \n",
    "        inflected_phones = \" \".join(inflected_phones)\n",
    "        post_divergence = \" \".join(post_divergence)\n",
    "        inflection_instances.append({\n",
    "            \"inflection\": inflection,\n",
    "            \"base\": row.base,\n",
    "            \"inflected\": row.inflected,\n",
    "            \"inflected_instance_idx\": inflected_instance_idx,\n",
    "            \"inflected_phones\": inflected_phones,\n",
    "            \"post_divergence\": post_divergence,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_instance_df = pd.DataFrame(inflection_instances)\n",
    "\n",
    "# Now merge with type-level information.\n",
    "inflection_instance_df = pd.merge(inflection_instance_df,\n",
    "                                  inflection_results_df.reset_index(),\n",
    "                                  how=\"left\",\n",
    "                                  on=[\"inflection\", \"base\", \"inflected\"])\n",
    "inflection_instance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute most frequent allomorph of each inflection\n",
    "most_common_allomorphs = inflection_instance_df.groupby([\"inflection\", \"base\"]).post_divergence \\\n",
    "    .apply(lambda xs: xs.value_counts().idxmax()) \\\n",
    "    .rename(\"most_common_allomorph\").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build full cross product of stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_cross_instances = []\n",
    "base_cross_instances = []\n",
    "\n",
    "for inflection, row in tqdm(inflection_results_df.iterrows(), total=len(inflection_results_df)):\n",
    "    inflected_instance_idxs = ss_spans.query(f\"label == @row.inflected\").instance_idx\n",
    "    inflected_forms = cut_phonemic_forms.loc[row.inflected]\n",
    "    for inflected_instance_idx in inflected_instance_idxs:\n",
    "        inflection_cross_instances.append({\n",
    "            \"inflection\": inflection,\n",
    "            \"base\": row.base,\n",
    "            \"inflected\": row.inflected,\n",
    "            \"inflected_instance_idx\": inflected_instance_idx,\n",
    "            \"inflected_phones\": inflected_forms.loc[inflected_instance_idx]\n",
    "        })\n",
    "\n",
    "    base_instance_idxs = ss_spans.query(f\"label == @row.base\").instance_idx\n",
    "    base_forms = cut_phonemic_forms.loc[row.base]\n",
    "    for base_instance_idx in base_instance_idxs:\n",
    "        base_cross_instances.append({\n",
    "            \"inflection\": inflection,\n",
    "            \"base\": row.base,\n",
    "            \"inflected\": row.inflected,\n",
    "            \"base_instance_idx\": base_instance_idx,\n",
    "            \"base_phones\": base_forms.loc[base_instance_idx]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in post-divergence information\n",
    "inflection_cross_instances_df = pd.DataFrame(inflection_cross_instances)\n",
    "merge_on = [\"inflection\", \"base\", \"inflected\", \"inflected_instance_idx\"]\n",
    "inflection_cross_instances_df = pd.merge(inflection_cross_instances_df,\n",
    "                                         inflection_instance_df[merge_on + [\"post_divergence\"]],\n",
    "                                         on=merge_on)\n",
    "\n",
    "all_cross_instances = pd.merge(pd.DataFrame(base_cross_instances),\n",
    "         inflection_cross_instances_df,\n",
    "         on=[\"inflection\", \"base\", \"inflected\"],\n",
    "         how=\"outer\")\n",
    "\n",
    "# Now merge with type-level information.\n",
    "all_cross_instances = pd.merge(inflection_results_df.reset_index(),\n",
    "                               all_cross_instances,\n",
    "                               on=[\"inflection\", \"base\", \"inflected\"],\n",
    "                               validate=\"1:m\")\n",
    "\n",
    "all_cross_instances[\"exclude_main\"] = False\n",
    "all_cross_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forced-choice experiment materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forced_choice_cross_instances(fc_pair: tuple[str, str], allomorph_guesser,\n",
    "                                      min_frequency=2):\n",
    "    \"\"\"\n",
    "    The \"forced choice\" experiment asks whether a model prefers to make predictions\n",
    "    which are consistent with an allomorphy structure or not.\n",
    "\n",
    "    For example, allomorphs of the plural morpheme in English can be either /s/ /z/\n",
    "    or /Iz/ depending on the final phoneme of the base form. `allomorph_guesser`\n",
    "    specifies this allomorphy rule for the given request.\n",
    "\n",
    "    Parameters:\n",
    "    - fc_pair: pair of phoneme strings (space-sparated CMUDICT phonemes) which\n",
    "        form the forced choice pair\n",
    "    - allomorph_guesser: function taking a list of CMUDICT phonemes and returning\n",
    "        the appropriate allomorph for the forced choice setup\n",
    "    - min_frequency: minimum number of instances required for a given pair to be\n",
    "        included in the output\n",
    "    \"\"\"\n",
    "\n",
    "    label_counts = cut_phonemic_forms.groupby(\"label\").size()\n",
    "\n",
    "    step0 = cut_phonemic_forms.loc[cut_phonemic_forms.str[-len(fc_pair[0]):] == fc_pair[0]]\n",
    "    # if you remove post-div content, it's still attested\n",
    "    step1 = step0.loc[step0.str[:-len(fc_pair[0])].str.strip().isin(cut_phonemic_forms)]\n",
    "    # and the alternative post-div is also attested\n",
    "    step2 = step1.loc[(step1.str[:-len(fc_pair[0])].str.strip() + (\" \" + fc_pair[1])).isin(cut_phonemic_forms)]\n",
    "    step2 = step2.reset_index()\n",
    "\n",
    "    def get_label0(description):\n",
    "        candidates = cut_phonemic_forms[cut_phonemic_forms == description[:-len(fc_pair[0])].strip()].reset_index()\n",
    "        return candidates.groupby([\"label\", \"description\"]).size().index[0]\n",
    "    def get_label2(description):\n",
    "        candidates = cut_phonemic_forms[cut_phonemic_forms == description[:-len(fc_pair[0])].strip() + \" \" + fc_pair[1]].reset_index()\n",
    "        return candidates.groupby([\"label\", \"description\"]).size().index[0]\n",
    "    step2_label0 = {description: get_label0(description) for description in step2.description.unique()}\n",
    "    step2_label2 = {description: get_label2(description) for description in step2.description.unique()}\n",
    "    step2[\"inferred_label0\"] = step2.description.map({desc: label for desc, (label, _) in step2_label0.items()})\n",
    "    step2[\"inferred_form0\"] = step2.description.map({desc: form for desc, (_, form) in step2_label0.items()})\n",
    "    step2[\"inferred_label2\"] = step2.description.map({desc: label for desc, (label, _) in step2_label2.items()})\n",
    "    step2[\"inferred_form2\"] = step2.description.map({desc: form for desc, (_, form) in step2_label2.items()})\n",
    "\n",
    "    # ignore where label2 == label\n",
    "    step3 = step2.loc[step2.inferred_label2 != step2.label]\n",
    "\n",
    "    # filter by frequency\n",
    "    step4 = pd.merge(step3, label_counts.rename(\"label_count\"),\n",
    "                    left_on=\"label\", right_index=True, how=\"inner\")\n",
    "    step4 = pd.merge(step4, label_counts.rename(\"label0_count\"),\n",
    "                    left_on=\"inferred_label0\", right_index=True, how=\"inner\")\n",
    "    step4 = pd.merge(step4, label_counts.rename(\"label2_count\"),\n",
    "                    left_on=\"inferred_label2\", right_index=True, how=\"inner\")\n",
    "    step4 = step4[(step4.label_count >= min_frequency)\n",
    "                  & (step4.label0_count >= min_frequency)\n",
    "                  & (step4.label2_count >= min_frequency)]\n",
    "    \n",
    "    step4 = step4.rename(columns={\n",
    "        \"label\": \"label1\",\n",
    "        \"inferred_label0\": \"label0\",\n",
    "        \"inferred_label2\": \"label2\",\n",
    "\n",
    "        \"description\": \"form1\",\n",
    "        \"inferred_form0\": \"form0\",\n",
    "        \"inferred_form2\": \"form2\",\n",
    "\n",
    "        \"instance_idx\": \"instance_idx1\",\n",
    "    }).drop(columns=[\"label0_count\", \"label_count\", \"label2_count\"])\n",
    "    \n",
    "    # retrieve all instances of the variants\n",
    "    fc_cross = pd.merge(\n",
    "        step4,\n",
    "        cut_phonemic_forms.reset_index().rename(\n",
    "            columns={\"label\": \"label0\", \"description\": \"form0\",\n",
    "                     \"instance_idx\": \"instance_idx0\"}),\n",
    "        on=[\"label0\", \"form0\"], how=\"left\")\n",
    "    fc_cross = pd.merge(\n",
    "        fc_cross,\n",
    "        cut_phonemic_forms.reset_index().rename(\n",
    "            columns={\"label\": \"label2\", \"description\": \"form2\",\n",
    "                     \"instance_idx\": \"instance_idx2\"}),\n",
    "        on=[\"label2\", \"form2\"], how=\"left\")\n",
    "    \n",
    "    fc_cross = fc_cross[[\"label0\", \"label1\", \"label2\",\n",
    "                         \"form0\", \"form1\", \"form2\",\n",
    "                         \"instance_idx0\", \"instance_idx1\", \"instance_idx2\"]]\n",
    "    \n",
    "    # now prepare a single flat structure relating base (label0) to inflected\n",
    "    # (label1 w.l.o.g.)\n",
    "    # we will record frequency of inflection to label1 vs label2\n",
    "    fc_cross = fc_cross.rename(columns={\n",
    "        \"label0\": \"base\",\n",
    "        \"label1\": \"inflected\",\n",
    "        \"label2\": \"inflected2\",\n",
    "        \n",
    "        \"form0\": \"base_phones\",\n",
    "        \"form1\": \"inflected_phones\",\n",
    "        \"form2\": \"inflected2_phones\",\n",
    "\n",
    "        \"instance_idx0\": \"base_instance_idx\",\n",
    "        \"instance_idx1\": \"inflected_instance_idx\",\n",
    "    }).drop(columns=[\"instance_idx2\"])\n",
    "    fc_cross[\"inflection\"] = \"FC-\" + \"_\".join(fc_pair)\n",
    "    fc_cross[\"post_divergence\"] = fc_pair[0]\n",
    "    allomorph_map = {base_phones: allomorph_guesser(base_phones.split()) for base_phones in fc_cross.base_phones.unique()}\n",
    "    fc_cross[\"strong\"] = fc_cross.base_phones.map(allomorph_map) == fc_pair[0]\n",
    "\n",
    "    fc_cross[\"base_idx\"] = fc_cross.base.map({l: i for i, l in enumerate(state_space_spec.labels)})\n",
    "    fc_cross[\"inflected_idx\"] = fc_cross.inflected.map({l: i for i, l in enumerate(state_space_spec.labels)})\n",
    "    fc_cross[\"exclude_main\"] = True\n",
    "    \n",
    "    return fc_cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_pairs = [((\"Z\", \"S\"), guess_nns_vbz_allomorph),\n",
    "            ((\"Z\", \"IH Z\"), guess_nns_vbz_allomorph),\n",
    "            ((\"S\", \"IH Z\"), guess_nns_vbz_allomorph),\n",
    "            \n",
    "            ((\"D\", \"T\"), guess_past_allomorph),\n",
    "            ((\"D\", \"IH D\"), guess_past_allomorph),\n",
    "            ((\"T\", \"IH D\"), guess_past_allomorph),]\n",
    "\n",
    "fc_cross_instances = pd.concat([get_forced_choice_cross_instances(*fc_pair) for fc_pair in fc_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cross_instances = pd.concat([all_cross_instances, fc_cross_instances], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False friend production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_false_friends():\n",
    "    false_friends_dfs = {}\n",
    "    inflection_allomorph_grouper = most_common_allomorphs \\\n",
    "        [~most_common_allomorphs.inflection.isin((\"random\", \"NOT-latin\"))] \\\n",
    "        .groupby(\"inflection\").most_common_allomorph \\\n",
    "        .apply(lambda xs: xs.value_counts()[:3]).index\n",
    "    for inflection, post_divergence in tqdm(inflection_allomorph_grouper):\n",
    "        avoid_inflections = {\"POS\", inflection}\n",
    "        if inflection == \"NNS\":\n",
    "            avoid_inflections.add(\"VBZ\")\n",
    "        elif inflection == \"VBZ\":\n",
    "            avoid_inflections.add(\"NNS\")\n",
    "        avoid_inflections = list(avoid_inflections)\n",
    "\n",
    "        try:\n",
    "            false_friends_dfs[inflection, post_divergence] = \\\n",
    "                analogy.prepare_false_friends(\n",
    "                    inflection_results_df,\n",
    "                    inflection_instance_df,\n",
    "                    cut_phonemic_forms,\n",
    "                    post_divergence,\n",
    "                    avoid_inflections=avoid_inflections)\n",
    "        except:\n",
    "            print(\"Failed for\", inflection, post_divergence)\n",
    "            continue\n",
    "\n",
    "    return false_friends_dfs\n",
    "\n",
    "false_friends_dfs = compute_false_friends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_friends_df = pd.concat(false_friends_dfs, names=[\"inflection\", \"post_divergence\"]).droplevel(-1)\n",
    "\n",
    "# manually exclude some cases that don't get filtered out, often just because they're too\n",
    "# low frequency for both true base and inflected form to appear\n",
    "\n",
    "# share exclusion list for NNS and VBZ since we have experiments relating these two\n",
    "# so this is any false-friend for which their is a phonologically identical \"base\"\n",
    "# that could instantiate a VBZ or NNS inflection\n",
    "exclude_NNS_VBZ = (\"adds americans arabs assyrians berries carlyle's childs christians \"\n",
    "                   \"counties cruise dares dealings delawares europeans excellencies \"\n",
    "                   \"fins fours galleries gaze germans indians isles maids mary's negroes \"\n",
    "                   \"nuns peas phrase pyes reflections rodgers romans russians simpsons \"\n",
    "                   \"spaniards sundays vickers weeds wigwams williams \"\n",
    "                   \"jews odds news hose dis yes ice cease peace s us \"\n",
    "                    \n",
    "                   \"greeks lapse mix philips trunks its \"\n",
    "                    \n",
    "                   \"breeches occurrences personages\").split()\n",
    "false_friends_manual_exclude = {\n",
    "    \"NNS\": exclude_NNS_VBZ,\n",
    "    \"VBZ\": exclude_NNS_VBZ,\n",
    "    \"VBD\": (\"armored bald bard counseled crude dared enquired healed knowed legged \"\n",
    "            \"mourned natured renowned rude second ward wild willed withered hauled \"\n",
    "\n",
    "            \"tract wrapped fitted hearted heralded intrusted knitted wretched\").split(),\n",
    "    \"VBG\": (\"ceiling daring fleeting morning roaming wasting weaving weighing \"\n",
    "            \"whining willing chuckling kneeling sparkling startling\").split()\n",
    "}\n",
    "\n",
    "false_friends_df = false_friends_df.groupby(\"inflection\", as_index=False).apply(\n",
    "    lambda xs: xs[~xs.inflected.isin(false_friends_manual_exclude.get(xs.name, []))]).droplevel(0)\n",
    "\n",
    "# exclude the (quite interesting) cases where the \"base\" and \"inflected\" form are\n",
    "# actually orthographically matched, and we're seeing the divergence due to a pronunciation\n",
    "# variant (e.g. don't as D OW N vs D O WN T)\n",
    "false_friends_df = false_friends_df[false_friends_df.base != false_friends_df.inflected]\n",
    "\n",
    "false_friends_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_friends_df.loc[[\"NNS\", \"VBZ\"], \"strong_expected\"] = false_friends_df.loc[[\"NNS\", \"VBZ\"]].apply(lambda xs: guess_nns_vbz_allomorph(xs.base_form.split(\" \")), axis=1)\n",
    "false_friends_df.loc[[\"VBD\"], \"strong_expected\"] = false_friends_df.loc[[\"VBD\"]].apply(lambda xs: guess_past_allomorph(xs.base_form.split(\" \")), axis=1)\n",
    "false_friends_df[\"strong\"] = false_friends_df.index.get_level_values(\"post_divergence\") == false_friends_df.strong_expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare false-friends cross product and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_false_friends_df = pd.merge(false_friends_df.reset_index(),\n",
    "         cut_phonemic_forms.reset_index().rename(\n",
    "             columns={\"label\": \"base\", \"description\": \"base_form\",\n",
    "                      \"instance_idx\": \"base_instance_idx\"}),\n",
    "         on=[\"base\", \"base_form\"], how=\"left\")\n",
    "cross_false_friends_df = pd.merge(cross_false_friends_df,\n",
    "         cut_phonemic_forms.reset_index().rename(\n",
    "             columns={\"label\": \"inflected\", \"description\": \"inflected_form\",\n",
    "                      \"instance_idx\": \"inflected_instance_idx\"}),\n",
    "         on=[\"inflected\", \"inflected_form\"], how=\"left\")\n",
    "\n",
    "# update to match all_cross_instances schema\n",
    "cross_false_friends_df = cross_false_friends_df.rename(\n",
    "    columns={\"base_form\": \"base_phones\",\n",
    "             \"inflected_form\": \"inflected_phones\"})\n",
    "cross_false_friends_df[\"base_idx\"] = cross_false_friends_df.base.map({l: i for i, l in enumerate(state_space_spec.labels)})\n",
    "cross_false_friends_df[\"inflected_idx\"] = cross_false_friends_df.inflected.map({l: i for i, l in enumerate(state_space_spec.labels)})\n",
    "cross_false_friends_df[\"is_regular\"] = True\n",
    "\n",
    "cross_false_friends_df[\"inflection\"] = (cross_false_friends_df.inflection + \"-FF-\").str.cat(cross_false_friends_df.post_divergence, sep=\"\")\n",
    "cross_false_friends_df[\"exclude_main\"] = True\n",
    "cross_false_friends_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cross_instances = pd.concat([all_cross_instances, cross_false_friends_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space_spec.to_hdf5(f\"{output_dir}/state_space_spec.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_results_df.to_parquet(f\"{output_dir}/inflection_results.parquet\")\n",
    "inflection_instance_df.to_parquet(f\"{output_dir}/inflection_instances.parquet\")\n",
    "all_cross_instances.to_parquet(f\"{output_dir}/all_cross_instances.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_friends_df.to_csv(f\"{output_dir}/false_friends.csv\")\n",
    "most_common_allomorphs.to_csv(f\"{output_dir}/most_common_allomorphs.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
