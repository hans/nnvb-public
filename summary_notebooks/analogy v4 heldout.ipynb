{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import logit, ols\n",
    "\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec\n",
    "from src.utils import concat_csv_with_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"paper\", font_scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_friends_path = \"outputs/analogy/inputs/librispeech-test-clean/w2v2/false_friends.csv\"\n",
    "state_space_path = \"outputs/analogy/inputs/librispeech-test-clean/w2v2/state_space_spec.h5\"\n",
    "most_common_allomorphs_path = \"outputs/analogy/inputs/librispeech-test-clean/w2v2/most_common_allomorphs.csv\"\n",
    "cross_instances_path = \"outputs/analogy/inputs/librispeech-test-clean/w2v2/all_cross_instances.parquet\"\n",
    "output_dir = \"analogy_figures_heldout\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping variables on experiment results dataframe to select a single run\n",
    "run_groupers = [\"base_model_name\", \"model_name\", \"equivalence\"]\n",
    "\n",
    "# plot_runs = [(f\"w2v2_{i}\", \"ff_32\", \"word_broad_10frames_fixedlen25\") for i in range(12)] + \\\n",
    "#             [(f\"w2v2_{i}\", \"discrim-ff_32\", \"word_broad_10frames_fixedlen25\") for i in range(12)] + \\\n",
    "#             [(f\"w2v2_{i}\", \"id\", \"id\") for i in range(12)] + \\\n",
    "#             [(f\"w2v2_{i}\", \"ffff_32\", \"word_broad_10frames_fixedlen25\") for i in range(12)]\n",
    "plot_runs = [(f\"w2v2_{i}\", \"ffff_32\", \"word_broad_10frames_fixedlen25\") for i in range(12)] + \\\n",
    "             [(f\"w2v2_{i}\", \"id\", \"id\") for i in range(12)]\n",
    "# plot_runs = [(\"w2v2_8\", \"ff_32\", \"word_broad_10frames_fixedlen25\"),]\n",
    "\n",
    "main_plot_run = (\"w2v2_8\", \"ffff_32\", \"word_broad_10frames_fixedlen25\")\n",
    "foil_plot_run = (\"w2v2_8\", \"id\", \"id\")\n",
    "main_plot_name = \"Word\"\n",
    "foil_plot_name = \"Wav2Vec\"\n",
    "# main_plot_run = (\"w2v2_8\", \"discrim-ff_32\", \"word_broad_10frames_fixedlen25\")\n",
    "# choose a vmin, vmax so that all heatmaps have the same color scale\n",
    "main_plot_vmin, main_plot_vmax = 0.4, 0.9\n",
    "\n",
    "plot_inflections = [\"NNS\", \"VBZ\"]\n",
    "plot_metrics = [\"correct\", \"gt_label_rank\", \"gt_distance\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_df = pd.read_csv(\"data/WorldLex_Eng_US.Freq.2.txt\", sep=\"\\t\", index_col=\"Word\")\n",
    "# compute weighted average frequency across domains\n",
    "word_freq_df[\"BlogFreq_rel\"] = word_freq_df.BlogFreq / word_freq_df.BlogFreq.sum()\n",
    "word_freq_df[\"TwitterFreq_rel\"] = word_freq_df.TwitterFreq / word_freq_df.TwitterFreq.sum()\n",
    "word_freq_df[\"NewsFreq_rel\"] = word_freq_df.NewsFreq / word_freq_df.NewsFreq.sum()\n",
    "word_freq_df[\"Freq\"] = word_freq_df[[\"BlogFreq_rel\", \"TwitterFreq_rel\", \"NewsFreq_rel\"]].mean(axis=1) \\\n",
    "    * word_freq_df[[\"BlogFreq\", \"TwitterFreq\", \"NewsFreq\"]].sum().mean()\n",
    "word_freq_df[\"LogFreq\"] = np.log10(word_freq_df.Freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_friends_df = pd.read_csv(false_friends_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_allomorphs = pd.read_csv(most_common_allomorphs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cross_instances = pd.read_parquet(cross_instances_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = concat_csv_with_indices(\n",
    "        \"outputs/analogy/runs/**/librispeech-test-clean/experiment_results.csv\",\n",
    "        [lambda p: p.parents[1].name, lambda p: p.parents[2].name,\n",
    "            lambda p: p.parents[3].name],\n",
    "        [\"equivalence\", \"model_name\", \"base_model_name\"]) \\\n",
    "    .droplevel(-1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_id_results = concat_csv_with_indices(\n",
    "        \"outputs/analogy/runs_id/librispeech-test-clean/**/experiment_results.csv\",\n",
    "        [lambda p: p.parents[0].name],\n",
    "        [\"base_model_name\"]) \\\n",
    "    .droplevel(-1).reset_index()\n",
    "all_id_results[\"model_name\"] = \"id\"\n",
    "all_id_results[\"equivalence\"] = \"id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.concat([all_results, all_id_results], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[\"group\"] = all_results.group.apply(lambda x: eval(x) if not (isinstance(x, float) and np.isnan(x)) else None)\n",
    "all_results = all_results.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post-hoc exclude some words which have base/inflected forms which are homophonous with items of another category\n",
    "exclusions = [\n",
    "    (\"VBZ\", \"know\"), # nose\n",
    "    (\"VBZ\", \"seem\"), # seam\n",
    "    (\"VBZ\", \"please\"), # pleas\n",
    "    (\"VBZ\", \"write\"), # right\n",
    "    (\"VBZ\", \"meet\"), # meat\n",
    "    (\"VBZ\", \"read\"), # reed\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anti join\n",
    "all_results = all_results.merge(pd.DataFrame(exclusions, columns=[\"inflection_from\", \"base_from\"]),\n",
    "                  on=[\"inflection_from\", \"base_from\"], how=\"left\", indicator=True) \\\n",
    "    .query(\"_merge == 'left_only'\").drop(columns=\"_merge\") \\\n",
    "    .merge(pd.DataFrame(exclusions, columns=[\"inflection_to\", \"base_to\"]),\n",
    "           on=[\"inflection_to\", \"base_to\"], how=\"left\", indicator=True) \\\n",
    "    .query(\"_merge == 'left_only'\").drop(columns=\"_merge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lw = all_results.query(\"experiment == 'regular'\").copy()\n",
    "plot_lw = plot_lw.groupby(run_groupers + [\"group\", \"inflection_from\"])[[\"correct\", \"gt_label_rank\", \"gt_distance\"]].mean()\n",
    "plot_lw = plot_lw.reindex([(*plot_run, group, inflection_from)\n",
    "                           for group in plot_lw.index.get_level_values(\"group\").unique()\n",
    "                           for inflection_from in plot_lw.index.get_level_values(\"inflection_from\").unique()\n",
    "                           for plot_run in plot_runs]).reset_index()\n",
    "plot_lw[\"group0\"] = plot_lw.group.apply(lambda x: x[0] if x is not None else None)\n",
    "plot_lw[\"group1\"] = plot_lw.group.apply(lambda x: x[1] if x is not None else None)\n",
    "plot_lw[\"layer\"] = plot_lw.base_model_name.str.extract(r\"_(\\d+)$\").astype(int) + 1\n",
    "plot_lw[\"model_name\"] = plot_lw[\"model_name\"].map({\"id\": \"Wav2Vec\", main_plot_run[1]: \"Word\"})\n",
    "\n",
    "lw_random = plot_lw[plot_lw.group0 == \"random\"].groupby([\"inflection_from\", \"layer\"])[[\"correct\", \"gt_label_rank\"]].mean().reset_index().dropna()\n",
    "\n",
    "plot_lw = plot_lw[plot_lw.inflection_from.isin(plot_inflections)]\n",
    "plot_lw = plot_lw[(plot_lw.group1 == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(data=plot_lw, x=\"layer\", y=\"correct\", hue=\"model_name\", row=\"inflection_from\",\n",
    "                kind=\"point\", height=3, aspect=3)\n",
    "\n",
    "for ax, row_name in zip(g.axes.flat, g.row_names):\n",
    "    sns.lineplot(data=lw_random,\n",
    "                 x=\"layer\", y=\"correct\", ax=ax, color=\"gray\", linestyle=\"--\",\n",
    "                 legend=None)\n",
    "    ax.set_title(ax.get_title().split(\"=\")[1].strip())\n",
    "    ax.set_ylabel(\"Correct\")\n",
    "    if ax.get_xlabel() == \"layer\":\n",
    "        ax.set_xlabel(\"Layer\")\n",
    "\n",
    "# g.figure.tight_layout()\n",
    "g.figure.savefig(f\"{output_dir}/layer_wise.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hue_order = [\"Word\", \"Wav2Vec\"]\n",
    "g = sns.catplot(data=plot_lw, x=\"layer\", y=\"gt_label_rank\",\n",
    "                hue=\"model_name\", hue_order=hue_order,\n",
    "                row=\"inflection_from\", row_order=plot_inflections,\n",
    "                kind=\"point\", sharey=False, height=3, aspect=1.25)\n",
    "\n",
    "for (row, col, hue), data in g.facet_data():\n",
    "    ax = g.axes[row, col]\n",
    "    ax.set_title(ax.get_title().split(\"=\")[1].strip())\n",
    "    ax.set_ylabel(\"Rank\")\n",
    "    if ax.get_xlabel() == \"layer\":\n",
    "        ax.set_xlabel(\"Layer\")\n",
    "\n",
    "    # Add an inset showing the middle layers\n",
    "    inset_ax = ax.inset_axes([0.45, 0.45, 0.4, 0.5])\n",
    "    sns.pointplot(data=data[data.layer.between(7, 9)], x=\"layer\", y=\"gt_label_rank\",\n",
    "                  hue=\"model_name\", hue_order=hue_order,\n",
    "                  ax=inset_ax, legend=None)\n",
    "    inset_ax.set_xlabel(None)\n",
    "    inset_ax.set_ylabel(None)\n",
    "    for spine in inset_ax.spines.values():\n",
    "        spine.set_edgecolor(\"gray\")\n",
    "    inset_ax.tick_params(color=\"gray\")\n",
    "\n",
    "g.legend.remove()\n",
    "g.add_legend(title=\"Model\", bbox_to_anchor=(0.19, 0.99), loc=\"upper left\")\n",
    "g.figure.tight_layout()\n",
    "g.figure.savefig(f\"{output_dir}/layer_wise-rank.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(data=plot_lw, x=\"layer\", y=\"gt_distance\", hue=\"model_name\", row=\"inflection_from\",\n",
    "                kind=\"point\", height=3, aspect=3)\n",
    "\n",
    "for ax, row_name in zip(g.axes.flat, g.row_names):\n",
    "    # sns.lineplot(data=lw_random,\n",
    "    #              x=\"layer\", y=\"gt_label_rank\", ax=ax, color=\"gray\", linestyle=\"--\",\n",
    "    #              legend=None)\n",
    "    ax.set_title(ax.get_title().split(\"=\")[1].strip())\n",
    "    ax.set_ylabel(\"Correct\")\n",
    "    if ax.get_xlabel() == \"layer\":\n",
    "        ax.set_xlabel(\"Layer\")\n",
    "\n",
    "# g.figure.tight_layout()\n",
    "g.figure.savefig(f\"{output_dir}/layer_wise-distance.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute controlled NNVB results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nnvb_results = []\n",
    "\n",
    "for run, run_results in all_results.groupby(run_groupers):\n",
    "    run_results = run_results.set_index(\"experiment\")\n",
    "    nnvb_expts = run_results.index.unique()\n",
    "    nnvb_expts = nnvb_expts[nnvb_expts.str.contains(\"unambiguous-\")]\n",
    "\n",
    "    for expt in nnvb_expts:\n",
    "        inflection_from, allomorph_from, inflection_to, allomorph_to = \\\n",
    "            re.findall(r\"unambiguous-(\\w+)_([\\w\\s]+)_to_(\\w+)_([\\w\\s]+)\", expt)[0]\n",
    "        expt_df = run_results.loc[[expt]].copy()\n",
    "\n",
    "        num_seen_words = min(len(expt_df.base_from.unique()), len(expt_df.base_to.unique()))\n",
    "        # DEV\n",
    "        # if num_seen_words < 10:\n",
    "        #     print(f\"Skipping {expt} due to only {num_seen_words} seen words\")\n",
    "        #     continue\n",
    "\n",
    "        expt_df[\"inflection_from\"] = inflection_from\n",
    "        expt_df[\"allomorph_from\"] = allomorph_from\n",
    "        expt_df[\"inflection_to\"] = inflection_to\n",
    "        expt_df[\"allomorph_to\"] = allomorph_to\n",
    "\n",
    "        all_nnvb_results.append(expt_df)\n",
    "\n",
    "all_nnvb_results = pd.concat(all_nnvb_results)\n",
    "\n",
    "all_nnvb_results[\"inflected_from\"] = all_nnvb_results.from_equiv_label.apply(lambda x: eval(x)[1])\n",
    "all_nnvb_results[\"inflected_to\"] = all_nnvb_results.to_equiv_label.apply(lambda x: eval(x)[1])\n",
    "\n",
    "all_nnvb_results = pd.merge(all_nnvb_results, word_freq_df.LogFreq.rename(\"from_base_freq\"),\n",
    "                            left_on=\"base_from\", right_index=True)\n",
    "all_nnvb_results = pd.merge(all_nnvb_results, word_freq_df.LogFreq.rename(\"from_inflected_freq\"),\n",
    "                            left_on=\"inflected_from\", right_index=True)\n",
    "all_nnvb_results = pd.merge(all_nnvb_results, word_freq_df.LogFreq.rename(\"to_base_freq\"),\n",
    "                              left_on=\"base_to\", right_index=True)\n",
    "all_nnvb_results = pd.merge(all_nnvb_results, word_freq_df.LogFreq.rename(\"to_inflected_freq\"),\n",
    "                            left_on=\"inflected_to\", right_index=True)\n",
    "\n",
    "all_nnvb_results[\"from_freq\"] = all_nnvb_results[[\"from_base_freq\", \"from_inflected_freq\"]].mean(axis=1)\n",
    "all_nnvb_results[\"to_freq\"] = all_nnvb_results[[\"to_base_freq\", \"to_inflected_freq\"]].mean(axis=1)\n",
    "\n",
    "_, frequency_bins = pd.qcut(pd.concat([all_nnvb_results.to_freq, all_nnvb_results.from_freq]), q=5, retbins=True)\n",
    "all_nnvb_results[\"to_freq_bin\"] = pd.cut(all_nnvb_results.to_freq, bins=frequency_bins, labels=[f\"Q{i}\" for i in range(1, 6)])\n",
    "all_nnvb_results[\"from_freq_bin\"] = pd.cut(all_nnvb_results.from_freq, bins=frequency_bins, labels=[f\"Q{i}\" for i in range(1, 6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_nnvb_run(rows):\n",
    "    rows[\"source_label\"] = rows.inflection_from + \" \" + rows.allomorph_from\n",
    "    rows[\"target_label\"] = rows.inflection_to + \" \" + rows.allomorph_to\n",
    "\n",
    "    rows[\"transfer_label\"] = rows.inflection_from + \" -> \" + rows.inflection_to\n",
    "    rows[\"phon_label\"] = rows.allomorph_from + \" \" + rows.allomorph_to\n",
    "\n",
    "    # only retain cases where we have data in both transfer directions from source <-> target within inflection\n",
    "    rows[\"complement_exists\"] = rows.apply(lambda row: len(rows.query(\"source_label == @row.target_label and target_label == @row.source_label\")), axis=1)\n",
    "    rows = rows.query(\"complement_exists > 0\").drop(columns=[\"complement_exists\"])\n",
    "\n",
    "    return rows\n",
    "\n",
    "summary_groupers = [\"inflection_from\", \"inflection_to\", \"allomorph_from\", \"allomorph_to\"]\n",
    "nnvb_results_summary = all_nnvb_results.groupby(run_groupers + summary_groupers) \\\n",
    "    .correct.agg([\"count\", \"mean\"]) \\\n",
    "    .query(\"count >= 0\") \\\n",
    "    .reset_index(summary_groupers) \\\n",
    "    .groupby(run_groupers, group_keys=False) \\\n",
    "    .apply(summarize_nnvb_run) \\\n",
    "    .reset_index()\n",
    "\n",
    "nnvb_results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_results = []\n",
    "# for base_model_name, model_name, equivalence in plot_runs:\n",
    "#     results_i = nnvb_results_summary.query(\"base_model_name == @base_model_name and model_name == @model_name and equivalence == @equivalence\")\n",
    "#     if len(results_i) > 0:\n",
    "#         plot_results.append(results_i)\n",
    "# num_plot_runs = len(plot_results)\n",
    "\n",
    "# ncols = 4\n",
    "# nrows = int(np.ceil(num_plot_runs / ncols))\n",
    "# f, axs = plt.subplots(nrows, ncols, figsize=(ncols * 4, nrows * 4))\n",
    "\n",
    "# for ax, results_i in zip(axs.flat, plot_results):\n",
    "#     sns.heatmap(results_i.set_index([\"source_label\", \"target_label\"])[\"mean\"].unstack(),\n",
    "#                 vmin=0, vmax=1, ax=ax)\n",
    "#     key_row = results_i.iloc[0]\n",
    "#     ax.set_title(f\"{key_row.base_model_name} -> {key_row.model_name} ({key_row.equivalence})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focused plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_base_model, focus_model, focus_equivalence = main_plot_run\n",
    "foil_base_model, foil_model, foil_equivalence = \"w2v2_8\", \"id\", \"id\"\n",
    "\n",
    "nnvb_focus = all_nnvb_results.query(\"base_model_name == @focus_base_model and model_name == @focus_model and equivalence == @focus_equivalence\")\n",
    "nnvb_foil = all_nnvb_results.query(\"base_model_name == @foil_base_model and model_name == @foil_model and equivalence == @foil_equivalence\")\n",
    "nnvb_focus[\"model_label\"] = \"Word\"\n",
    "nnvb_foil[\"model_label\"] = \"Wav2Vec\"\n",
    "\n",
    "nnvb_focus = pd.concat([nnvb_focus, nnvb_foil])\n",
    "\n",
    "allomorph_labels = {\"Z\": \"z\", \"S\": \"s\", \"IH Z\": \"ɪz\"}\n",
    "nnvb_focus[\"allomorph_from\"] = nnvb_focus.allomorph_from.map(allomorph_labels)\n",
    "nnvb_focus[\"allomorph_to\"] = nnvb_focus.allomorph_to.map(allomorph_labels)\n",
    "nnvb_focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnvb_results_summary = nnvb_focus.groupby([\"model_label\", \"inflection_from\", \"inflection_to\",\n",
    "                                             \"allomorph_from\", \"allomorph_to\"]) \\\n",
    "    [plot_metrics].mean() \\\n",
    "    .reset_index().astype({\"correct\": float})\n",
    "\n",
    "nnvb_results_summary[\"source_label\"] = nnvb_results_summary.inflection_from + \"\\n\" + nnvb_results_summary.allomorph_from\n",
    "nnvb_results_summary[\"target_label\"] = nnvb_results_summary.inflection_to + \"\\n\" + nnvb_results_summary.allomorph_to\n",
    "\n",
    "nnvb_results_summary[\"transfer_label\"] = nnvb_results_summary.inflection_from + \" -> \" + nnvb_results_summary.inflection_to\n",
    "nnvb_results_summary[\"phon_label\"] = nnvb_results_summary.allomorph_from + \" \" + nnvb_results_summary.allomorph_to\n",
    "\n",
    "# only retain cases where we have data in both transfer directions from source <-> target within inflection\n",
    "nnvb_results_summary[\"complement_exists\"] = nnvb_results_summary.apply(lambda row: len(nnvb_results_summary.query(\"source_label == @row.target_label and target_label == @row.source_label\")), axis=1)\n",
    "nnvb_results_summary = nnvb_results_summary.query(\"complement_exists > 0\").drop(columns=[\"complement_exists\"])\n",
    "\n",
    "# drop VBZ IH Z, which only has 4 word types\n",
    "nnvb_results_summary = nnvb_results_summary[(nnvb_results_summary.source_label != \"VBZ\\nɪz\") & (nnvb_results_summary.target_label != \"VBZ\\nɪz\")]\n",
    "\n",
    "nnvb_results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(data=nnvb_focus.melt(id_vars=[\"inflection_from\", \"allomorph_from\", \"inflection_to\", \"model_label\"], value_vars=plot_metrics)\n",
    "                        .assign(source_label=lambda xs: xs.inflection_from + \" \" + xs.allomorph_from),\n",
    "                x=\"inflection_to\", hue=\"source_label\", y=\"value\", col=\"model_label\", row=\"variable\", kind=\"bar\", sharey=\"row\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in plot_metrics:\n",
    "    vmin = 0 if metric == \"correct\" else nnvb_results_summary[metric].min()\n",
    "    vmax = nnvb_results_summary[metric].max()\n",
    "\n",
    "    f, axs = plt.subplots(1, 3, figsize=(7 * 2, 6), gridspec_kw={'width_ratios': [1, 1, 0.04]})\n",
    "    # f.suptitle(f\"{metric}\", fontsize=20)\n",
    "    for i, (ax, (model_label, rows)) in enumerate(zip(axs, nnvb_results_summary.groupby(\"model_label\"))):\n",
    "        cbar_ax = None\n",
    "        if i == 1:\n",
    "            cbar_ax = axs.flat[-1]\n",
    "\n",
    "        ax.set_title(model_label)\n",
    "        sns.heatmap(rows.set_index([\"source_label\", \"target_label\"]).sort_index()[metric].unstack(\"target_label\"),\n",
    "                    vmin=vmin, vmax=vmax,\n",
    "                    annot=True, fmt=\".2g\", ax=ax,\n",
    "                    cbar=i == 1, cbar_ax=cbar_ax)\n",
    "\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "        ax.set_ylabel(\"Train\")\n",
    "        ax.set_xlabel(\"Test\")\n",
    "\n",
    "    f.tight_layout()\n",
    "    f.savefig(f\"{output_dir}/nnvb_allomorphs-{metric}.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnvb_results_summary2 = nnvb_focus.groupby([\"model_label\", \"inflection_from\", \"inflection_to\"]) \\\n",
    "    [plot_metrics].mean().reset_index().astype({\"correct\": float})\n",
    "\n",
    "nnvb_results_summary2[\"transfer_label\"] = nnvb_results_summary2.inflection_from + \" -> \" + nnvb_results_summary2.inflection_to\n",
    "nnvb_results_summary2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "\n",
    "for metric in plot_metrics:\n",
    "    vmin = 0 if metric == \"correct\" else nnvb_results_summary[metric].min()\n",
    "    vmax = nnvb_results_summary2[metric].max()\n",
    "\n",
    "    f = plt.figure(figsize=(3 * 2 + 2, 3))\n",
    "    gs = GridSpec(1, 3, width_ratios=[1, 1, 0.08],\n",
    "                  wspace=0.55)\n",
    "\n",
    "    axs = [f.add_subplot(gs[0]), f.add_subplot(gs[1])]\n",
    "    cbar_ax = f.add_subplot(gs[2])\n",
    "\n",
    "    for i, (ax, (model_label, rows)) in enumerate(zip(axs, nnvb_results_summary2.groupby(\"model_label\"))):\n",
    "        cbar_ax_i = None\n",
    "        if i == 1:\n",
    "            cbar_ax_i = cbar_ax\n",
    "        \n",
    "        ax.set_title(model_label)\n",
    "        sns.heatmap(rows.set_index([\"inflection_from\", \"inflection_to\"])[metric].unstack(),\n",
    "                    annot=True, vmin=vmin, vmax=vmax, ax=ax,\n",
    "                    cbar=i == 1, cbar_ax=cbar_ax)\n",
    "        ax.set_xlabel(\"Test\")\n",
    "        ax.set_ylabel(\"Train\")\n",
    "\n",
    "    f = plt.gcf()\n",
    "    f.tight_layout()\n",
    "    f.savefig(f\"{output_dir}/nnvb_results-{metric}.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnvb_plot = nnvb_focus.groupby([\"model_label\", \"inflection_from\", \"inflection_to\", \"base_to\"])[plot_metrics].mean().reset_index() \\\n",
    "    .melt(id_vars=[\"model_label\", \"inflection_from\", \"inflection_to\", \"base_to\"],\n",
    "          value_vars=plot_metrics, var_name=\"metric\", value_name=\"value\")\n",
    "nnvb_plot[\"transfer_label\"] = nnvb_plot.inflection_from + \" -> \" + nnvb_plot.inflection_to\n",
    "g = sns.catplot(data=nnvb_plot, x=\"transfer_label\", y=\"value\", kind=\"bar\", hue=\"model_label\", row=\"metric\", sharey=\"row\",\n",
    "                errorbar=\"se\", height=3, aspect=2.5)\n",
    "g._legend.set_title(\"Model\")\n",
    "ax = g.axes.flat[0]\n",
    "\n",
    "ax.set_xlabel(\"Evaluation\")\n",
    "ax.set_ylabel(\"Rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(9, 8))\n",
    "\n",
    "transfer_label_pretty = {\n",
    "    \"NNS -> NNS\": \"NNS → NNS\",\n",
    "    \"VBZ -> NNS\": \"VBZ → NNS\",\n",
    "    \"NNS -> VBZ\": \"NNS → VBZ\",\n",
    "    \"VBZ -> VBZ\": \"VBZ → VBZ\",\n",
    "}\n",
    "ax = sns.barplot(data=nnvb_plot.set_index(sorted(set(nnvb_plot.columns) - {\"value\"}))\n",
    "                    .value.unstack(\"metric\").reset_index()\n",
    "                    .assign(transfer_label=lambda xs: xs.transfer_label.map(transfer_label_pretty)),\n",
    "                 x=\"gt_label_rank\", y=\"transfer_label\", hue=\"model_label\",\n",
    "                 errorbar=\"se\", ax=ax)\n",
    "ax.set_ylabel(None)\n",
    "ax.set_xlabel(\"Rank\")\n",
    "ax.legend(title=\"Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interaction_strength(rows):\n",
    "    rows[\"correct\"] = rows.correct.astype(int)\n",
    "    \n",
    "    # exclude rare\n",
    "    rows = rows[~((rows.inflection_from == \"VBZ\") & rows.inflection_from == \"IH Z\") &\n",
    "                ~((rows.inflection_to == \"VBZ\") & rows.inflection_to == \"IH Z\")]\n",
    "    \n",
    "    # standardize frequency\n",
    "    rows[\"from_freq\"] = (rows.from_freq - rows.from_freq.mean()) / rows.from_freq.std()\n",
    "    rows[\"to_freq\"] = (rows.to_freq - rows.to_freq.mean()) / rows.to_freq.std()\n",
    "    \n",
    "    # formula = \"correct ~ C(inflection_from, Treatment(reference='NNS')) * C(inflection_to, Treatment(reference='NNS')) + \" \\\n",
    "    #           \"C(allomorph_from, Treatment(reference='Z')) * C(allomorph_to, Treatment(reference='Z')) +\" \\\n",
    "    #           \"from_freq + to_freq\"\n",
    "    # model = logit(formula, data=rows).fit()\n",
    "\n",
    "    formula = \"gt_label_rank ~ C(inflection_from, Treatment(reference='NNS')) * C(inflection_to, Treatment(reference='NNS')) * \" \\\n",
    "              \"C(allomorph_from, Treatment(reference='Z')) * C(allomorph_to, Treatment(reference='Z')) +\" \\\n",
    "              \"from_freq + to_freq\"\n",
    "    # fit OLS, remove outliers\n",
    "    model = ols(formula, data=rows[rows.gt_label_rank < np.percentile(rows.gt_label_rank, 90)]).fit()\n",
    "\n",
    "    return model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_model_fits = all_nnvb_results.groupby(run_groupers).apply(get_interaction_strength) \\\n",
    "    .reset_index().melt(id_vars=run_groupers, value_name=\"coef_norm\")\n",
    "interaction_strengths = interaction_model_fits[interaction_model_fits.variable.str.contains(\":\", regex=True)]\n",
    "interaction_strengths = interaction_strengths.groupby(run_groupers).coef_norm.apply(lambda xs: np.linalg.norm(xs, ord=1)).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_model_fits.query(\"base_model_name == 'w2v2_8' and model_name in ['ffff_32', 'ff_32', 'id']\") \\\n",
    "    .assign(inter=lambda xs: xs.variable.str.contains(\":\"),\n",
    "            variable=lambda xs: xs.variable.str.replace(r\", Treatment\\(reference='Z'\\)|, Treatment\\(reference='NNS'\\)\", r\"\", regex=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaghetti_data = interaction_model_fits[interaction_model_fits.model_name.isin((\"ffff_32\", \"id\")) &\n",
    "                                        (interaction_model_fits.base_model_name == \"w2v2_8\") &\n",
    "                                        (interaction_model_fits.variable.str.contains(\":\"))] \\\n",
    "    .pivot_table(index=\"variable\", columns=\"model_name\", values=\"coef_norm\")\n",
    "# reorder\n",
    "assert set(spaghetti_data.columns) == {\"ffff_32\", \"id\"}\n",
    "spaghetti_data = spaghetti_data[[\"id\", \"ffff_32\"]]\n",
    "spaghetti_data.columns = spaghetti_data.columns.map({\"id\": \"Wav2Vec\", main_plot_run[1]: \"Word\"})\n",
    "spaghetti_data = np.abs(spaghetti_data)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(5, 3))\n",
    "for variable, row in spaghetti_data.iterrows():\n",
    "    ax.plot(np.arange(len(row)), row, color=sns.color_palette()[0], marker=\"o\", alpha=0.5)\n",
    "ax.set_xticks(np.arange(len(row)))\n",
    "ax.set_xticklabels(spaghetti_data.columns)\n",
    "ax.set_xlim((-0.25, 1.25))\n",
    "ax.set_xlabel(\"Model\")\n",
    "ax.set_ylabel(\"Interaction\\nstrength\")\n",
    "\n",
    "f.tight_layout()\n",
    "f.savefig(f\"{output_dir}/interaction_strength_spaghetti.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_is = interaction_strengths.reindex(plot_runs).reset_index()\n",
    "plot_is[\"layer\"] = plot_is.base_model_name.str.extract(r\"_(\\d+)$\").astype(int)\n",
    "plot_is[\"model_name\"] = plot_is[\"model_name\"].map({\"id\": \"Wav2Vec\", main_plot_run[1]: \"Word\"})\n",
    "\n",
    "g = sns.catplot(data=plot_is[plot_is.layer == 8],\n",
    "                x=\"model_name\", y=\"coef_norm\", order=[\"Wav2Vec\", \"Word\"],\n",
    "                kind=\"bar\", height=3, aspect=2)\n",
    "g.axes.flat[0].set_ylabel(\"Allomorph/inflection\\ninteraction strength\", fontsize=14)\n",
    "g.axes.flat[0].set_xlabel(\"Model\")\n",
    "\n",
    "g.tight_layout()\n",
    "g.savefig(f\"{output_dir}/interaction_strength.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct for verb paradigm size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study_df = nnvb_focus[(nnvb_focus.inflection_to == \"VBZ\")].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study_df[\"predicted_stem\"] = study_df.predicted_label.str.replace(r\"s$|ed$|ings?$\", \"\", regex=True)\n",
    "# study_df[\"base_to_stem\"] = study_df.base_to.str.replace(r\"e$\", \"\", regex=True).replace(r\"y$\", \"i\", regex=True)\n",
    "# study_df[\"predicted_within_inflection\"] = \\\n",
    "#     (study_df.predicted_stem == study_df.base_to) | (study_df.predicted_stem == study_df.base_to_stem)\n",
    "# vb_irregulars = [(\"do\", \"did\"), (\"do\", \"does\"), (\"begin\", \"began\"), (\"learn\", \"learnt\"), (\"send\", \"sent\"), (\"shine\", \"shone\"), (\"seem\", \"seem'd\"), (\"read\", \"red\"),\n",
    "#                  (\"possess\", \"possesses\"), (\"bring\", \"brings\"), (\"carry\", \"carries\"), (\"occur\", \"occurred\"), (\"think\", \"thinkest\"), (\"grow\", \"grew\"),\n",
    "#                  (\"put\", \"putting\"), (\"begin\", \"beginning\"), (\"give\", \"givest\"),\n",
    "#                  # homophones\n",
    "#                  (\"allow\", \"aloud\"), (\"write\", \"rights\"), (\"write\", \"wright's\"), (\"depend\", \"dependent\")]\n",
    "# for base, predicted in vb_irregulars:\n",
    "#     study_df.loc[study_df.base_to == base, \"predicted_within_inflection\"] |= study_df.loc[study_df.base_to == base].predicted_label == predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_keys = [\"experiment\", \"equivalence\", \"model_label\", \"model_name\", \"base_model_name\", \"group\", \"inflection_from\", \"inflection_to\", \"base_from\", \"inflected_from\", \"base_to\", \"inflected_to\"]\n",
    "# nnvb_focus = pd.merge(nnvb_focus.reset_index(), study_df.reset_index()[merge_keys + [\"predicted_within_inflection\"]],\n",
    "#          on=merge_keys, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nnvb_focus[\"correct_or_predicted_within_inflection\"] = nnvb_focus.correct | nnvb_focus.predicted_within_inflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nnvb_results_summary2 = nnvb_focus.groupby([\"model_label\", \"inflection_from\", \"inflection_to\"]) \\\n",
    "#     [[\"correct_or_predicted_within_inflection\", \"correct\", \"gt_label_rank\", \"gt_distance\"]].mean().reset_index()\n",
    "\n",
    "# nnvb_results_summary2[\"transfer_label\"] = nnvb_results_summary2.inflection_from + \" -> \" + nnvb_results_summary2.inflection_to\n",
    "# nnvb_results_summary2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, axs = plt.subplots(1, 3, figsize=(4 * 2, 3), gridspec_kw={'width_ratios': [1, 1, 0.04]})\n",
    "\n",
    "# vmin = 0\n",
    "# vmax = nnvb_results_summary2.correct_or_predicted_within_inflection.max()\n",
    "\n",
    "# for i, (ax, (model_label, rows)) in enumerate(zip(axs, nnvb_results_summary2.groupby(\"model_label\"))):\n",
    "#     cbar_ax = None\n",
    "#     if i == 1:\n",
    "#         cbar_ax = axs.flat[-1]\n",
    "    \n",
    "#     ax.set_title(model_label)\n",
    "#     sns.heatmap(rows.set_index([\"inflection_from\", \"inflection_to\"]).correct_or_predicted_within_inflection.unstack(),\n",
    "#                 vmin=vmin, vmax=vmax,\n",
    "#                 annot=True, ax=ax, cbar=i == 1, cbar_ax=cbar_ax)\n",
    "#     ax.set_xlabel(\"Test\")\n",
    "#     ax.set_ylabel(\"Train\")\n",
    "\n",
    "# f = plt.gcf()\n",
    "# f.tight_layout()\n",
    "# f.savefig(f\"{output_dir}/nnvb_results-correct_inflection.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nnvb_plot = nnvb_focus.groupby([\"model_label\", \"inflection_from\", \"inflection_to\", \"base_to\"]).correct_or_predicted_within_inflection.mean().reset_index()\n",
    "# nnvb_plot[\"transfer_label\"] = nnvb_plot.inflection_from + \" -> \" + nnvb_plot.inflection_to\n",
    "# order = [\"NNS -> VBZ\", \"VBZ -> VBZ\", \"VBZ -> NNS\", \"NNS -> NNS\"]\n",
    "# g = sns.catplot(data=nnvb_plot, x=\"transfer_label\", y=\"correct_or_predicted_within_inflection\", kind=\"bar\", hue=\"model_label\", order=order, errorbar=\"se\", height=4, aspect=2)\n",
    "# g._legend.set_title(\"Model\")\n",
    "# ax = g.axes.flat[0]\n",
    "\n",
    "# ax.set_xlabel(\"Evaluation\")\n",
    "# ax.set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=nnvb_focus.reset_index(),\n",
    "            x=\"from_freq_bin\", y=\"correct\", hue=\"model_name\",\n",
    "            row=\"inflection_from\", col=\"inflection_to\", units=\"base_from\", kind=\"point\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=nnvb_focus.reset_index(),\n",
    "            x=\"to_freq_bin\", y=\"correct\", hue=\"model_name\",\n",
    "            row=\"inflection_from\", col=\"inflection_to\", units=\"base_to\", kind=\"point\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=nnvb_focus.reset_index().query(\"model_name == @focus_model\"),\n",
    "            x=\"from_freq_bin\", y=\"gt_label_rank\", hue=\"model_name\",\n",
    "            row=\"inflection_from\", col=\"inflection_to\",\n",
    "            units=\"base_from\", kind=\"point\",\n",
    "            sharey=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=nnvb_focus.reset_index().query(\"model_name == @focus_model\"),\n",
    "            x=\"to_freq_bin\", y=\"gt_label_rank\", hue=\"model_name\",\n",
    "            row=\"inflection_from\", col=\"inflection_to\",\n",
    "            sharey=False,\n",
    "            units=\"base_to\", kind=\"point\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlled VBD analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vbd_results = all_results.query(\"experiment == 'regular' and inflection_from == 'VBD'\")\n",
    "all_vbd_results = pd.merge(all_vbd_results, most_common_allomorphs.rename(columns={\"base\": \"base_from\", \"inflection\": \"inflection_from\", \"most_common_allomorph\": \"allomorph_from\"}),\n",
    "               on=[\"base_from\", \"inflection_from\"], how=\"left\")\n",
    "all_vbd_results = pd.merge(all_vbd_results, most_common_allomorphs.rename(columns={\"base\": \"base_to\", \"inflection\": \"inflection_to\", \"most_common_allomorph\": \"allomorph_to\"}),\n",
    "               on=[\"base_to\", \"inflection_to\"], how=\"left\")\n",
    "all_vbd_results[[\"allomorph_from\", \"allomorph_to\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop do -> did; how did these irregulars get in here?\n",
    "exclude_vbd_base = [\"do\", \"hide\", \"make\"]\n",
    "# exclude stress shift items; we don't know which token items have which stress\n",
    "exclude_vbd_base += \"record permit protest reject subject conduct contract conflict increase decrease contest insult impact address escort\".split()\n",
    "# multiple possible\n",
    "exclude_vbd_base += \"dream leap\".split()\n",
    "all_vbd_results = all_vbd_results[~all_vbd_results.base_from.isin(exclude_vbd_base) & ~all_vbd_results.base_to.isin(exclude_vbd_base)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_vbd_allomorphs = all_vbd_results.allomorph_from.value_counts().head(3).index\n",
    "all_vbd_results = all_vbd_results[all_vbd_results.allomorph_from.isin(keep_vbd_allomorphs)\n",
    "                                  & all_vbd_results.allomorph_to.isin(keep_vbd_allomorphs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add frequency information\n",
    "\n",
    "all_vbd_results[\"inflected_from\"] = all_vbd_results.from_equiv_label.apply(lambda x: eval(x)[1])\n",
    "all_vbd_results[\"inflected_to\"] = all_vbd_results.to_equiv_label.apply(lambda x: eval(x)[1])\n",
    "\n",
    "all_vbd_results = pd.merge(all_vbd_results, word_freq_df[\"LogFreq\"].rename(\"from_base_freq\"),\n",
    "                           left_on=\"base_from\", right_index=True)\n",
    "all_vbd_results = pd.merge(all_vbd_results, word_freq_df[\"LogFreq\"].rename(\"to_base_freq\"),\n",
    "                           left_on=\"base_to\", right_index=True)\n",
    "all_vbd_results = pd.merge(all_vbd_results, word_freq_df[\"LogFreq\"].rename(\"from_inflected_freq\"),\n",
    "                            left_on=\"inflected_from\", right_index=True)\n",
    "all_vbd_results = pd.merge(all_vbd_results, word_freq_df[\"LogFreq\"].rename(\"to_inflected_freq\"),\n",
    "                            left_on=\"inflected_to\", right_index=True)\n",
    "\n",
    "all_vbd_results[\"from_freq\"] = all_vbd_results[[\"from_base_freq\", \"from_inflected_freq\"]].mean(axis=1)\n",
    "all_vbd_results[\"to_freq\"] = all_vbd_results[[\"to_base_freq\", \"to_inflected_freq\"]].mean(axis=1)\n",
    "\n",
    "_, vbd_frequency_bins = pd.qcut(pd.concat([all_vbd_results.to_freq, all_vbd_results.from_freq]), q=3, retbins=True)\n",
    "all_vbd_results[\"from_freq_bin\"] = pd.cut(all_vbd_results.from_freq, bins=vbd_frequency_bins, labels=[f\"Q{i}\" for i in range(1, 4)])\n",
    "all_vbd_results[\"to_freq_bin\"] = pd.cut(all_vbd_results.to_freq, bins=vbd_frequency_bins, labels=[f\"Q{i}\" for i in range(1, 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_vbd_run(rows):\n",
    "    rows[\"source_label\"] = rows.inflection_from + \" \" + rows.allomorph_from\n",
    "    rows[\"target_label\"] = rows.inflection_to + \" \" + rows.allomorph_to\n",
    "\n",
    "    rows[\"transfer_label\"] = rows.inflection_from + \" -> \" + rows.inflection_to\n",
    "    rows[\"phon_label\"] = rows.allomorph_from + \" -> \" + rows.allomorph_to\n",
    "\n",
    "    return rows\n",
    "\n",
    "summary_groupers = [\"inflection_from\", \"inflection_to\", \"allomorph_from\", \"allomorph_to\"]\n",
    "vbd_results_summary = all_vbd_results.groupby(run_groupers + summary_groupers) \\\n",
    "    [plot_metrics].mean() \\\n",
    "    .reset_index(summary_groupers) \\\n",
    "    .groupby(run_groupers, group_keys=False) \\\n",
    "    .apply(summarize_vbd_run) \\\n",
    "    .reset_index()\n",
    "\n",
    "vbd_results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_results = []\n",
    "# for base_model_name, model_name, equivalence in plot_runs:\n",
    "#     results_i = vbd_results_summary.query(\"base_model_name == @base_model_name and model_name == @model_name and equivalence == @equivalence\")\n",
    "#     if len(results_i) > 0:\n",
    "#         plot_results.append(results_i)\n",
    "# num_plot_runs = len(plot_results)\n",
    "\n",
    "# ncols = 4\n",
    "# nrows = int(np.ceil(num_plot_runs / ncols))\n",
    "# f, axs = plt.subplots(nrows, ncols, figsize=(ncols * 4, nrows * 4))\n",
    "\n",
    "# for ax, results_i in zip(axs.flat, plot_results):\n",
    "#     sns.heatmap(results_i.set_index([\"source_label\", \"target_label\"]).correct.unstack(),\n",
    "#                 vmin=0, vmax=1, ax=ax)\n",
    "#     key_row = results_i.iloc[0]\n",
    "#     ax.set_title(f\"{key_row.base_model_name} -> {key_row.model_name} ({key_row.equivalence})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbd_focus = all_vbd_results.query(\"base_model_name == @focus_base_model and model_name == @focus_model and equivalence == @focus_equivalence\")\n",
    "vbd_foil = all_vbd_results.query(\"base_model_name == @foil_base_model and model_name == @foil_model and equivalence == @foil_equivalence\")\n",
    "vbd_focus[\"model_label\"] = \"Word\"\n",
    "vbd_foil[\"model_label\"] = \"Wav2Vec\"\n",
    "vbd_focus = pd.concat([vbd_focus, vbd_foil])\n",
    "\n",
    "vbd_results_summary2 = vbd_focus.groupby([\"model_label\", \"inflection_from\", \"inflection_to\"]) \\\n",
    "    [plot_metrics].mean().reset_index()\n",
    "\n",
    "vbd_results_summary2[\"transfer_label\"] = vbd_results_summary2.inflection_from + \" -> \" + vbd_results_summary2.inflection_to\n",
    "vbd_results_summary2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in plot_metrics:\n",
    "    vmin = 0 if metric == \"correct\" else nnvb_results_summary[metric].min()\n",
    "    vmax = vbd_results_summary2[metric].max()\n",
    "\n",
    "    f, axs = plt.subplots(1, 3, figsize=(4 * 2, 3), gridspec_kw={'width_ratios': [1, 1, 0.04]})\n",
    "\n",
    "    for i, (ax, (model_label, rows)) in enumerate(zip(axs, vbd_results_summary2.groupby(\"model_label\"))):\n",
    "        cbar_ax = None\n",
    "        if i == 1:\n",
    "            cbar_ax = axs.flat[-1]\n",
    "        \n",
    "        ax.set_title(model_label)\n",
    "        sns.heatmap(rows.set_index([\"inflection_from\", \"inflection_to\"])[metric].unstack(),\n",
    "                    annot=True, vmin=vmin, vmax=vmax, ax=ax,\n",
    "                    cbar=i == 1, cbar_ax=cbar_ax)\n",
    "        ax.set_xlabel(\"Test\")\n",
    "        ax.set_ylabel(\"Train\")\n",
    "\n",
    "    f = plt.gcf()\n",
    "    f.tight_layout()\n",
    "    f.savefig(f\"{output_dir}/vbd_results-{metric}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focused plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_base_model, focus_model, focus_equivalence = main_plot_run\n",
    "foil_base_model, foil_model, foil_equivalence = \"w2v2_8\", \"id\", \"id\"\n",
    "\n",
    "vbd_focus = all_vbd_results.query(\"base_model_name == @focus_base_model and model_name == @focus_model and equivalence == @focus_equivalence\")\n",
    "vbd_foil = all_vbd_results.query(\"base_model_name == @foil_base_model and model_name == @foil_model and equivalence == @foil_equivalence\")\n",
    "vbd_focus[\"model_label\"] = \"Word\"\n",
    "vbd_foil[\"model_label\"] = \"Wav2Vec\"\n",
    "\n",
    "vbd_focus = pd.concat([vbd_focus, vbd_foil])\n",
    "\n",
    "allomorph_labels = {\"D\": \"d\", \"T\": \"t\", \"IH D\": \"ɪd\"}\n",
    "vbd_focus[\"allomorph_from\"] = vbd_focus.allomorph_from.map(allomorph_labels)\n",
    "vbd_focus[\"allomorph_to\"] = vbd_focus.allomorph_to.map(allomorph_labels)\n",
    "vbd_focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbd_focus.query(\"inflection_to == 'VBD' and allomorph_to == 'ɪd'\").groupby([\"model_name\", \"base_to\"]).gt_label_rank.mean().sort_values().tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbd_focus.query(\"inflection_to == 'VBD' and allomorph_to == 't'\").groupby([\"model_name\", \"base_to\"]).gt_label_rank.mean().sort_values().tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbd_focus.query(\"inflection_to == 'VBD' and allomorph_to == 'd'\").groupby([\"model_name\", \"base_to\"]).gt_label_rank.mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbd_results_summary = vbd_focus.groupby([\"model_label\", \"inflection_from\", \"inflection_to\",\n",
    "                                             \"allomorph_from\", \"allomorph_to\"]) \\\n",
    "    [plot_metrics].mean() \\\n",
    "    .reset_index()\n",
    "\n",
    "vbd_results_summary[\"source_label\"] = vbd_results_summary.inflection_from + \"\\n\" + vbd_results_summary.allomorph_from\n",
    "vbd_results_summary[\"target_label\"] = vbd_results_summary.inflection_to + \"\\n\" + vbd_results_summary.allomorph_to\n",
    "\n",
    "vbd_results_summary[\"transfer_label\"] = vbd_results_summary.inflection_from + \" -> \" + vbd_results_summary.inflection_to\n",
    "vbd_results_summary[\"phon_label\"] = vbd_results_summary.allomorph_from + \" \" + vbd_results_summary.allomorph_to\n",
    "\n",
    "# only retain cases where we have data in both transfer directions from source <-> target within inflection\n",
    "vbd_results_summary[\"complement_exists\"] = vbd_results_summary.apply(lambda row: len(vbd_results_summary.query(\"source_label == @row.target_label and target_label == @row.source_label\")), axis=1)\n",
    "vbd_results_summary = vbd_results_summary.query(\"complement_exists > 0\").drop(columns=[\"complement_exists\"])\n",
    "\n",
    "vbd_results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=vbd_focus.melt(id_vars=[\"inflection_from\", \"allomorph_from\", \"inflection_to\", \"model_label\"], value_vars=plot_metrics)\n",
    "                        .assign(source_label=lambda xs: xs.inflection_from + \" \" + xs.allomorph_from),\n",
    "                x=\"inflection_to\", hue=\"source_label\", y=\"value\", col=\"model_label\", row=\"variable\", kind=\"bar\", sharey=\"row\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in plot_metrics:\n",
    "    vmin = 0 if metric == \"correct\" else nnvb_results_summary[metric].min()\n",
    "    vmax = vbd_results_summary[metric].max()\n",
    "\n",
    "    f, axs = plt.subplots(1, 3, figsize=(7 * 2, 6), gridspec_kw={'width_ratios': [1, 1, 0.04]})\n",
    "    f.suptitle(f\"{metric}\", fontsize=20)\n",
    "    for i, (ax, (model_label, rows)) in enumerate(zip(axs, vbd_results_summary.groupby(\"model_label\"))):\n",
    "        cbar_ax = None\n",
    "        if i == 1:\n",
    "            cbar_ax = axs.flat[-1]\n",
    "\n",
    "        ax.set_title(model_label)\n",
    "        sns.heatmap(rows.set_index([\"source_label\", \"target_label\"]).sort_index()[metric].unstack(\"target_label\"),\n",
    "                    vmin=vmin, vmax=vmax,\n",
    "                    annot=True, ax=ax,\n",
    "                    cbar=i == 1, cbar_ax=cbar_ax)\n",
    "\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "        ax.set_ylabel(\"Train\")\n",
    "        ax.set_xlabel(\"Test\")\n",
    "\n",
    "    f.tight_layout()\n",
    "    f.savefig(f\"{output_dir}/vbd_allomorphs-{metric}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vbd_interaction_strength(rows):\n",
    "    rows[\"correct\"] = rows.correct.astype(int)\n",
    "    \n",
    "    # exclude rare\n",
    "    rows = rows[~((rows.inflection_from == \"VBZ\") & rows.inflection_from == \"IH Z\") &\n",
    "                ~((rows.inflection_to == \"VBZ\") & rows.inflection_to == \"IH Z\")]\n",
    "    \n",
    "    # standardize frequency\n",
    "    rows[\"from_freq\"] = (rows.from_freq - rows.from_freq.mean()) / rows.from_freq.std()\n",
    "    rows[\"to_freq\"] = (rows.to_freq - rows.to_freq.mean()) / rows.to_freq.std()\n",
    "    \n",
    "    # formula = \"correct ~ C(inflection_from, Treatment(reference='NNS')) * C(inflection_to, Treatment(reference='NNS')) + \" \\\n",
    "    #           \"C(allomorph_from, Treatment(reference='Z')) * C(allomorph_to, Treatment(reference='Z')) +\" \\\n",
    "    #           \"from_freq + to_freq\"\n",
    "    # model = logit(formula, data=rows).fit()\n",
    "\n",
    "    formula = \"gt_label_rank ~ C(allomorph_from, Treatment(reference='T')) * C(allomorph_to, Treatment(reference='T')) +\" \\\n",
    "              \"from_freq + to_freq\"\n",
    "    # fit OLS, remove outliers\n",
    "    model = ols(formula, data=rows[rows.gt_label_rank < np.percentile(rows.gt_label_rank, 90)]).fit()\n",
    "\n",
    "    return model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbd_interaction_model_fits = all_vbd_results.groupby(run_groupers).apply(get_vbd_interaction_strength) \\\n",
    "    .reset_index().melt(id_vars=run_groupers, value_name=\"coef_norm\")\n",
    "vbd_interaction_model_fits = vbd_interaction_model_fits[vbd_interaction_model_fits.variable.str.contains(\":\", regex=True)]\n",
    "vbd_interaction_model_fits = vbd_interaction_model_fits.groupby(run_groupers).coef_norm.apply(lambda xs: np.linalg.norm(xs, ord=1)).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_is = vbd_interaction_model_fits.reindex(plot_runs).reset_index()\n",
    "plot_is[\"layer\"] = plot_is.base_model_name.str.extract(r\"_(\\d+)$\").astype(int)\n",
    "\n",
    "g = sns.catplot(data=plot_is[plot_is.layer == 8], x=\"model_name\", y=\"coef_norm\", kind=\"bar\", height=3, aspect=2)\n",
    "g.axes.flat[0].set_ylabel(\"Allomorph/inflection\\ninteraction strength\")\n",
    "g.axes.flat[0].set_xlabel(\"Model\")\n",
    "\n",
    "g.savefig(f\"{output_dir}/vbd_interaction_strength.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False friend analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ff_results = []\n",
    "\n",
    "for run, run_results in all_results.groupby(run_groupers):\n",
    "    run_results = run_results.set_index(\"experiment\")\n",
    "    false_friend_expts = run_results.index.unique()\n",
    "    false_friend_expts = false_friend_expts[false_friend_expts.str.contains(\"FF\")]\n",
    "\n",
    "    for expt_name in false_friend_expts:\n",
    "        expt_df = run_results.loc[expt_name].copy()\n",
    "        num_seen_words = min(len(expt_df.base_from.unique()), len(expt_df.base_to.unique()))\n",
    "\n",
    "        if num_seen_words < 10:\n",
    "            print(f\"Skipping {expt} due to only {num_seen_words} seen words\")\n",
    "            continue\n",
    "\n",
    "        if expt_name.count(\"-FF-\") == 2:\n",
    "            allomorph_from, allomorph_to = re.findall(r\"-FF-([\\w\\s]+)-to-.+FF-([\\w\\s]+)\", expt_name)[0]\n",
    "            ff_from, ff_to = True, True\n",
    "        else:\n",
    "            try:\n",
    "                allomorph_from, allomorph_to = re.findall(r\"_([\\w\\s]+)-to-.+FF-([\\w\\s]+)\", expt_name)[0]\n",
    "                # is the false friend on the \"from\" side?\n",
    "                ff_from, ff_to = False, True\n",
    "            except:\n",
    "                allomorph_from, allomorph_to = re.findall(r\".+FF-([\\w\\s]+)-to-.+_([\\w\\s]+)\", expt_name)[0]\n",
    "                ff_from, ff_to = True, False\n",
    "\n",
    "        expt_df[\"allomorph_from\"] = allomorph_from\n",
    "        expt_df[\"allomorph_to\"] = allomorph_to\n",
    "\n",
    "        if ff_from:\n",
    "            expt_df[\"inflection_from\"] = expt_df.inflection_from.str.replace(\"-FF-.+\", \"-FF\", regex=True)\n",
    "        if ff_to:\n",
    "            expt_df[\"inflection_to\"] = expt_df.inflection_to.str.replace(\"-FF-.+\", \"-FF\", regex=True)\n",
    "\n",
    "        expt_df[\"ff_from\"] = ff_from\n",
    "        expt_df[\"ff_to\"] = ff_to\n",
    "\n",
    "        all_ff_results.append(expt_df)\n",
    "\n",
    "    # add within-false-friend tests\n",
    "    expt_df = run_results.loc[\"false_friends\"].copy()\n",
    "    expt_df[\"allomorph_from\"] = expt_df.inflection_from.str.extract(r\"FF-(.+)$\")\n",
    "    expt_df[\"allomorph_to\"] = expt_df.inflection_to.str.extract(r\"FF-(.+)$\")\n",
    "    expt_df[\"inflection_from\"] = expt_df.inflection_from.str.replace(\"-FF-.+\", \"-FF\", regex=True)\n",
    "    expt_df[\"inflection_to\"] = expt_df.inflection_to.str.replace(\"-FF-.+\", \"-FF\", regex=True)\n",
    "    expt_df[\"ff_from\"] = True\n",
    "    expt_df[\"ff_to\"] = True\n",
    "\n",
    "    all_ff_results.append(expt_df)\n",
    "\n",
    "    # expt_df = expt_df[expt_df.inflection_from.isin(all_ff_results.inflection_from.unique())]\n",
    "\n",
    "all_ff_results = pd.concat(all_ff_results).reset_index()\n",
    "\n",
    "ff_exclude = \"a b c wreck d e eh wandering lo chiu ha hahn meek jew shew ah co des re san ol der k la ye ll\"\n",
    "ff_exclude_inflected = \"bunce los oft mast hauled sward\"\n",
    "\n",
    "# exclude FF bases\n",
    "all_ff_results = all_ff_results[~(all_ff_results.inflection_from.str.endswith(\"-FF\") & all_ff_results.base_from.isin(ff_exclude.split()))]\n",
    "all_ff_results = all_ff_results[~(all_ff_results.inflection_to.str.endswith(\"-FF\") & all_ff_results.base_to.isin(ff_exclude.split()))]\n",
    "\n",
    "all_ff_results[\"inflected_from\"] = all_ff_results.from_equiv_label.apply(lambda x: eval(x)[1])\n",
    "all_ff_results[\"inflected_to\"] = all_ff_results.to_equiv_label.apply(lambda x: eval(x)[1])\n",
    "\n",
    "# exclude FF inflected\n",
    "all_ff_results = all_ff_results[~(all_ff_results.inflection_from.str.endswith(\"-FF\") & all_ff_results.inflected_from.isin(ff_exclude_inflected.split()))]\n",
    "all_ff_results = all_ff_results[~(all_ff_results.inflection_to.str.endswith(\"-FF\") & all_ff_results.inflected_to.isin(ff_exclude_inflected.split()))]\n",
    "\n",
    "# add frequency information\n",
    "all_ff_results = pd.merge(all_ff_results, word_freq_df[\"LogFreq\"].rename(\"from_base_freq\"), left_on=\"base_from\", right_index=True)\n",
    "all_ff_results = pd.merge(all_ff_results, word_freq_df[\"LogFreq\"].rename(\"to_base_freq\"), left_on=\"base_to\", right_index=True)\n",
    "all_ff_results = pd.merge(all_ff_results, word_freq_df[\"LogFreq\"].rename(\"from_inflected_freq\"), left_on=\"inflected_from\", right_index=True)\n",
    "all_ff_results = pd.merge(all_ff_results, word_freq_df[\"LogFreq\"].rename(\"to_inflected_freq\"), left_on=\"inflected_to\", right_index=True)\n",
    "all_ff_results[\"from_freq\"] = all_ff_results[[\"from_base_freq\", \"from_inflected_freq\"]].mean(axis=1)\n",
    "all_ff_results[\"to_freq\"] = all_ff_results[[\"to_base_freq\", \"to_inflected_freq\"]].mean(axis=1)\n",
    "\n",
    "all_ff_results[\"transfer_label\"] = all_ff_results.inflection_from + \" -> \" + all_ff_results.inflection_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-hoc fix some bugs\n",
    "all_ff_results.loc[(all_ff_results.base_to == \"tho\") & (all_ff_results.predicted_label == \"though\") & (all_ff_results.gt_label_rank == 1), \"correct\"] = True\n",
    "all_ff_results.loc[(all_ff_results.base_to == \"philip\") & (all_ff_results.predicted_label == \"philip's\"), \"correct\"] = True\n",
    "all_ff_results.loc[(all_ff_results.base_to == \"adam\") & (all_ff_results.predicted_label == \"adam's\"), \"correct\"] = True\n",
    "all_ff_results.loc[(all_ff_results.base_to == \"who\") & (all_ff_results.predicted_label == \"who's\"), \"correct\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude bad VBD bases\n",
    "print(\"before filtering: \", len(all_ff_results))\n",
    "all_ff_results = all_ff_results[~(((all_ff_results.inflection_to == \"VBD\") & all_ff_results.base_to.isin(exclude_vbd_base)) |\n",
    "                                  ((all_ff_results.inflection_from == \"VBD\") & all_ff_results.base_from.isin(exclude_vbd_base)))]\n",
    "print(\"after filtering: \", len(all_ff_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_friend_strong_lookup = false_friends_df.set_index([\"base\", \"inflected\", \"post_divergence\"]).strong.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_is_strong(rows):\n",
    "    keys = []\n",
    "    row = rows.iloc[0]\n",
    "    if \"-FF\" in row.inflection_from:\n",
    "        keys.append((row.base_from, row.inflected_from, row.allomorph_from))\n",
    "    if \"-FF\" in row.inflection_to:\n",
    "        keys.append((row.base_to, row.inflected_to, row.allomorph_to))\n",
    "\n",
    "    # print(keys)\n",
    "    strong_results = [false_friend_strong_lookup[base, inflected, allomorph] for base, inflected, allomorph in keys]\n",
    "    return all(strong_results)\n",
    "\n",
    "strong_grouper = [\"inflection_from\", \"inflection_to\", \"inflected_from\", \"inflected_to\", \"base_from\", \"base_to\", \"allomorph_from\", \"allomorph_to\"]\n",
    "strong_values = all_ff_results.groupby(strong_grouper).apply(get_is_strong).rename(\"is_strong\")\n",
    "all_ff_results = pd.merge(all_ff_results, strong_values, left_on=strong_grouper, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_ff_results = all_ff_results[~all_ff_results.is_strong]\n",
    "\n",
    "# ONLY STRONG\n",
    "all_ff_results = all_ff_results[all_ff_results.is_strong]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ff_results.query(\"inflection_from == 'VBD-FF' & inflection_to == 'VBD' and model_name == 'ffff_32' and base_model_name == 'w2v2_8'\") \\\n",
    "    .groupby(\"base_to\").gt_label_rank.mean().sort_values().tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main FF analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_frequency_bins = pd.qcut(pd.concat([all_ff_results.to_freq, all_ff_results.from_freq]), q=3, retbins=True)[1]\n",
    "all_ff_results[\"from_freq_bin\"] = pd.cut(all_ff_results.from_freq, bins=ff_frequency_bins, labels=[f\"Q{i}\" for i in range(1, 4)])\n",
    "all_ff_results[\"to_freq_bin\"] = pd.cut(all_ff_results.to_freq, bins=ff_frequency_bins, labels=[f\"Q{i}\" for i in range(1, 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distribution of false friend word frequencies to distribution of NN/VB frequencies.\n",
    "# This is to see if the false friends are more likely to be rare words.\n",
    "false_friend_words = pd.concat([all_ff_results.query(\"ff_from\").base_from, all_ff_results.query(\"ff_to\").base_to]).unique()\n",
    "nn_words = pd.concat([all_nnvb_results.query(\"inflection_from == 'NNS'\").base_from,\n",
    "                        all_nnvb_results.query(\"inflection_to == 'NNS'\").base_to]).unique()\n",
    "vb_words = pd.concat([all_nnvb_results.query(\"inflection_from == 'VBZ'\").base_from,\n",
    "                        all_nnvb_results.query(\"inflection_to == 'VBZ'\").base_to]).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_word_freqs = pd.concat({\n",
    "    \"false_friends\": word_freq_df.loc[false_friend_words].LogFreq,\n",
    "    \"NN\": word_freq_df.loc[nn_words].LogFreq,\n",
    "    \"VB\": word_freq_df.loc[vb_words].LogFreq\n",
    "}, names=[\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=expt_word_freqs.reset_index(), x=\"LogFreq\", row=\"type\", kind=\"hist\", bins=15,\n",
    "            height=3, aspect=3, facet_kws={\"sharey\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_ff_results = all_ff_results.query(\"base_model_name == @focus_base_model and model_name == @focus_model and equivalence == @focus_equivalence\")\n",
    "foil_ff_results = all_ff_results.query(\"base_model_name == @foil_base_model and model_name == @foil_model and equivalence == @foil_equivalence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_results_summary2 = pd.concat([focus_ff_results, foil_ff_results]).groupby([\"model_name\", \"inflection_from\", \"inflection_to\"]) \\\n",
    "    [plot_metrics].mean().reset_index()\n",
    "ff_results_summary2[\"transfer_label\"] = ff_results_summary2.inflection_from + \" -> \" + ff_results_summary2.inflection_to\n",
    "assert set(ff_results_summary2.model_name) == {\"ffff_32\", \"id\"}\n",
    "ff_results_summary2[\"model_label\"] = ff_results_summary2[\"model_name\"].map({\"id\": \"Wav2Vec\", \"ffff_32\": \"Word\"})\n",
    "ff_results_summary2 = ff_results_summary2.drop(columns=[\"model_name\"])\n",
    "\n",
    "ff_results_summary2 = pd.concat([ff_results_summary2,\n",
    "    nnvb_results_summary2.query(\"inflection_from == inflection_to\"),\n",
    "    vbd_results_summary2.query(\"inflection_from == inflection_to\")])\n",
    "\n",
    "ff_results_summary2[\"base_inflection\"] = ff_results_summary2.inflection_from.str.replace(\"-FF\", \"\")\n",
    "ff_results_summary2 = ff_results_summary2[ff_results_summary2.base_inflection.isin(plot_inflections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(data=ff_results_summary2, row=\"model_label\", col=\"base_inflection\",\n",
    "                  row_order=[\"Wav2Vec\", \"Word\"], sharex=False, sharey=False)\n",
    "plot_variable = \"gt_label_rank\"\n",
    "\n",
    "vmin = ff_results_summary2[plot_variable].min()\n",
    "vmax = ff_results_summary2[plot_variable].max()\n",
    "\n",
    "def mapfn(data, **kwargs):\n",
    "    ax = plt.gca()\n",
    "    sns.heatmap(data.set_index([\"inflection_from\", \"inflection_to\"])[plot_variable].unstack(\"inflection_to\"),\n",
    "                vmin=vmin, vmax=vmax, annot=True, fmt=\".2g\", ax=ax, cbar=False)\n",
    "\n",
    "g.map_dataframe(mapfn)\n",
    "\n",
    "for i, row in enumerate(g.axes):\n",
    "    for j, ax in enumerate(row):\n",
    "        ax.set_title(ax.get_title().replace(\"base_inflection = \", \"\").replace(\"model_label = \", \"\"))\n",
    "        if j > 0:\n",
    "            ax.set_ylabel(\"\")\n",
    "        else:\n",
    "            ax.set_ylabel(\"Train\")\n",
    "        \n",
    "        if i < len(g.axes) - 1:\n",
    "            ax.set_xlabel(\"\")\n",
    "        else:\n",
    "            ax.set_xlabel(\"Test\")\n",
    "\n",
    "# add colorbar\n",
    "cbar_ax = g.fig.add_axes([0.99, 0.18, 0.03, 0.7])\n",
    "cbar = plt.colorbar(g.axes.flat[0].collections[0], cax=cbar_ax)\n",
    "cbar.outline.set_visible(False)\n",
    "\n",
    "g.tight_layout()\n",
    "g.savefig(f\"{output_dir}/ff_results-merged.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_results_summary2 = focus_ff_results.groupby([\"inflection_from\", \"inflection_to\"]) \\\n",
    "    [plot_metrics].mean().reset_index()\n",
    "\n",
    "ff_results_summary2[\"transfer_label\"] = ff_results_summary2.inflection_from + \" -> \" + ff_results_summary2.inflection_to\n",
    "\n",
    "# add in data for NNS->NNS and VBZ->VBZ\n",
    "ff_results_summary2 = pd.concat([\n",
    "    ff_results_summary2,\n",
    "    nnvb_results_summary2.query(\"inflection_from == inflection_to and model_label == 'Word'\"),\n",
    "    vbd_results_summary2.query(\"inflection_from == inflection_to and model_label == 'Word'\")], axis=0)\n",
    "\n",
    "ff_results_summary2[\"base_inflection\"] = ff_results_summary2.inflection_from.str.replace(\"-FF\", \"\")\n",
    "ff_results_summary2 = ff_results_summary2[ff_results_summary2.base_inflection.isin(plot_inflections)]\n",
    "\n",
    "# g = sns.FacetGrid(ff_results_summary2, col=\"base_inflection\", sharex=False, sharey=False)\n",
    "g = sns.FacetGrid(data=ff_results_summary2.melt(id_vars=[\"inflection_from\", \"inflection_to\", \"transfer_label\", \"base_inflection\"],\n",
    "                                                value_vars=plot_metrics),\n",
    "                    col=\"base_inflection\", row=\"variable\", sharex=False, sharey=False,\n",
    "                    height=3, aspect=1.25)\n",
    "\n",
    "def mapfn(data, **kwargs):\n",
    "    ax = plt.gca()\n",
    "    metric = data.variable.iloc[0]\n",
    "    vmin = 0 if metric == \"correct\" else data.value.min()\n",
    "    vmax = data.value.max()\n",
    "\n",
    "    sns.heatmap(data.set_index([\"inflection_from\", \"inflection_to\"]).value.unstack(\"inflection_to\"),\n",
    "                vmin=vmin, vmax=vmax, annot=True, ax=ax)\n",
    "\n",
    "g.map_dataframe(mapfn)\n",
    "\n",
    "for i, row in enumerate(g.axes):\n",
    "    for j, ax in enumerate(row):\n",
    "        ax.set_title(ax.get_title().replace(\"base_inflection = \", \"\"))\n",
    "        if j > 0:\n",
    "            ax.set_ylabel(\"\")\n",
    "        if j < len(row) - 1:\n",
    "            ax.collections[0].colorbar.remove()\n",
    "\n",
    "g.fig.tight_layout()\n",
    "g.fig.savefig(f\"{output_dir}/ff_results.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_foil_results_summary2 = foil_ff_results.groupby([\"inflection_from\", \"inflection_to\"]) \\\n",
    "    [[\"correct\", \"gt_label_rank\", \"gt_distance\"]].mean().reset_index()\n",
    "\n",
    "ff_foil_results_summary2[\"transfer_label\"] = ff_foil_results_summary2.inflection_from + \" -> \" + ff_foil_results_summary2.inflection_to\n",
    "\n",
    "# add in data for NNS->NNS and VBZ->VBZ\n",
    "ff_foil_results_summary2 = pd.concat([\n",
    "    ff_foil_results_summary2,\n",
    "    nnvb_results_summary2.query(\"inflection_from == inflection_to and model_label == 'Wav2Vec'\"),\n",
    "    vbd_results_summary2.query(\"inflection_from == inflection_to and model_label == 'Wav2Vec'\")], axis=0)\n",
    "\n",
    "ff_foil_results_summary2[\"base_inflection\"] = ff_foil_results_summary2.inflection_from.str.replace(\"-FF\", \"\")\n",
    "\n",
    "ff_foil_results_summary2 = ff_foil_results_summary2[ff_foil_results_summary2.base_inflection.isin(plot_inflections)]\n",
    "\n",
    "g = sns.FacetGrid(data=ff_foil_results_summary2.melt(id_vars=[\"inflection_from\", \"inflection_to\", \"transfer_label\", \"base_inflection\"],\n",
    "                                                value_vars=plot_metrics),\n",
    "                    col=\"base_inflection\", row=\"variable\", sharex=False, sharey=False,\n",
    "                    height=3, aspect=1.25)\n",
    "def mapfn(data, **kwargs):\n",
    "    ax = plt.gca()\n",
    "    metric = data.variable.iloc[0]\n",
    "    vmin = 0 if metric == \"correct\" else data.value.min()\n",
    "    vmax = data.value.max()\n",
    "\n",
    "    sns.heatmap(data.set_index([\"inflection_from\", \"inflection_to\"]).value.unstack(\"inflection_to\"),\n",
    "                vmin=vmin, vmax=vmax, annot=True, ax=ax)\n",
    "g.map_dataframe(mapfn)\n",
    "\n",
    "for i, row in enumerate(g.axes):\n",
    "    for j, ax in enumerate(row):\n",
    "        ax.set_title(ax.get_title().replace(\"base_inflection = \", \"\"))\n",
    "        if j > 0:\n",
    "            ax.set_ylabel(\"\")\n",
    "        if j < len(row) - 1:\n",
    "            ax.collections[0].colorbar.remove()\n",
    "\n",
    "g.fig.tight_layout()\n",
    "g.fig.savefig(f\"{output_dir}/ff_results-foil.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_weak_ff_results = weak_ff_results.query(\"base_model_name == @focus_base_model and model_name == @focus_model and equivalence == @focus_equivalence\")\n",
    "\n",
    "weak_ff_results_summary2 = focus_weak_ff_results.groupby([\"inflection_from\", \"inflection_to\"]) \\\n",
    "    [[\"correct\", \"gt_label_rank\", \"gt_distance\"]].mean().reset_index()\n",
    "\n",
    "weak_ff_results_summary2[\"transfer_label\"] = weak_ff_results_summary2.inflection_from + \" -> \" + weak_ff_results_summary2.inflection_to\n",
    "\n",
    "# add in data for NNS->NNS and VBZ->VBZ\n",
    "weak_ff_results_summary2 = pd.concat([weak_ff_results_summary2, nnvb_results_summary2.query(\"inflection_from == inflection_to and model_label == 'Word'\")], axis=0)\n",
    "# add in data for VBD->VBD\n",
    "weak_ff_results_summary2 = pd.concat([weak_ff_results_summary2, vbd_results_summary2.query(\"inflection_from == inflection_to and model_label == 'Word'\")], axis=0)\n",
    "\n",
    "weak_ff_results_summary2[\"base_inflection\"] = weak_ff_results_summary2.inflection_from.str.replace(\"-FF\", \"\")\n",
    "\n",
    "weak_ff_results_summary2 = weak_ff_results_summary2[weak_ff_results_summary2.base_inflection.isin(plot_inflections)]\n",
    "\n",
    "g = sns.FacetGrid(data=weak_ff_results_summary2.melt(id_vars=[\"inflection_from\", \"inflection_to\", \"transfer_label\", \"base_inflection\"],\n",
    "                                                value_vars=plot_metrics),\n",
    "                    col=\"base_inflection\", row=\"variable\", sharex=False, sharey=False,\n",
    "                    height=3, aspect=1.25)\n",
    "def mapfn(data, **kwargs):\n",
    "    ax = plt.gca()\n",
    "    metric = data.variable.iloc[0]\n",
    "    vmin = 0 if metric == \"correct\" else data.value.min()\n",
    "    vmax = data.value.max()\n",
    "\n",
    "    sns.heatmap(data.set_index([\"inflection_from\", \"inflection_to\"]).value.unstack(\"inflection_to\"),\n",
    "                vmin=vmin, vmax=vmax, annot=True, ax=ax)\n",
    "    \n",
    "g.map_dataframe(mapfn)\n",
    "\n",
    "for i, row in enumerate(g.axes):\n",
    "    for j, ax in enumerate(row):\n",
    "        ax.set_title(ax.get_title().replace(\"base_inflection = \", \"\"))\n",
    "        if j > 0:\n",
    "            ax.set_ylabel(\"\")\n",
    "        if j < len(row) - 1:\n",
    "            ax.collections[0].colorbar.remove()\n",
    "\n",
    "g.fig.tight_layout()\n",
    "g.fig.savefig(f\"{output_dir}/ff_results_weak.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foil_weak_ff_results = weak_ff_results.query(\"base_model_name == @foil_base_model and model_name == @foil_model and equivalence == @foil_equivalence\")\n",
    "\n",
    "weak_ff_foil_results_summary2 = foil_weak_ff_results.groupby([\"inflection_from\", \"inflection_to\"]) \\\n",
    "    [[\"correct\", \"gt_label_rank\", \"gt_distance\"]].mean().reset_index()\n",
    "\n",
    "weak_ff_foil_results_summary2[\"transfer_label\"] = weak_ff_foil_results_summary2.inflection_from + \" -> \" + weak_ff_foil_results_summary2.inflection_to\n",
    "\n",
    "# add in data for NNS->NNS and VBZ->VBZ\n",
    "weak_ff_foil_results_summary2 = pd.concat([weak_ff_foil_results_summary2, nnvb_results_summary2.query(\"inflection_from == inflection_to and model_label == 'Wav2Vec'\")], axis=0)\n",
    "# add in data for VBD->VBD\n",
    "weak_ff_foil_results_summary2 = pd.concat([weak_ff_foil_results_summary2, vbd_results_summary2.query(\"inflection_from == inflection_to and model_label == 'Wav2Vec'\")], axis=0)\n",
    "\n",
    "weak_ff_foil_results_summary2[\"base_inflection\"] = weak_ff_foil_results_summary2.inflection_from.str.replace(\"-FF\", \"\")\n",
    "\n",
    "weak_ff_foil_results_summary2 = weak_ff_foil_results_summary2[weak_ff_foil_results_summary2.base_inflection.isin(plot_inflections)]\n",
    "\n",
    "g = sns.FacetGrid(data=weak_ff_foil_results_summary2.melt(id_vars=[\"inflection_from\", \"inflection_to\", \"transfer_label\", \"base_inflection\"],\n",
    "                                                value_vars=plot_metrics),\n",
    "                    col=\"base_inflection\", row=\"variable\", sharex=False, sharey=False,\n",
    "                    height=3, aspect=1.25)\n",
    "def mapfn(data, **kwargs):\n",
    "    ax = plt.gca()\n",
    "    metric = data.variable.iloc[0]\n",
    "    vmin = 0 if metric == \"correct\" else data.value.min()\n",
    "    vmax = data.value.max()\n",
    "\n",
    "    sns.heatmap(data.set_index([\"inflection_from\", \"inflection_to\"]).value.unstack(\"inflection_to\"),\n",
    "                vmin=vmin, vmax=vmax, annot=True, ax=ax)\n",
    "    \n",
    "g.map_dataframe(mapfn)\n",
    "for i, row in enumerate(g.axes):\n",
    "    for j, ax in enumerate(row):\n",
    "        ax.set_title(ax.get_title().replace(\"base_inflection = \", \"\"))\n",
    "        if j > 0:\n",
    "            ax.set_ylabel(\"\")\n",
    "        if j < len(row) - 1:\n",
    "            ax.collections[0].colorbar.remove()\n",
    "\n",
    "g.fig.tight_layout()\n",
    "g.fig.savefig(f\"{output_dir}/ff_results_weak-foil.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(data=focus_ff_results,\n",
    "            x=\"from_freq_bin\", y=\"correct\", hue=\"inflection_from\",\n",
    "            col=\"inflection_to\", col_wrap=4, units=\"base_from\", kind=\"point\", height=3)\n",
    "g.figure.suptitle(\"Effect of source frequency\")\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_title(ax.get_title().split(\"=\")[1].strip())\n",
    "g.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(data=focus_ff_results,\n",
    "            x=\"from_freq_bin\", y=\"gt_distance\", hue=\"inflection_from\",\n",
    "            col=\"inflection_to\", col_wrap=4, units=\"base_from\", kind=\"point\", height=3)\n",
    "g.figure.suptitle(\"Effect of source frequency\")\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_title(ax.get_title().split(\"=\")[1].strip())\n",
    "g.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(data=focus_ff_results,\n",
    "            x=\"to_freq_bin\", y=\"gt_label_rank\", hue=\"inflection_from\",\n",
    "            col=\"inflection_to\", row=\"model_name\", units=\"base_to\", kind=\"point\", height=3)\n",
    "g.figure.suptitle(\"Effect of target frequency\")\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_title(ax.get_title().split(\"=\")[1].strip())\n",
    "g.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(data=focus_ff_results,\n",
    "            x=\"to_freq_bin\", y=\"gt_label_rank\", hue=\"inflection_from\",\n",
    "            col=\"inflection_to\", row=\"model_name\", units=\"base_to\", kind=\"point\", height=3)\n",
    "g.figure.suptitle(\"Effect of target frequency\")\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_title(ax.get_title().split(\"=\")[1].strip())\n",
    "g.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forced-choice analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = StateSpaceAnalysisSpec.from_hdf5(state_space_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_nns_vbz_allomorph(base_phones):\n",
    "    \"\"\"\n",
    "    Given a list of CMUDICT phones for a base form, \n",
    "    return the 'expected' post-divergence allomorph \n",
    "    (S, Z, or IH Z, etc.) for the English plural / 3sg verb.\n",
    "    \"\"\"\n",
    "    last_phone = base_phones[-1]\n",
    "\n",
    "    # Define sets or lists for final-phoneme checks\n",
    "    SIBILANTS = {\"S\", \"Z\", \"SH\", \"CH\", \"JH\", \"ZH\"}\n",
    "    VOICELESS = {\"P\", \"T\", \"K\", \"F\", \"TH\"}  # Could add others as needed\n",
    "    \n",
    "    if last_phone in SIBILANTS:\n",
    "        # e.g., 'CH' -> \"IH Z\"\n",
    "        return \"IH Z\"\n",
    "    elif last_phone in VOICELESS:\n",
    "        # e.g., 'K', 'P', 'T' -> \"S\"\n",
    "        return \"S\"\n",
    "    else:\n",
    "        # default to voiced => \"Z\"\n",
    "        return \"Z\"\n",
    "\n",
    "\n",
    "def guess_past_allomorph(base_phones):\n",
    "    \"\"\"\n",
    "    Given a list of CMUDICT phones for a base form,\n",
    "    return the 'expected' post-divergence allomorph\n",
    "    (T, D, or IH D) for the English past tense.\n",
    "    \"\"\"\n",
    "    last_phone = base_phones[-1]\n",
    "\n",
    "    ALVEOLAR_STOPS = {\"T\", \"D\"}\n",
    "    # Example set of voiceless consonants (non-exhaustive—adjust as needed).\n",
    "    VOICELESS = {\"P\", \"F\", \"K\", \"S\", \"SH\", \"CH\", \"TH\"}\n",
    "    \n",
    "    if last_phone in ALVEOLAR_STOPS:\n",
    "        # E.g., \"want\" -> \"wanted\" => \"AH0 D\"\n",
    "        return \"IH D\"\n",
    "    elif last_phone in VOICELESS:\n",
    "        # E.g., \"jump\" -> \"jumped\" => \"T\"\n",
    "        return \"T\"\n",
    "    else:\n",
    "        # default to voiced => \"D\"\n",
    "        return \"D\"\n",
    "    \n",
    "STRONG_GUESSERS = {\n",
    "    \"Z_S\": guess_nns_vbz_allomorph,\n",
    "    \"D_T\": guess_past_allomorph,\n",
    "    \"T_IH D\": guess_past_allomorph,\n",
    "    \"D_IH D\": guess_past_allomorph,\n",
    "}\n",
    "\n",
    "WEAK_SPACES = {\n",
    "    \"Z_S\": {\"Z\", \"S\", \"IH Z\"},\n",
    "    \"D_T\": {\"D\", \"T\", \"IH D\"},\n",
    "    \"T_IH D\": {\"D\", \"T\", \"IH D\"},\n",
    "    \"D_IH D\": {\"D\", \"T\", \"IH D\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts_df = state_space.cuts.xs(\"phoneme\", level=\"level\").drop(columns=[\"onset_frame_idx\", \"offset_frame_idx\"])\n",
    "cuts_df[\"label_idx\"] = cuts_df.index.get_level_values(\"label\").map({l: i for i, l in enumerate(state_space.labels)})\n",
    "cuts_df[\"frame_idx\"] = cuts_df.groupby([\"label\", \"instance_idx\"]).cumcount()\n",
    "cuts_df = cuts_df.reset_index().set_index([\"label\", \"instance_idx\", \"frame_idx\"]).sort_index()\n",
    "cut_phonemic_forms = cuts_df.groupby([\"label\", \"instance_idx\"]).description.agg(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_exclude = \"ay ba c des eh p k pa ca na b co ben been shun own\".split()\n",
    "fc_exclude_inflected = \"look'd push'd los ince\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_results = all_results.loc[all_results.inflection_to.str.startswith(\"FC-\")].copy()\n",
    "fc_results = fc_results[~fc_results.base_to.isin(fc_exclude)]\n",
    "fc_results[\"fc_pair\"] = fc_results.inflection_to.str.extract(r\"FC-([\\w\\s_]+)\")\n",
    "fc_results[\"inflected_from\"] = fc_results.from_equiv_label.apply(lambda x: eval(x)[1])\n",
    "fc_results[\"inflected_to\"] = fc_results.to_equiv_label.apply(lambda x: eval(x)[1])\n",
    "fc_results[\"layer\"] = fc_results.base_model_name.str.extract(r\"_(\\d+)$\").astype(int)\n",
    "fc_results = fc_results[~fc_results.inflected_to.isin(fc_exclude_inflected)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_metadata = all_cross_instances[all_cross_instances.inflection.str.startswith(\"FC-\")] \\\n",
    "    .groupby([\"inflection\", \"base\", \"inflected\"]).head(1) \\\n",
    "        [[\"inflection\", \"base\", \"base_phones\",\n",
    "          \"inflected\", \"inflected_phones\", \"inflected2\", \"inflected2_phones\",\n",
    "          \"post_divergence\"]] \\\n",
    "    .rename(columns={\"base\": \"base_to\", \"base_phones\": \"base_to_phones\",\n",
    "                     \"inflected\": \"inflected_to\", \"inflected_phones\": \"inflected_to_phones\",\n",
    "                     \"inflected2\": \"inflected2_to\", \"inflected2_phones\": \"inflected2_to_phones\",\n",
    "                     \"post_divergence\": \"post_divergence_to\"})\n",
    "fc_metadata[\"fc_pair\"] = fc_metadata.inflection.str.extract(r\"FC-([\\w\\s_]+)\")\n",
    "fc_metadata = fc_metadata.drop(columns=[\"inflection\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_results = pd.merge(fc_results, fc_metadata, on=[\"fc_pair\", \"base_to\", \"inflected_to\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_results[\"strong_expected\"] = fc_results.apply(lambda xs: STRONG_GUESSERS[xs.fc_pair](xs.base_to_phones.split(\" \")), axis=1)\n",
    "fc_results[\"strong_phones\"] = fc_results.apply(lambda xs: \" \".join([xs.base_to_phones, STRONG_GUESSERS[xs.fc_pair](xs.base_to_phones.split(\" \"))]), axis=1)\n",
    "fc_results[\"inflected_to_strong\"] = fc_results.apply(lambda xs: xs.inflected_to_phones[len(xs.base_to_phones) + 1:] == xs.strong_expected, axis=1)\n",
    "fc_results[\"inflected2_to_strong\"] = fc_results.apply(lambda xs: xs.inflected2_to_phones[len(xs.base_to_phones) + 1:] == xs.strong_expected, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in metadata about \"from\" item\n",
    "fc_from_metadata = all_cross_instances[all_cross_instances.inflection.isin(fc_results.inflection_from)] \\\n",
    "    .groupby([\"inflection\", \"base\", \"inflected\"]).apply(lambda xs: xs.post_divergence.value_counts().index[0]) \\\n",
    "    .rename(\"post_divergence\") \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={\"inflection\": \"inflection_from\", \"base\": \"base_from\", \"inflected\": \"inflected_from\", \"post_divergence\": \"post_divergence_from\"})\n",
    "\n",
    "fc_results = pd.merge(fc_results, fc_from_metadata, on=[\"inflection_from\", \"base_from\", \"inflected_from\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the most frequent suffixes\n",
    "keep_post_divergence_n = 4\n",
    "keep_post_divergence = fc_results.groupby(\"fc_pair\").apply(lambda xs: xs.post_divergence_from.value_counts().head(keep_post_divergence_n))\n",
    "fc_results = pd.concat([\n",
    "    fc_results[(fc_results.fc_pair == fc_pair) & fc_results.post_divergence_from.isin(rows.index.get_level_values(\"post_divergence_from\"))]\n",
    "    for fc_pair, rows in keep_post_divergence.groupby(\"fc_pair\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_items = pd.concat([\n",
    "    fc_results.query(\"inflected_to_strong\").groupby([\"fc_pair\", \"base_to\"]).inflected_to.apply(lambda xs: xs.head(1)).droplevel(-1),\n",
    "    fc_results.query(\"inflected2_to_strong\").groupby([\"fc_pair\", \"base_to\"]).inflected2_to.apply(lambda xs: xs.head(1)).droplevel(-1)\n",
    "], axis=0)\n",
    "weak_items = pd.concat([\n",
    "    fc_results.query(\"not inflected_to_strong\").groupby([\"fc_pair\", \"base_to\"]).inflected_to.apply(lambda xs: xs.head(1)).droplevel(-1),\n",
    "    fc_results.query(\"not inflected2_to_strong\").groupby([\"fc_pair\", \"base_to\"]).inflected2_to.apply(lambda xs: xs.head(1)).droplevel(-1)\n",
    "], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import itertools\n",
    "\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def _get_strong_items(fc_pair: str, base_phones: tuple[str, ...]):\n",
    "    strong_phones = \" \".join([*base_phones, STRONG_GUESSERS[fc_pair](base_phones)])\n",
    "    homophones = cut_phonemic_forms[cut_phonemic_forms == strong_phones].index.get_level_values(\"label\").unique()\n",
    "    return \" \".join(homophones)\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def _get_weak_items(fc_pair, base_phones: tuple[str, ...]):\n",
    "    weak_suffixes = WEAK_SPACES[fc_pair] - set([STRONG_GUESSERS[fc_pair](base_phones)])\n",
    "    homophones = set()\n",
    "    for suffix in weak_suffixes:\n",
    "        homophones |= set(cut_phonemic_forms[cut_phonemic_forms == \" \".join([*base_phones, suffix])].index.get_level_values(\"label\").unique())\n",
    "    return \" \".join(homophones)\n",
    "    \n",
    "def get_strong_items(ser):\n",
    "    fc_pair, base_phones = tuple(ser)\n",
    "    base_phones = tuple(base_phones.split(\" \"))\n",
    "    return _get_strong_items(fc_pair, base_phones)\n",
    "def get_weak_items(ser):\n",
    "    fc_pair, base_phones = tuple(ser)\n",
    "    base_phones = tuple(base_phones.split(\" \"))\n",
    "    return _get_weak_items(fc_pair, base_phones)\n",
    "fc_results[\"strong_item_to\"] = fc_results[[\"fc_pair\", \"base_to_phones\"]].apply(get_strong_items, axis=1)\n",
    "fc_results[\"weak_item_to\"] = fc_results[[\"fc_pair\", \"base_to_phones\"]].apply(get_weak_items, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude any items for which the orthographic strong/weak forms overlap\n",
    "fc_results = fc_results[fc_results.apply(lambda xs: set(xs.strong_item_to.split()).isdisjoint(set(xs.weak_item_to.split())), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_results[\"chose_strong\"] = fc_results.apply(lambda xs: (re.search(f\"\\\\b{xs.predicted_label}\\\\b\", xs.strong_item_to) is not None) if xs.strong_item_to is not None else None, axis=1)\n",
    "fc_results[\"chose_weak\"] = fc_results.apply(lambda xs: (re.search(f\"\\\\b{xs.predicted_label}\\\\b\", xs.weak_item_to) is not None) if xs.weak_item_to is not None else None, axis=1)\n",
    "fc_results[\"chose_strong_or_weak\"] = fc_results.chose_strong.fillna(False) | fc_results.chose_weak.fillna(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_results.groupby(\"fc_pair\").base_to.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_results.query(\"base_model_name == 'w2v2_8' and model_name == 'ff_32' and fc_pair == 'Z_S'\").groupby(\"base_to\").chose_strong.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=fc_results.groupby(run_groupers + [\"layer\", \"fc_pair\", \"base_to\"]).apply(\n",
    "    lambda xs: pd.Series({\n",
    "        \"chose_strong_norm\": xs.chose_strong.sum() / xs.chose_strong_or_weak.sum(),\n",
    "        \"chose_weak_norm\": xs.chose_weak.sum() / xs.chose_strong_or_weak.sum()\n",
    "    })) \\\n",
    "    .dropna().reset_index(),\n",
    "    x=\"layer\", col=\"model_name\", y=\"chose_strong_norm\", hue=\"fc_pair\", kind=\"point\", errorbar=\"se\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_fc_results = fc_results.query(\"base_model_name == @focus_base_model and model_name == @focus_model and equivalence == @focus_equivalence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_fc_results.groupby(\"fc_pair\").sample(4)[[\"fc_pair\", \"base_to\", \"inflected_to\", \"strong_item_to\", \"weak_item_to\", \"base_from\", \"inflected_from\", \"predicted_label\", \"chose_strong\", \"chose_weak\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=focus_fc_results.groupby([\"fc_pair\", \"post_divergence_from\", \"base_to\"]).chose_strong.mean().dropna().reset_index(),\n",
    "            x=\"fc_pair\", hue=\"post_divergence_from\", y=\"chose_strong\", kind=\"bar\", errorbar=\"se\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc_results_by_word = fc_results.groupby(run_groupers + [\"layer\", \"fc_pair\", \"base_to\"]).apply(\n",
    "#     lambda xs: pd.Series({\n",
    "#         \"chose_strong_norm\": xs.chose_strong.sum() / xs.chose_strong_or_weak.sum(),\n",
    "#         \"chose_weak_norm\": xs.chose_weak.sum() / xs.chose_strong_or_weak.sum(),\n",
    "#         \"chose_strong_or_weak_proportion\": xs.chose_strong_or_weak.mean(),\n",
    "#     }))\n",
    "# fc_results_by_word = fc_results_by_word[fc_results_by_word.chose_strong_or_weak_proportion > 0.1] \\\n",
    "#     .dropna().reset_index().sort_values(\"chose_strong_norm\").assign(run_name=lambda xs: xs.model_name + \" \" + xs.layer.map(\"{:02d}\".format))\n",
    "\n",
    "# g = sns.FacetGrid(data=fc_results_by_word, col=\"fc_pair\", row=\"run_name\",\n",
    "#                   row_order=sorted(fc_results_by_word.run_name.unique()),\n",
    "#                   height=12, aspect=0.6, sharey=False)\n",
    "# def f(data, **kwargs):\n",
    "#     sns.heatmap(data=data.set_index(\"base_to\")[[\"chose_strong_norm\", \"chose_weak_norm\"]],\n",
    "#                 vmin=0, vmax=1, cbar=False, **kwargs)\n",
    "# g.map_dataframe(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_results_by_word_and_source = focus_fc_results.groupby(run_groupers + [\"fc_pair\", \"post_divergence_from\", \"base_to\"]).apply(\n",
    "    lambda xs: pd.Series({\n",
    "        \"chose_strong_norm\": xs.chose_strong.sum() / xs.chose_strong_or_weak.sum(),\n",
    "        \"chose_weak_norm\": xs.chose_weak.sum() / xs.chose_strong_or_weak.sum()\n",
    "    })) \\\n",
    "    .dropna().reset_index().sort_values(\"chose_strong_norm\")\n",
    "\n",
    "sns.catplot(data=fc_results_by_word_and_source,\n",
    "            col=\"fc_pair\", x=\"post_divergence_from\", y=\"chose_strong_norm\",\n",
    "            kind=\"bar\", errorbar=\"se\", sharex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.displot(data=fc_results_by_word_and_source.query(\"fc_pair == 'Z_S'\").assign(post_divergence_from=lambda xs: xs.post_divergence_from.map({\"S\": \"s\", \"Z\": \"z\", \"IH Z\": \"ɪz\", \"AH Z\": \"ɪz\"})),\n",
    "                hue=\"post_divergence_from\", x=\"chose_strong_norm\",\n",
    "                kind=\"ecdf\", height=4, aspect=1.5, linewidth=4)\n",
    "\n",
    "ax = g.axes.flat[0]\n",
    "ax.set_xlabel(\"Proportion of phonologically\\nconsistent choices\")\n",
    "ax.set_ylabel(\"Proportion of\\nword types\")\n",
    "g.legend.set_title(\"Source\\nallomorph\")\n",
    "g.legend.set_bbox_to_anchor((0.5, 0.6), transform=ax.transAxes)\n",
    "\n",
    "g.tight_layout()\n",
    "g.savefig(f\"{output_dir}/fc_results-ecdf.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.catplot(data=fc_results_by_word_and_source.query(\"fc_pair == 'Z_S'\").assign(post_divergence_from=lambda xs: xs.post_divergence_from.map({\"S\": \"s\", \"Z\": \"z\", \"IH Z\": \"ɪz\", \"AH Z\": \"ɪz\"})),\n",
    "#             col=\"fc_pair\", y=\"post_divergence_from\", x=\"chose_strong_norm\",\n",
    "#             kind=\"swarm\", height=7, sharex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(data=fc_results_by_word_and_source.query(\"fc_pair == 'Z_S'\").assign(post_divergence_from=lambda xs: xs.post_divergence_from.map({\"S\": \"s\", \"Z\": \"z\", \"IH Z\": \"ɪz\", \"AH Z\": \"ɪz\"})),\n",
    "                x=\"post_divergence_from\", y=\"chose_strong_norm\",\n",
    "                kind=\"bar\", errorbar=\"se\", sharex=False, height=4)\n",
    "\n",
    "ax = g.axes.flat[0]\n",
    "ax.set_ylabel(\"Proportion of\\nphonologically\\nconsistent choices\")\n",
    "ax.set_xlabel(\"Source allomorph\")\n",
    "\n",
    "g.savefig(f\"{output_dir}/fc_results_by_word_and_source-Z_S.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = sns.FacetGrid(data=fc_results_by_word_and_source, col=\"fc_pair\", row=\"post_divergence_from\",\n",
    "#                   height=12, aspect=0.6, sharey=False)\n",
    "# def f(data, **kwargs):\n",
    "#     sns.heatmap(data=data.set_index(\"base_to\")[[\"chose_strong_norm\", \"chose_weak_norm\"]],\n",
    "#                 vmin=0, vmax=1, cbar=False, **kwargs)\n",
    "# g.map_dataframe(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_results = pd.merge(fc_results, word_freq_df.LogFreq.rename(\"from_base_freq\"),\n",
    "                            left_on=\"base_from\", right_index=True)\n",
    "fc_results = pd.merge(fc_results, word_freq_df.LogFreq.rename(\"from_inflected_freq\"),\n",
    "                            left_on=\"inflected_from\", right_index=True)\n",
    "fc_results = pd.merge(fc_results, word_freq_df.LogFreq.rename(\"to_base_freq\"),\n",
    "                              left_on=\"base_to\", right_index=True)\n",
    "fc_results = pd.merge(fc_results, word_freq_df.LogFreq.rename(\"to_inflected_freq\"),\n",
    "                            left_on=\"inflected_to\", right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_results[\"from_freq\"] = fc_results[[\"from_base_freq\", \"from_inflected_freq\"]].mean(axis=1)\n",
    "fc_results[\"to_freq\"] = fc_results[[\"to_base_freq\", \"to_inflected_freq\"]].mean(axis=1)\n",
    "\n",
    "_, fc_frequency_bins = pd.qcut(pd.concat([fc_results.to_freq, fc_results.from_freq]), q=4, retbins=True)\n",
    "fc_results[\"from_freq_bin\"] = pd.cut(fc_results.from_freq, bins=fc_frequency_bins, labels=[f\"Q{i}\" for i in range(1, 5)])\n",
    "fc_results[\"to_freq_bin\"] = pd.cut(fc_results.to_freq, bins=fc_frequency_bins, labels=[f\"Q{i}\" for i in range(1, 5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_fc_results = fc_results.query(\"base_model_name == @focus_base_model and model_name == @focus_model and equivalence == @focus_equivalence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=focus_fc_results.groupby([\"fc_pair\", \"post_divergence_from\", \"base_to\", \"from_freq_bin\"]).apply(\n",
    "    lambda xs: pd.Series({\n",
    "        \"chose_strong_norm\": xs.chose_strong.sum() / xs.chose_strong_or_weak.sum(),\n",
    "        \"chose_weak_norm\": xs.chose_weak.sum() / xs.chose_strong_or_weak.sum()\n",
    "    })) \\\n",
    "    .dropna().reset_index().sort_values(\"chose_strong_norm\"),\n",
    "    x=\"from_freq_bin\", y=\"chose_strong_norm\", hue=\"post_divergence_from\", col=\"fc_pair\", kind=\"point\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=focus_fc_results.groupby([\"fc_pair\", \"post_divergence_from\", \"base_to\", \"to_freq_bin\"]).apply(\n",
    "    lambda xs: pd.Series({\n",
    "        \"chose_strong_norm\": xs.chose_strong.sum() / xs.chose_strong_or_weak.sum(),\n",
    "        \"chose_weak_norm\": xs.chose_weak.sum() / xs.chose_strong_or_weak.sum()\n",
    "    })) \\\n",
    "    .dropna().reset_index().sort_values(\"chose_strong_norm\"),\n",
    "    x=\"to_freq_bin\", y=\"chose_strong_norm\", hue=\"post_divergence_from\",  col=\"fc_pair\", kind=\"point\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory for viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl = nnvb_focus.query(\"model_name == 'ffff_32' and ((inflection_from == 'VBZ' and inflection_to == 'NNS') or (inflection_from == 'NNS' and inflection_to == 'VBZ'))\").reset_index()\n",
    "expl.loc[expl.inflection_to == \"NNS\", \"involved_noun\"] = expl.base_to\n",
    "expl.loc[expl.inflection_from == \"NNS\", \"involved_noun\"] = expl.base_from\n",
    "expl.loc[expl.inflection_to == \"NNS\", \"involved_allomorph\"] = expl.allomorph_to\n",
    "expl.loc[expl.inflection_from == \"NNS\", \"involved_allomorph\"] = expl.allomorph_from\n",
    "expl[\"direction\"] = expl.apply(lambda xs: \"target\" if xs.inflection_from == \"VBZ\" else \"source\", axis=1)\n",
    "# only include stable counts\n",
    "expl = expl.groupby([\"involved_noun\", \"direction\"]).filter(lambda xs: len(xs) > 5)\n",
    "expl = expl.groupby([\"involved_noun\", \"involved_allomorph\", \"direction\"]).gt_label_rank.mean().unstack().dropna()\n",
    "expl[\"ratio\"] = (expl.target + 1) / (expl.source + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl.sort_values(\"ratio\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl.sort_values(\"ratio\").tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl.sort_values(\"target\").head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl.sort_values(\"source\").head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnvb_focus.query(\"model_name == 'ffff_32' and base_to == 'cedar'\").gt_label_rank.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(expl.sort_values(\"ratio\")[[\"source\", \"target\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory for viz 2\n",
    "\n",
    "Find a noun N and a verb V for which performance is high within-inflection but for which transfer in one or both directions is poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnvb_focus.query(\"model_name == 'ffff_32' and inflection_from == inflection_to\") \\\n",
    "    .groupby([\"inflection_from\", \"base_to\"]).filter(lambda xs: len(xs) > 5) \\\n",
    "    .groupby([\"inflection_from\", \"base_from\"]).filter(lambda xs: len(xs) > 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl2 = nnvb_focus.query(\"model_name == 'ffff_32' and inflection_from == inflection_to\") \\\n",
    "    .groupby([\"inflection_from\", \"base_to\"]).filter(lambda xs: len(xs) > 5) \\\n",
    "    .groupby([\"inflection_from\", \"base_from\"]).filter(lambda xs: len(xs) > 5) \\\n",
    "\n",
    "expl2 = pd.DataFrame({\n",
    "    \"target_rank\": expl2.groupby([\"inflection_from\", \"base_to\"]).gt_label_rank.mean(),\n",
    "    \"source_rank\": expl2.groupby([\"inflection_from\", \"base_from\"]).gt_label_rank.mean()\n",
    "}).dropna()\n",
    "expl2[\"ratio\"] = (expl2.target_rank + 1) / (expl2.source_rank + 1)\n",
    "expl2.index.names = [\"inflection\", \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl2.sort_values(\"ratio\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl2.sort_values(\"ratio\").tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl2[(expl2.target_rank < 1) & (expl2.source_rank < 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_ranks = nnvb_focus.query(\"model_name == 'ffff_32' and inflection_from != inflection_to\") \\\n",
    "    .groupby([\"inflection_from\", \"base_to\"]).filter(lambda xs: len(xs) > 5) \\\n",
    "    .groupby([\"inflection_from\", \"base_from\"]).filter(lambda xs: len(xs) > 5)\n",
    "\n",
    "transfer_ranks = pd.DataFrame({\n",
    "    \"source_transfer_rank\": transfer_ranks.groupby([\"inflection_from\", \"base_from\"]).gt_label_rank.mean(),\n",
    "    \"target_transfer_rank\": transfer_ranks.groupby([\"inflection_to\", \"base_to\"]).gt_label_rank.mean()\n",
    "}).dropna()\n",
    "transfer_ranks.index.names = [\"inflection\", \"label\"]\n",
    "transfer_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl3 = pd.merge(expl2, transfer_ranks, left_index=True, right_index=True)\n",
    "expl3[\"mean_within\"] = (expl3.source_rank + expl3.target_rank) / 2\n",
    "expl3[\"mean_transfer\"] = (expl3.source_transfer_rank + expl3.target_transfer_rank) / 2\n",
    "expl3[\"ratio\"] = (expl3.mean_within + 1) / (expl3.mean_transfer + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl3[(expl3.target_rank < 1) & (expl3.source_rank < 1)]# & (expl3.source_transfer_rank > 1) & (expl3.target_transfer_rank > 1)].sort_values(\"ratio\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
