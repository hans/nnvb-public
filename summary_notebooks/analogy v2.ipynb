{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import logit\n",
    "\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec\n",
    "from src.utils import concat_csv_with_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"paper\", font_scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_friends_path = \"outputs/analogy/inputs/librispeech-train-clean-100/w2v2/false_friends.csv\"\n",
    "state_space_path = \"outputs/analogy/inputs/librispeech-train-clean-100/w2v2/state_space_spec.h5\"\n",
    "output_dir = \"analogy_figures\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping variables on experiment results dataframe to select a single run\n",
    "run_groupers = [\"base_model_name\", \"model_name\", \"equivalence\"]\n",
    "\n",
    "plot_runs = [(f\"w2v2_{i}\", \"ff_32\", \"word_broad_10frames_fixedlen25\") for i in range(12)] + \\\n",
    "            [(f\"w2v2_{i}\", \"id\", \"id\") for i in range(12)]\n",
    "# [(f\"w2v2_{i}\", \"discrim-ff_32\", \"word_broad_10frames_fixedlen25\") for i in range(12)] + \\\n",
    "\n",
    "main_plot_run = (\"w2v2_8\", \"ff_32\", \"word_broad_10frames_fixedlen25\")\n",
    "# choose a vmin, vmax so that all heatmaps have the same color scale\n",
    "main_plot_vmin, main_plot_vmax = 0.4, 0.9\n",
    "\n",
    "plot_inflections = [\"NNS\", \"VBZ\", \"VBD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_df = pd.read_csv(\"data/WorldLex_Eng_US.Freq.2.txt\", sep=\"\\t\", index_col=\"Word\")\n",
    "# compute weighted average frequency across domains\n",
    "word_freq_df[\"BlogFreq_rel\"] = word_freq_df.BlogFreq / word_freq_df.BlogFreq.sum()\n",
    "word_freq_df[\"TwitterFreq_rel\"] = word_freq_df.TwitterFreq / word_freq_df.TwitterFreq.sum()\n",
    "word_freq_df[\"NewsFreq_rel\"] = word_freq_df.NewsFreq / word_freq_df.NewsFreq.sum()\n",
    "word_freq_df[\"Freq\"] = word_freq_df[[\"BlogFreq\", \"TwitterFreq\", \"NewsFreq\"]].mean(axis=1) \\\n",
    "    * word_freq_df[[\"BlogFreq\", \"TwitterFreq\", \"NewsFreq\"]].sum().mean()\n",
    "word_freq_df[\"LogFreq\"] = np.log10(word_freq_df.Freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_friends_df = pd.read_csv(false_friends_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_friends_df.query(\"inflection == 'VBD' and not strong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical generalization matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflections = [\"NNS\", \"VBZ\"]\n",
    "allomorphs = [\"Z\", \"S\", \"IH Z\"]\n",
    "rows = [{\"from_inflection\": from_inflection, \"from_allomorph\": from_allomorph,\n",
    "            \"to_inflection\": to_inflection, \"to_allomorph\": to_allomorph}\n",
    "        for from_inflection in inflections\n",
    "        for from_allomorph in allomorphs\n",
    "        for to_inflection in inflections\n",
    "        for to_allomorph in allomorphs]\n",
    "\n",
    "generalization_df = pd.DataFrame(rows)\n",
    "generalization_df[\"source_label\"] = generalization_df[\"from_inflection\"] + \" \" + generalization_df[\"from_allomorph\"]\n",
    "generalization_df[\"target_label\"] = generalization_df[\"to_inflection\"] + \" \" + generalization_df[\"to_allomorph\"]\n",
    "generalization_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phonetic_generalization_df = generalization_df.copy()\n",
    "phonetic_generalization_df[\"correct\"] = False\n",
    "phonetic_generalization_df.loc[phonetic_generalization_df.from_allomorph == phonetic_generalization_df.to_allomorph, \"correct\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(phonetic_generalization_df.set_index([\"source_label\", \"target_label\"]).correct.unstack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morphological_generalization_df = generalization_df.copy()\n",
    "morphological_generalization_df[\"correct\"] = False\n",
    "morphological_generalization_df.loc[phonetic_generalization_df.from_inflection == phonetic_generalization_df.to_inflection, \"correct\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(morphological_generalization_df.set_index([\"source_label\", \"target_label\"]).correct.unstack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaphon_generalization_df = generalization_df.copy()\n",
    "metaphon_generalization_df[\"correct\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(metaphon_generalization_df.set_index([\"source_label\", \"target_label\"]).correct.unstack(),\n",
    "            vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With false friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflections = [\"NNS\", \"VBZ\"]\n",
    "allomorphs = [\"Z\", \"S\", \"IH Z\"]\n",
    "rows = [{\"from_inflection_base\": from_inflection, \"from_allomorph\": from_allomorph,\n",
    "         \"to_inflection_base\": to_inflection, \"to_allomorph\": to_allomorph,\n",
    "         \"ff_from\": ff_from, \"ff_to\": ff_to}\n",
    "        for from_inflection in inflections\n",
    "        for from_allomorph in allomorphs\n",
    "        for to_inflection in inflections\n",
    "        for to_allomorph in allomorphs\n",
    "        for ff_from in [False, True]\n",
    "        for ff_to in [False, True]]\n",
    "\n",
    "ff_generalization_df = pd.DataFrame(rows)\n",
    "ff_generalization_df[\"from_inflection\"] = ff_generalization_df[\"from_inflection_base\"] + ff_generalization_df.ff_from.map({False: \"\", True: \"-FF\"})\n",
    "ff_generalization_df[\"to_inflection\"] = ff_generalization_df[\"to_inflection_base\"] + ff_generalization_df.ff_to.map({False: \"\", True: \"-FF\"})\n",
    "ff_generalization_df[\"source_label\"] = ff_generalization_df[\"from_inflection\"] + \" \" + ff_generalization_df[\"from_allomorph\"]\n",
    "ff_generalization_df[\"target_label\"] = ff_generalization_df[\"to_inflection\"] + \" \" + ff_generalization_df[\"to_allomorph\"]\n",
    "ff_generalization_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_phonetic_generalization_df = ff_generalization_df.copy()\n",
    "ff_phonetic_generalization_df[\"correct\"] = False\n",
    "ff_phonetic_generalization_df.loc[ff_phonetic_generalization_df.from_allomorph == ff_phonetic_generalization_df.to_allomorph, \"correct\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(ff_phonetic_generalization_df.set_index([\"source_label\", \"target_label\"]).correct.unstack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_morphological_generalization_df = ff_generalization_df.copy()\n",
    "ff_morphological_generalization_df[\"correct\"] = False\n",
    "ff_morphological_generalization_df.loc[(ff_phonetic_generalization_df.from_inflection == ff_phonetic_generalization_df.to_inflection)\n",
    "                                       & ~ff_phonetic_generalization_df.from_inflection.str.contains(\"-FF\"), \"correct\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(ff_morphological_generalization_df.set_index([\"source_label\", \"target_label\"]).correct.unstack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_metaphon_generalization_df = ff_generalization_df.copy()\n",
    "ff_metaphon_generalization_df[\"correct\"] = 0.3\n",
    "ff_metaphon_generalization_df.loc[~ff_metaphon_generalization_df.ff_from &\n",
    "                                  ~ff_metaphon_generalization_df.ff_to, \"correct\"] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(ff_metaphon_generalization_df.set_index([\"source_label\", \"target_label\"]).correct.unstack(),\n",
    "            vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = concat_csv_with_indices(\n",
    "        \"outputs/analogy/runs/**/experiment_results.csv\",\n",
    "        [lambda p: p.parent.name, lambda p: p.parents[1].name,\n",
    "            lambda p: p.parents[2].name],\n",
    "        [\"equivalence\", \"model_name\", \"base_model_name\"]) \\\n",
    "    .droplevel(-1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_id_results = concat_csv_with_indices(\n",
    "        \"outputs/analogy/runs_id/**/experiment_results.csv\",\n",
    "        [lambda p: p.parent.name],\n",
    "        [\"base_model_name\"]) \\\n",
    "    .droplevel(-1).reset_index()\n",
    "all_id_results[\"model_name\"] = \"id\"\n",
    "all_id_results[\"equivalence\"] = \"id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.concat([all_results, all_id_results], ignore_index=True)\n",
    "all_results[\"group\"] = all_results.group.apply(lambda x: eval(x) if not (isinstance(x, float) and np.isnan(x)) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh analogy_results_20250217.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lw = all_results.query(\"experiment == 'regular'\").copy()\n",
    "plot_lw = plot_lw.groupby(run_groupers + [\"group\", \"inflection_from\"]).correct.mean()\n",
    "plot_lw = plot_lw.reindex([(*plot_run, group, inflection_from)\n",
    "                           for group in plot_lw.index.get_level_values(\"group\").unique()\n",
    "                           for inflection_from in plot_lw.index.get_level_values(\"inflection_from\").unique()\n",
    "                           for plot_run in plot_runs]).reset_index()\n",
    "plot_lw[\"group0\"] = plot_lw.group.apply(lambda x: x[0] if x is not None else None)\n",
    "plot_lw[\"group1\"] = plot_lw.group.apply(lambda x: x[1] if x is not None else None)\n",
    "plot_lw[\"layer\"] = plot_lw.base_model_name.str.extract(r\"_(\\d+)$\").astype(int)\n",
    "\n",
    "lw_random = plot_lw[plot_lw.group0 == \"random\"].groupby([\"inflection_from\", \"layer\"]).correct.mean().reset_index().dropna()\n",
    "\n",
    "plot_lw = plot_lw[plot_lw.inflection_from.isin(plot_inflections)]\n",
    "plot_lw = plot_lw[(plot_lw.group1 == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lw_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(data=plot_lw, x=\"layer\", y=\"correct\", hue=\"model_name\", row=\"inflection_from\",\n",
    "                kind=\"point\", height=3, aspect=3)\n",
    "\n",
    "for ax, row_name in zip(g.axes.flat, g.row_names):\n",
    "    sns.lineplot(data=lw_random,\n",
    "                 x=\"layer\", y=\"correct\", ax=ax, color=\"gray\", linestyle=\"--\",\n",
    "                 legend=None)\n",
    "    ax.set_title(ax.get_title().split(\"=\")[1].strip())\n",
    "    ax.set_ylabel(\"Correct\")\n",
    "    if ax.get_xlabel() == \"layer\":\n",
    "        ax.set_xlabel(\"Layer\")\n",
    "\n",
    "# g.figure.tight_layout()\n",
    "g.figure.savefig(f\"{output_dir}/layer_wise.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mca = pd.read_csv(\"outputs/analogy/inputs/librispeech-train-clean-100/w2v2/most_common_allomorphs.csv\", index_col=0)\n",
    "mca.query(\"inflection == 'VBD'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute controlled NNVB results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nnvb_results = []\n",
    "\n",
    "for run, run_results in all_results.groupby(run_groupers):\n",
    "    run_results = run_results.set_index(\"experiment\")\n",
    "    nnvb_expts = run_results.index.unique()\n",
    "    nnvb_expts = nnvb_expts[nnvb_expts.str.contains(\"unambiguous-\")]\n",
    "\n",
    "    for expt in nnvb_expts:\n",
    "        inflection_from, allomorph_from, inflection_to, allomorph_to = \\\n",
    "            re.findall(r\"unambiguous-(\\w+)_([\\w\\s]+)_to_(\\w+)_([\\w\\s]+)\", expt)[0]\n",
    "        expt_df = run_results.loc[expt].copy()\n",
    "\n",
    "        num_seen_words = min(len(expt_df.base_from.unique()), len(expt_df.base_to.unique()))\n",
    "        # DEV\n",
    "        # if num_seen_words < 10:\n",
    "        #     print(f\"Skipping {expt} due to only {num_seen_words} seen words\")\n",
    "        #     continue\n",
    "\n",
    "        expt_df[\"inflection_from\"] = inflection_from\n",
    "        expt_df[\"allomorph_from\"] = allomorph_from\n",
    "        expt_df[\"inflection_to\"] = inflection_to\n",
    "        expt_df[\"allomorph_to\"] = allomorph_to\n",
    "\n",
    "        all_nnvb_results.append(expt_df)\n",
    "\n",
    "all_nnvb_results = pd.concat(all_nnvb_results)\n",
    "\n",
    "all_nnvb_results[\"inflected_from\"] = all_nnvb_results.from_equiv_label.apply(lambda x: eval(x)[1])\n",
    "all_nnvb_results[\"inflected_to\"] = all_nnvb_results.to_equiv_label.apply(lambda x: eval(x)[1])\n",
    "\n",
    "all_nnvb_results = pd.merge(all_nnvb_results, word_freq_df.LogFreq.rename(\"from_base_freq\"),\n",
    "                            left_on=\"base_from\", right_index=True)\n",
    "all_nnvb_results = pd.merge(all_nnvb_results, word_freq_df.LogFreq.rename(\"from_inflected_freq\"),\n",
    "                            left_on=\"inflected_from\", right_index=True)\n",
    "all_nnvb_results = pd.merge(all_nnvb_results, word_freq_df.LogFreq.rename(\"to_base_freq\"),\n",
    "                              left_on=\"base_to\", right_index=True)\n",
    "all_nnvb_results = pd.merge(all_nnvb_results, word_freq_df.LogFreq.rename(\"to_inflected_freq\"),\n",
    "                            left_on=\"inflected_to\", right_index=True)\n",
    "\n",
    "all_nnvb_results[\"from_freq\"] = all_nnvb_results[[\"from_base_freq\", \"from_inflected_freq\"]].mean(axis=1)\n",
    "all_nnvb_results[\"to_freq\"] = all_nnvb_results[[\"to_base_freq\", \"to_inflected_freq\"]].mean(axis=1)\n",
    "\n",
    "_, frequency_bins = pd.qcut(pd.concat([all_nnvb_results.to_freq, all_nnvb_results.from_freq]), q=5, retbins=True)\n",
    "all_nnvb_results[\"to_freq_bin\"] = pd.cut(all_nnvb_results.to_freq, bins=frequency_bins, labels=[f\"Q{i}\" for i in range(1, 6)])\n",
    "all_nnvb_results[\"from_freq_bin\"] = pd.cut(all_nnvb_results.from_freq, bins=frequency_bins, labels=[f\"Q{i}\" for i in range(1, 6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_nnvb_run(rows):\n",
    "    rows[\"source_label\"] = rows.inflection_from + \" \" + rows.allomorph_from\n",
    "    rows[\"target_label\"] = rows.inflection_to + \" \" + rows.allomorph_to\n",
    "\n",
    "    rows[\"transfer_label\"] = rows.inflection_from + \" -> \" + rows.inflection_to\n",
    "    rows[\"phon_label\"] = rows.allomorph_from + \" \" + rows.allomorph_to\n",
    "\n",
    "    # only retain cases where we have data in both transfer directions from source <-> target within inflection\n",
    "    rows[\"complement_exists\"] = rows.apply(lambda row: len(rows.query(\"source_label == @row.target_label and target_label == @row.source_label\")), axis=1)\n",
    "    rows = rows.query(\"complement_exists > 0\").drop(columns=[\"complement_exists\"])\n",
    "\n",
    "    return rows\n",
    "\n",
    "summary_groupers = [\"inflection_from\", \"inflection_to\", \"allomorph_from\", \"allomorph_to\"]\n",
    "nnvb_results_summary = all_nnvb_results.groupby(run_groupers + summary_groupers) \\\n",
    "    .correct.agg([\"count\", \"mean\"]) \\\n",
    "    .query(\"count >= 0\") \\\n",
    "    .reset_index(summary_groupers) \\\n",
    "    .groupby(run_groupers, group_keys=False) \\\n",
    "    .apply(summarize_nnvb_run) \\\n",
    "    .reset_index()\n",
    "\n",
    "nnvb_results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results = []\n",
    "for base_model_name, model_name, equivalence in plot_runs:\n",
    "    results_i = nnvb_results_summary.query(\"base_model_name == @base_model_name and model_name == @model_name and equivalence == @equivalence\")\n",
    "    if len(results_i) > 0:\n",
    "        plot_results.append(results_i)\n",
    "num_plot_runs = len(plot_results)\n",
    "\n",
    "ncols = 2\n",
    "nrows = int(np.ceil(num_plot_runs / ncols))\n",
    "f, axs = plt.subplots(nrows, ncols, figsize=(ncols * 4, nrows * 4))\n",
    "\n",
    "for ax, results_i in zip(axs.flat, plot_results):\n",
    "    sns.heatmap(results_i.set_index([\"source_label\", \"target_label\"])[\"mean\"].unstack(),\n",
    "                vmin=0, vmax=1, ax=ax)\n",
    "    key_row = results_i.iloc[0]\n",
    "    ax.set_title(f\"{key_row.base_model_name} -> {key_row.model_name} ({key_row.equivalence})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focused plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_base_model, focus_model, focus_equivalence = main_plot_run\n",
    "foil_base_model, foil_model, foil_equivalence = \"w2v2_8\", \"id\", \"id\"\n",
    "\n",
    "nnvb_focus = all_nnvb_results.query(\"base_model_name == @focus_base_model and model_name == @focus_model and equivalence == @focus_equivalence\")\n",
    "nnvb_foil = all_nnvb_results.query(\"base_model_name == @foil_base_model and model_name == @foil_model and equivalence == @foil_equivalence\")\n",
    "nnvb_focus[\"model_label\"] = \"Word\"\n",
    "nnvb_foil[\"model_label\"] = \"Wav2Vec\"\n",
    "\n",
    "nnvb_focus = pd.concat([nnvb_focus, nnvb_foil])\n",
    "\n",
    "allomorph_labels = {\"Z\": \"z\", \"S\": \"s\", \"IH Z\": \"ɪz\"}\n",
    "nnvb_focus[\"allomorph_from\"] = nnvb_focus.allomorph_from.map(allomorph_labels)\n",
    "nnvb_focus[\"allomorph_to\"] = nnvb_focus.allomorph_to.map(allomorph_labels)\n",
    "nnvb_focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnvb_results_summary = nnvb_focus.groupby([\"model_label\", \"inflection_from\", \"inflection_to\",\n",
    "                                             \"allomorph_from\", \"allomorph_to\"]) \\\n",
    "    .correct.agg([\"count\", \"mean\"]) \\\n",
    "    .query(\"count >= 0\") \\\n",
    "    .reset_index()\n",
    "\n",
    "nnvb_results_summary[\"source_label\"] = nnvb_results_summary.inflection_from + \"\\n\" + nnvb_results_summary.allomorph_from\n",
    "nnvb_results_summary[\"target_label\"] = nnvb_results_summary.inflection_to + \"\\n\" + nnvb_results_summary.allomorph_to\n",
    "\n",
    "nnvb_results_summary[\"transfer_label\"] = nnvb_results_summary.inflection_from + \" -> \" + nnvb_results_summary.inflection_to\n",
    "nnvb_results_summary[\"phon_label\"] = nnvb_results_summary.allomorph_from + \" \" + nnvb_results_summary.allomorph_to\n",
    "\n",
    "# only retain cases where we have data in both transfer directions from source <-> target within inflection\n",
    "nnvb_results_summary[\"complement_exists\"] = nnvb_results_summary.apply(lambda row: len(nnvb_results_summary.query(\"source_label == @row.target_label and target_label == @row.source_label\")), axis=1)\n",
    "nnvb_results_summary = nnvb_results_summary.query(\"complement_exists > 0\").drop(columns=[\"complement_exists\"])\n",
    "\n",
    "# drop VBZ IH Z, which only has 4 word types\n",
    "nnvb_results_summary = nnvb_results_summary[(nnvb_results_summary.source_label != \"VBZ\\nɪz\") & (nnvb_results_summary.target_label != \"VBZ\\nɪz\")]\n",
    "\n",
    "nnvb_results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnvb_focus_bar = nnvb_focus.assign(source_label=lambda xs: xs.inflection_from + \" \" + xs.allomorph_from)\n",
    "nnvb_focus_bar = nnvb_focus_bar[(nnvb_focus_bar.source_label != \"VBZ ɪz\")]\n",
    "order = nnvb_focus_bar.groupby(\"source_label\").correct.mean().sort_values().index\n",
    "g = sns.catplot(data=nnvb_focus_bar, x=\"inflection_to\", hue=\"source_label\", y=\"correct\", col=\"model_label\", kind=\"bar\")\n",
    "g._legend.set_title(\"Train inflection\\nand allomorph\")\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_title(ax.get_title().split(\"=\")[1].strip())\n",
    "    ax.set_xlabel(\"Test inflection\")\n",
    "    ax.set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, ax = plt.subplots(1, 2, figsize=(7 * 2, 6))\n",
    "\n",
    "f, axs = plt.subplots(1, 3, figsize=(7 * 2, 6), gridspec_kw={'width_ratios': [1, 1, 0.04]})\n",
    "for i, (ax, (model_label, rows)) in enumerate(zip(axs, nnvb_results_summary.groupby(\"model_label\"))):\n",
    "    cbar_ax = None\n",
    "    if i == 1:\n",
    "        cbar_ax = axs.flat[-1]\n",
    "\n",
    "    ax.set_title(model_label)\n",
    "    sns.heatmap(rows.set_index([\"source_label\", \"target_label\"]).sort_index()[\"mean\"].unstack(\"target_label\"),\n",
    "                vmin=main_plot_vmin, vmax=main_plot_vmax, annot=True, ax=ax,\n",
    "                cbar=i == 1, cbar_ax=cbar_ax)\n",
    "\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "    ax.set_ylabel(\"Train\")\n",
    "    ax.set_xlabel(\"Test\")\n",
    "\n",
    "f.tight_layout()\n",
    "f.savefig(f\"{output_dir}/nnvb_allomorphs.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnvb_results_summary2 = nnvb_focus.groupby([\"model_label\", \"inflection_from\", \"inflection_to\"]) \\\n",
    "    [[\"correct\", \"gt_label_rank\", \"gt_distance\"]].mean().reset_index()\n",
    "\n",
    "nnvb_results_summary2[\"transfer_label\"] = nnvb_results_summary2.inflection_from + \" -> \" + nnvb_results_summary2.inflection_to\n",
    "nnvb_results_summary2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1, 3, figsize=(4 * 2, 3), gridspec_kw={'width_ratios': [1, 1, 0.04]})\n",
    "\n",
    "for i, (ax, (model_label, rows)) in enumerate(zip(axs, nnvb_results_summary2.groupby(\"model_label\"))):\n",
    "    cbar_ax = None\n",
    "    if i == 1:\n",
    "        cbar_ax = axs.flat[-1]\n",
    "    \n",
    "    ax.set_title(model_label)\n",
    "    sns.heatmap(rows.set_index([\"inflection_from\", \"inflection_to\"]).correct.unstack(),\n",
    "                annot=True, vmin=main_plot_vmin, vmax=main_plot_vmax, ax=ax,\n",
    "                cbar=i == 1, cbar_ax=cbar_ax)\n",
    "    ax.set_xlabel(\"Test\")\n",
    "    ax.set_ylabel(\"Train\")\n",
    "\n",
    "f = plt.gcf()\n",
    "f.tight_layout()\n",
    "f.savefig(f\"{output_dir}/nnvb_results.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnvb_plot = nnvb_focus.groupby([\"model_label\", \"inflection_from\", \"inflection_to\", \"base_to\"]).correct.mean().reset_index()\n",
    "nnvb_plot[\"transfer_label\"] = nnvb_plot.inflection_from + \" -> \" + nnvb_plot.inflection_to\n",
    "order = nnvb_plot.groupby(\"transfer_label\").correct.mean().sort_values().index\n",
    "g = sns.catplot(data=nnvb_plot, x=\"transfer_label\", y=\"correct\", kind=\"bar\", hue=\"model_label\", order=order, errorbar=\"se\", height=4, aspect=2)\n",
    "g._legend.set_title(\"Model\")\n",
    "ax = g.axes.flat[0]\n",
    "\n",
    "ax.set_xlabel(\"Evaluation\")\n",
    "ax.set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnvb_plot = nnvb_focus.groupby([\"model_label\", \"inflection_from\", \"inflection_to\", \"base_to\"]).correct.mean().reset_index()\n",
    "nnvb_plot[\"transfer_label\"] = nnvb_plot.inflection_from + \" -> \" + nnvb_plot.inflection_to\n",
    "order = nnvb_plot.groupby(\"transfer_label\").correct.mean().sort_values().index\n",
    "g = sns.catplot(data=nnvb_plot.query(\"model_label == 'Word'\"), x=\"transfer_label\", y=\"correct\", kind=\"bar\", hue=\"model_label\", order=order, errorbar=\"se\", height=4, aspect=2)\n",
    "g._legend.set_title(\"Model\")\n",
    "ax = g.axes.flat[0]\n",
    "\n",
    "ax.set_xlabel(\"Evaluation\")\n",
    "ax.set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnvb_phase1 = nnvb_plot[nnvb_plot.inflection_from == nnvb_plot.inflection_to]\n",
    "order = nnvb_phase1.groupby(\"transfer_label\").correct.mean().sort_values().index\n",
    "g = sns.catplot(data=nnvb_phase1, x=\"transfer_label\", y=\"correct\", kind=\"bar\", hue=\"inflection_to\", order=order, errorbar=\"se\", height=4, aspect=1)\n",
    "# remove legend\n",
    "g._legend.remove()\n",
    "ax = g.axes.flat[0]\n",
    "\n",
    "ax.set_xlabel(\"Evaluation\")\n",
    "ax.set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = sorted(nnvb_results_summary.transfer_label.unique(), key=lambda x: x[4:])\n",
    "g = sns.catplot(data=nnvb_results_summary, x=\"transfer_label\", y=\"mean\", hue=\"inflection_to\",\n",
    "            order=order, kind=\"swarm\", errorbar=\"se\", height=5, aspect=1.5, size=11)\n",
    "ax = g.axes.flat[0]\n",
    "ax.set_xlabel(\"Evaluation\")\n",
    "ax.set_ylabel(\"Mean accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_interaction_strength(rows):\n",
    "#     rows[\"correct\"] = rows.correct.astype(int)\n",
    "    \n",
    "#     # exclude rare\n",
    "#     rows = rows[~((rows.inflection_from == \"VBZ\") & rows.inflection_from == \"IH Z\") &\n",
    "#                 ~((rows.inflection_to == \"VBZ\") & rows.inflection_to == \"IH Z\")]\n",
    "    \n",
    "#     # standardize frequency\n",
    "#     rows[\"from_freq\"] = (rows.from_freq - rows.from_freq.mean()) / rows.from_freq.std()\n",
    "#     rows[\"to_freq\"] = (rows.to_freq - rows.to_freq.mean()) / rows.to_freq.std()\n",
    "    \n",
    "#     formula = \"correct ~ C(inflection_from, Treatment(reference='NNS')) * C(inflection_to, Treatment(reference='NNS')) + \" \\\n",
    "#               \"C(allomorph_from, Treatment(reference='Z')) * C(allomorph_to, Treatment(reference='Z')) +\" \\\n",
    "#               \"from_freq + to_freq\"\n",
    "    \n",
    "#     model = logit(formula, data=rows).fit()\n",
    "\n",
    "#     return model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interaction_strengths = all_nnvb_results.groupby(run_groupers).apply(get_interaction_strength) \\\n",
    "#     .reset_index().melt(id_vars=run_groupers, value_name=\"coef_norm\")\n",
    "# interaction_strengths = interaction_strengths[interaction_strengths.variable.str.contains(\":\", regex=True)]\n",
    "# interaction_strengths = interaction_strengths.groupby(run_groupers).coef_norm.apply(lambda xs: np.linalg.norm(xs, ord=1)).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_accuracy = all_nnvb_results.groupby(run_groupers).correct.mean().reindex(plot_runs).reset_index()\n",
    "# plot_accuracy[\"layer\"] = plot_accuracy.base_model_name.str.extract(r\"_(\\d+)$\").astype(int)\n",
    "\n",
    "# g = sns.catplot(data=plot_accuracy, x=\"layer\", y=\"correct\", hue=\"model_name\", height=3, aspect=2, kind=\"point\")\n",
    "# # g.figure.tight_layout()\n",
    "# g.figure.savefig(f\"{output_dir}/nnvb_layer_wise.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_is = interaction_strengths.reindex(plot_runs).reset_index()\n",
    "# plot_is[\"layer\"] = plot_is.base_model_name.str.extract(r\"_(\\d+)$\").astype(int)\n",
    "\n",
    "# g = sns.catplot(data=plot_is, x=\"layer\", y=\"coef_norm\", hue=\"model_name\", kind=\"point\", height=3, aspect=2)\n",
    "# g.axes.flat[0].set_ylabel(\"Allomorph/inflection\\ninteraction strength\")\n",
    "# g.axes.flat[0].set_xlabel(\"Layer\")\n",
    "\n",
    "# g.savefig(f\"{output_dir}/interaction_strength.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_df = nnvb_focus[(nnvb_focus.inflection_to == \"VBZ\")].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_df[\"predicted_stem\"] = study_df.predicted_label.str.replace(r\"s$|ed$|ings?$\", \"\", regex=True)\n",
    "study_df[\"base_to_stem\"] = study_df.base_to.str.replace(r\"e$\", \"\", regex=True).replace(r\"y$\", \"i\", regex=True)\n",
    "study_df[\"predicted_within_inflection\"] = \\\n",
    "    (study_df.predicted_stem == study_df.base_to) | (study_df.predicted_stem == study_df.base_to_stem)\n",
    "vb_irregulars = [(\"do\", \"did\"), (\"do\", \"does\"), (\"begin\", \"began\"), (\"learn\", \"learnt\"), (\"send\", \"sent\"), (\"shine\", \"shone\"), (\"seem\", \"seem'd\"), (\"read\", \"red\"),\n",
    "                 (\"possess\", \"possesses\"), (\"bring\", \"brings\"), (\"carry\", \"carries\"), (\"occur\", \"occurred\"), (\"think\", \"thinkest\"), (\"grow\", \"grew\"),\n",
    "                 (\"put\", \"putting\"), (\"begin\", \"beginning\"), (\"give\", \"givest\"),\n",
    "                 # homophones\n",
    "                 (\"allow\", \"aloud\"), (\"write\", \"rights\"), (\"write\", \"wright's\"), (\"depend\", \"dependent\")]\n",
    "for base, predicted in vb_irregulars:\n",
    "    study_df.loc[study_df.base_to == base, \"predicted_within_inflection\"] |= study_df.loc[study_df.base_to == base].predicted_label == predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study_df[~study_df.predicted_within_inflection].groupby(\"base_to\").predicted_label.value_counts().sort_values(ascending=False).iloc[60:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnvb_focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_keys = [\"experiment\", \"equivalence\", \"model_label\", \"model_name\", \"base_model_name\", \"group\", \"inflection_from\", \"inflection_to\", \"base_from\", \"inflected_from\", \"base_to\", \"inflected_to\"]\n",
    "nnvb_focus = pd.merge(nnvb_focus.reset_index(), study_df.reset_index()[merge_keys + [\"predicted_within_inflection\"]],\n",
    "         on=merge_keys, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnvb_focus[\"correct_or_predicted_within_inflection\"] = nnvb_focus.correct | nnvb_focus.predicted_within_inflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnvb_results_summary2 = nnvb_focus.groupby([\"model_label\", \"inflection_from\", \"inflection_to\"]) \\\n",
    "    [[\"correct_or_predicted_within_inflection\", \"correct\", \"gt_label_rank\", \"gt_distance\"]].mean().reset_index()\n",
    "\n",
    "nnvb_results_summary2[\"transfer_label\"] = nnvb_results_summary2.inflection_from + \" -> \" + nnvb_results_summary2.inflection_to\n",
    "nnvb_results_summary2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1, 3, figsize=(4 * 2, 3), gridspec_kw={'width_ratios': [1, 1, 0.04]})\n",
    "\n",
    "for i, (ax, (model_label, rows)) in enumerate(zip(axs, nnvb_results_summary2.groupby(\"model_label\"))):\n",
    "    cbar_ax = None\n",
    "    if i == 1:\n",
    "        cbar_ax = axs.flat[-1]\n",
    "    \n",
    "    ax.set_title(model_label)\n",
    "    sns.heatmap(rows.set_index([\"inflection_from\", \"inflection_to\"]).correct_or_predicted_within_inflection.unstack(),\n",
    "                vmin=main_plot_vmin, vmax=main_plot_vmax,\n",
    "                annot=True, ax=ax, cbar=i == 1, cbar_ax=cbar_ax)\n",
    "    ax.set_xlabel(\"Test\")\n",
    "    ax.set_ylabel(\"Train\")\n",
    "\n",
    "f = plt.gcf()\n",
    "f.tight_layout()\n",
    "f.savefig(f\"{output_dir}/nnvb_results-correct_inflection.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnvb_plot = nnvb_focus.groupby([\"model_label\", \"inflection_from\", \"inflection_to\", \"base_to\"]).correct_or_predicted_within_inflection.mean().reset_index()\n",
    "nnvb_plot[\"transfer_label\"] = nnvb_plot.inflection_from + \" -> \" + nnvb_plot.inflection_to\n",
    "order = [\"NNS -> VBZ\", \"VBZ -> VBZ\", \"VBZ -> NNS\", \"NNS -> NNS\"]\n",
    "g = sns.catplot(data=nnvb_plot, x=\"transfer_label\", y=\"correct_or_predicted_within_inflection\", kind=\"bar\", hue=\"model_label\", order=order, errorbar=\"se\", height=4, aspect=2)\n",
    "g._legend.set_title(\"Model\")\n",
    "ax = g.axes.flat[0]\n",
    "\n",
    "ax.set_xlabel(\"Evaluation\")\n",
    "ax.set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=all_nnvb_results.query(\"base_model_name == 'w2v2_8'\").reset_index(),\n",
    "            x=\"from_freq_bin\", y=\"correct\", hue=\"model_name\",\n",
    "            row=\"inflection_from\", col=\"inflection_to\", units=\"base_from\", kind=\"point\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=all_nnvb_results.query(\"base_model_name == 'w2v2_8'\").reset_index(),\n",
    "            x=\"to_freq_bin\", y=\"correct\", hue=\"model_name\",\n",
    "            row=\"inflection_from\", col=\"inflection_to\", units=\"base_to\", kind=\"point\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False friend analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ff_results = []\n",
    "\n",
    "for run, run_results in all_results.groupby(run_groupers):\n",
    "    run_results = run_results.set_index(\"experiment\")\n",
    "    false_friend_expts = run_results.index.unique()\n",
    "    false_friend_expts = false_friend_expts[false_friend_expts.str.contains(\"FF\")]\n",
    "\n",
    "    for expt_name in false_friend_expts:\n",
    "        expt_df = run_results.loc[expt_name].copy()\n",
    "        num_seen_words = min(len(expt_df.base_from.unique()), len(expt_df.base_to.unique()))\n",
    "\n",
    "        if num_seen_words < 10:\n",
    "            print(f\"Skipping {expt} due to only {num_seen_words} seen words\")\n",
    "            continue\n",
    "\n",
    "        if expt_name.count(\"-FF-\") == 2:\n",
    "            allomorph_from, allomorph_to = re.findall(r\"-FF-([\\w\\s]+)-to-.+FF-([\\w\\s]+)\", expt_name)[0]\n",
    "            ff_from, ff_to = True, True\n",
    "        else:\n",
    "            try:\n",
    "                allomorph_from, allomorph_to = re.findall(r\"_([\\w\\s]+)-to-.+FF-([\\w\\s]+)\", expt_name)[0]\n",
    "                # is the false friend on the \"from\" side?\n",
    "                ff_from, ff_to = False, True\n",
    "            except:\n",
    "                allomorph_from, allomorph_to = re.findall(r\".+FF-([\\w\\s]+)-to-.+_([\\w\\s]+)\", expt_name)[0]\n",
    "                ff_from, ff_to = True, False\n",
    "\n",
    "        expt_df[\"allomorph_from\"] = allomorph_from\n",
    "        expt_df[\"allomorph_to\"] = allomorph_to\n",
    "\n",
    "        if ff_from:\n",
    "            expt_df[\"inflection_from\"] = expt_df.inflection_from.str.replace(\"-FF-.+\", \"-FF\", regex=True)\n",
    "        if ff_to:\n",
    "            expt_df[\"inflection_to\"] = expt_df.inflection_to.str.replace(\"-FF-.+\", \"-FF\", regex=True)\n",
    "\n",
    "        expt_df[\"ff_from\"] = ff_from\n",
    "        expt_df[\"ff_to\"] = ff_to\n",
    "\n",
    "        all_ff_results.append(expt_df)\n",
    "\n",
    "    # add within-false-friend tests\n",
    "    expt_df = run_results.loc[\"false_friends\"].copy()\n",
    "    expt_df[\"allomorph_from\"] = expt_df.inflection_from.str.extract(r\"FF-(.+)$\")\n",
    "    expt_df[\"allomorph_to\"] = expt_df.inflection_to.str.extract(r\"FF-(.+)$\")\n",
    "    expt_df[\"inflection_from\"] = expt_df.inflection_from.str.replace(\"-FF-.+\", \"-FF\", regex=True)\n",
    "    expt_df[\"inflection_to\"] = expt_df.inflection_to.str.replace(\"-FF-.+\", \"-FF\", regex=True)\n",
    "    expt_df[\"ff_from\"] = True\n",
    "    expt_df[\"ff_to\"] = True\n",
    "\n",
    "    all_ff_results.append(expt_df)\n",
    "\n",
    "    # expt_df = expt_df[expt_df.inflection_from.isin(all_ff_results.inflection_from.unique())]\n",
    "\n",
    "all_ff_results = pd.concat(all_ff_results).reset_index()\n",
    "\n",
    "ff_exclude = \"wreck e eh wandering lo chiu ha hahn meek jew\"\n",
    "ff_exclude_inflected = \"bunce los\"\n",
    "\n",
    "# exclude FF bases\n",
    "all_ff_results = all_ff_results[~(all_ff_results.inflection_from.str.endswith(\"-FF\") & all_ff_results.base_from.isin(ff_exclude.split()))]\n",
    "all_ff_results = all_ff_results[~(all_ff_results.inflection_to.str.endswith(\"-FF\") & all_ff_results.base_to.isin(ff_exclude.split()))]\n",
    "\n",
    "all_ff_results[\"inflected_from\"] = all_ff_results.from_equiv_label.apply(lambda x: eval(x)[1])\n",
    "all_ff_results[\"inflected_to\"] = all_ff_results.to_equiv_label.apply(lambda x: eval(x)[1])\n",
    "\n",
    "# exclude FF inflected\n",
    "all_ff_results = all_ff_results[~(all_ff_results.inflection_from.str.endswith(\"-FF\") & all_ff_results.inflected_from.isin(ff_exclude_inflected.split()))]\n",
    "all_ff_results = all_ff_results[~(all_ff_results.inflection_to.str.endswith(\"-FF\") & all_ff_results.inflected_to.isin(ff_exclude_inflected.split()))]\n",
    "\n",
    "# add frequency information\n",
    "all_ff_results = pd.merge(all_ff_results, word_freq_df[\"LogFreq\"].rename(\"from_base_freq\"), left_on=\"base_from\", right_index=True)\n",
    "all_ff_results = pd.merge(all_ff_results, word_freq_df[\"LogFreq\"].rename(\"to_base_freq\"), left_on=\"base_to\", right_index=True)\n",
    "all_ff_results = pd.merge(all_ff_results, word_freq_df[\"LogFreq\"].rename(\"from_inflected_freq\"), left_on=\"inflected_from\", right_index=True)\n",
    "all_ff_results = pd.merge(all_ff_results, word_freq_df[\"LogFreq\"].rename(\"to_inflected_freq\"), left_on=\"inflected_to\", right_index=True)\n",
    "all_ff_results[\"from_freq\"] = all_ff_results[[\"from_base_freq\", \"from_inflected_freq\"]].mean(axis=1)\n",
    "all_ff_results[\"to_freq\"] = all_ff_results[[\"to_base_freq\", \"to_inflected_freq\"]].mean(axis=1)\n",
    "\n",
    "all_ff_results[\"transfer_label\"] = all_ff_results.inflection_from + \" -> \" + all_ff_results.inflection_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-hoc fix some bugs\n",
    "all_ff_results.loc[(all_ff_results.base_to == \"tho\") & (all_ff_results.predicted_label == \"though\") & (all_ff_results.gt_label_rank == 1), \"correct\"] = True\n",
    "all_ff_results.loc[(all_ff_results.base_to == \"philip\") & (all_ff_results.predicted_label == \"philip's\"), \"correct\"] = True\n",
    "all_ff_results.loc[(all_ff_results.base_to == \"adam\") & (all_ff_results.predicted_label == \"adam's\"), \"correct\"] = True\n",
    "all_ff_results.loc[(all_ff_results.base_to == \"who\") & (all_ff_results.predicted_label == \"who's\"), \"correct\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_friend_strong_lookup = false_friends_df.set_index([\"base\", \"inflected\", \"post_divergence\"]).strong.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_is_strong(rows):\n",
    "    keys = []\n",
    "    row = rows.iloc[0]\n",
    "    if \"-FF\" in row.inflection_from:\n",
    "        keys.append((row.base_from, row.inflected_from, row.allomorph_from))\n",
    "    if \"-FF\" in row.inflection_to:\n",
    "        keys.append((row.base_to, row.inflected_to, row.allomorph_to))\n",
    "\n",
    "    # print(keys)\n",
    "    strong_results = [false_friend_strong_lookup[base, inflected, allomorph] for base, inflected, allomorph in keys]\n",
    "    return all(strong_results)\n",
    "\n",
    "strong_grouper = [\"inflection_from\", \"inflection_to\", \"inflected_from\", \"inflected_to\", \"base_from\", \"base_to\", \"allomorph_from\", \"allomorph_to\"]\n",
    "strong_values = all_ff_results.groupby(strong_grouper).apply(get_is_strong).rename(\"is_strong\")\n",
    "all_ff_results = pd.merge(all_ff_results, strong_values, left_on=strong_grouper, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_ff_results = all_ff_results[~all_ff_results.is_strong]\n",
    "\n",
    "# ONLY STRONG\n",
    "all_ff_results = all_ff_results[all_ff_results.is_strong]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak sub-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StateSpaceAnalysisSpec.from_hdf5(state_space_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these base forms participate in a real inflection,\n",
    "# so they have a distractor\n",
    "weak_alternates = {\n",
    "    \"barbara\": \"barbara's\",\n",
    "    \"bay\": \"bays\",\n",
    "    \"den\": \"dens\",\n",
    "    \"dew\": \"dews\",\n",
    "    \"fall\": \"falls\",\n",
    "    \"fear\": \"fears\",\n",
    "    \"flee\": \"flees\",\n",
    "    \"hen\": \"hens\",\n",
    "    \"her\": \"hers\",\n",
    "    \"jew\": \"jews\",\n",
    "    \"joy\": \"joys\",\n",
    "    \"lay\": \"lays\",\n",
    "    \"one\": \"ones one's\",\n",
    "    \"patricia\": \"patricia's\",\n",
    "    \"peer\": \"peers\",\n",
    "    \"per\": \"purrs\",\n",
    "    \"river\": \"rivers river's\",\n",
    "    \"saw\": \"saws\",\n",
    "    \"scare\": \"scares\",\n",
    "    \"sin\": \"sins\",\n",
    "    \"syria\": \"syria's\",\n",
    "    \"victoria\": \"victoria's\"\n",
    "}\n",
    "\n",
    "# Only retain cases where the alternate is in the vocabulary\n",
    "drop_cases = [k for k, v in weak_alternates.items() if not any(word in ss.labels for word in v.split(\" \"))]\n",
    "print(f\"Dropping {len(drop_cases)} cases: {drop_cases}\")\n",
    "weak_alternates = {k: v for k, v in weak_alternates.items() if any(word in ss.labels for word in v.split(\" \"))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_ff_results[~weak_ff_results.base_to.map(weak_alternates).isnull() & weak_ff_results.ff_to].base_to.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_ff_results[weak_ff_results.base_to.map(weak_alternates).isnull() & weak_ff_results.ff_to].base_to.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_sub_df = weak_ff_results[weak_ff_results.ff_to].copy()\n",
    "weak_sub_df[\"competitor_to\"] = weak_sub_df.base_to.map(weak_alternates)\n",
    "weak_sub_df = weak_sub_df[weak_sub_df.competitor_to.notnull()]\n",
    "weak_sub_df = weak_sub_df.query(\"base_model_name == @focus_base_model and model_name == @focus_model and equivalence == @focus_equivalence\")\n",
    "weak_sub_df[\"alternate_match\"] = weak_sub_df.apply(lambda xs: re.match(f\"\\\\b{xs.predicted_label}\\\\b\", xs.competitor_to) is not None, axis=1)\n",
    "weak_sub_df[\"source_label\"] = weak_sub_df.inflection_from + \" \" + weak_sub_df.allomorph_from\n",
    "# just study /S/ false friends right now\n",
    "weak_sub_df = weak_sub_df.query(\"inflection_to in ['VBZ-FF', 'NNS-FF'] and allomorph_to == 'S' and allomorph_from in ['Z', 'S']\")\n",
    "weak_sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_props(xs):\n",
    "    return pd.Series({\n",
    "        \"correct\": xs.correct.sum(),\n",
    "        \"alternate_match\": xs.alternate_match.sum(),\n",
    "        \"prop_correct\": xs.correct.sum() / len(xs),#(xs.correct.sum() + xs.alternate_match.sum()),\n",
    "        \"prop_alternate_match\": xs.alternate_match.sum() / len(xs),#/ (xs.correct.sum() + xs.alternate_match.sum())\n",
    "    })\n",
    "weak_sub_plot = weak_sub_df.groupby([\"source_label\", \"inflection_to\", \"base_to\"]).apply(get_props).reset_index()\n",
    "weak_sub_plot[\"source_label\"] = weak_sub_plot.source_label.str.replace(r\"(NNS|VBZ)-?\\s*\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import bbox_artist\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "hue_order = weak_sub_plot.groupby(\"source_label\").prop_correct.mean().sort_values().index\n",
    "ax = sns.barplot(data=weak_sub_plot, x=\"inflection_to\", hue=\"source_label\", hue_order=hue_order, y=\"prop_alternate_match\", errorbar=\"se\")\n",
    "ax.set_ylabel(\"Proportion\\nchoices of /z/\", rotation=0, labelpad=70)\n",
    "ax.set_xlabel(\"Test inflection\")\n",
    "ax.legend(title=\"Source inflection\", loc=\"upper right\", bbox_to_anchor=(1.325, 1))\n",
    "# ax.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import bbox_artist\n",
    "\n",
    "f, ax2 = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "hue_order = weak_sub_plot.groupby(\"source_label\").prop_correct.mean().sort_values().index\n",
    "ax2 = sns.barplot(data=weak_sub_plot, x=\"inflection_to\", hue=\"source_label\", hue_order=hue_order, y=\"prop_correct\", errorbar=\"se\")\n",
    "ax2.set_ylabel(\"Proportion\\nchoices of /s/\", rotation=0, labelpad=70)\n",
    "ax2.set_xlabel(\"Test inflection\")\n",
    "ax2.set_ylim(ax.get_ylim())\n",
    "ax2.legend(title=\"Source inflection\", loc=\"upper right\", bbox_to_anchor=(1.325, 1))\n",
    "# ax.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_sub_plot = weak_sub_df.groupby([\"source_label\", \"inflection_to\", \"base_to\"])[[\"correct\", \"alternate_match\"]].mean().reset_index() \\\n",
    "    .melt(id_vars=[\"source_label\", \"inflection_to\", \"base_to\"])\n",
    "weak_sub_plot[\"source_label\"] = weak_sub_plot.source_label.str.replace(r\"(NNS|VBZ)-?\\s*\", \"\", regex=True)\n",
    "\n",
    "hue_order = weak_sub_plot.groupby(\"source_label\").value.mean().sort_values().index\n",
    "sns.catplot(data=weak_sub_plot, x=\"inflection_to\", hue=\"source_label\", y=\"value\", col=\"variable\",\n",
    "            hue_order=hue_order, kind=\"bar\", errorbar=\"se\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_sub_df.groupby([\"inflection_to\", \"source_label\"]).correct.mean().groupby(\"inflection_to\").apply(lambda xs: xs.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_sub_df.groupby([\"inflection_to\", \"source_label\"]).alternate_match.mean().groupby(\"inflection_to\").apply(lambda xs: xs.sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first consider the case of strong false friends, which have a concatenated /z/ /s/ or /Iz/ and could have been distributed by the same surface alternate pattern.\n",
    "For example, \"beside\" -- \"besides\" does not instantiate the relevant morphological pattern, but the concatenation of /z/ is consistent with the pattern on the surface.\n",
    "We see roughly similar results for strong false friends as for real morphological pairs.\n",
    "This indicates that the surface alternation pattern extends beyond real morphological patterns; the representation is thus capturing something not at the morphological level here, but at the surface-alternate level.\n",
    "\n",
    "We next consider the case of weak false friends, which have a concatenated /z/ /s/ or /Iz/ but do not respect the surface alternate pattern.\n",
    "For example, \"flee\" -- \"fleece\" has the right concatenative relationship but does not respect the pattern (here we would expect to see /z/ following the vowel).\n",
    "Generalization here is quite poor, indicating that the model is not appropriately applying the surface alternate pattern.\n",
    "Error analysis reveals that the model is respecting the surface alternate pattern just as we expect.\n",
    "\n",
    "We zoom in on weak false friends with concatenated /s/, for example \"flee\" \"fleece.\" Here there is a competitor \"flees\" which respects the surface alternation pattern.\n",
    "Call these items \"weak false friends with alternates\" (WFFA).\n",
    "We evaluate the model's performance on WFFA from two sources: valid instances of plurals with /z/, and valid instances of plurals with /s/.\n",
    "Computing analogies from the former category leads the model to predict the alternate form /z/, unsurprisingly.\n",
    "However, computing analogies from the latter category leads the model to predict the alternate form /z/ at roughly the same rate!\n",
    "This indicates that the latter category is not generating a context-free phonological representation, but rather a more abstract representation of \"give me the right kind of surface alternate.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_sub2_df = weak_ff_results[weak_ff_results.ff_to].copy()\n",
    "weak_sub2_df[\"competitor_to\"] = weak_sub2_df.base_to.map(weak_alternates)\n",
    "weak_sub2_df = weak_sub2_df[weak_sub2_df.competitor_to.isnull()]\n",
    "weak_sub2_df = weak_sub2_df.query(\"base_model_name == @focus_base_model and model_name == @focus_model and equivalence == @focus_equivalence\")\n",
    "weak_sub2_df[\"source_label\"] = weak_sub2_df.inflection_from + \" \" + weak_sub2_df.allomorph_from\n",
    "# just study /S/ false friends right now\n",
    "weak_sub2_df = weak_sub2_df.query(\"inflection_to in ['VBZ-FF', 'NNS-FF'] and allomorph_to == 'S' and allomorph_from in ['Z', 'S']\")\n",
    "weak_sub2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_sub2_df.groupby([\"inflection_to\", \"source_label\"]).correct.mean().groupby(\"inflection_to\").apply(lambda xs: xs.sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main FF analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_frequency_bins = pd.qcut(pd.concat([all_ff_results.to_freq, all_ff_results.from_freq]), q=3, retbins=True)[1]\n",
    "all_ff_results[\"from_freq_bin\"] = pd.cut(all_ff_results.from_freq, bins=ff_frequency_bins, labels=[f\"Q{i}\" for i in range(1, 4)])\n",
    "all_ff_results[\"to_freq_bin\"] = pd.cut(all_ff_results.to_freq, bins=ff_frequency_bins, labels=[f\"Q{i}\" for i in range(1, 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distribution of false friend word frequencies to distribution of NN/VB frequencies.\n",
    "# This is to see if the false friends are more likely to be rare words.\n",
    "false_friend_words = pd.concat([all_ff_results.query(\"ff_from\").base_from, all_ff_results.query(\"ff_to\").base_to]).unique()\n",
    "nn_words = pd.concat([all_nnvb_results.query(\"inflection_from == 'NNS'\").base_from,\n",
    "                        all_nnvb_results.query(\"inflection_to == 'NNS'\").base_to]).unique()\n",
    "vb_words = pd.concat([all_nnvb_results.query(\"inflection_from == 'VBZ'\").base_from,\n",
    "                        all_nnvb_results.query(\"inflection_to == 'VBZ'\").base_to]).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_word_freqs = pd.concat({\n",
    "    \"false_friends\": word_freq_df.loc[false_friend_words].LogFreq,\n",
    "    \"NN\": word_freq_df.loc[nn_words].LogFreq,\n",
    "    \"VB\": word_freq_df.loc[vb_words].LogFreq\n",
    "}, names=[\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=expt_word_freqs.reset_index(), x=\"LogFreq\", row=\"type\", kind=\"hist\", bins=15,\n",
    "            height=1, aspect=3, facet_kws={\"sharey\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_results_summary2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_ff_results = all_ff_results.query(\"base_model_name == @focus_base_model and model_name == @focus_model and equivalence == @focus_equivalence\")\n",
    "\n",
    "ff_results_summary2 = focus_ff_results.groupby([\"inflection_from\", \"inflection_to\"]) \\\n",
    "    [[\"correct\", \"gt_label_rank\", \"gt_distance\"]].mean().reset_index()\n",
    "\n",
    "ff_results_summary2[\"transfer_label\"] = ff_results_summary2.inflection_from + \" -> \" + ff_results_summary2.inflection_to\n",
    "\n",
    "# add in data for NNS->NNS and VBZ->VBZ\n",
    "ff_results_summary2 = pd.concat([ff_results_summary2, nnvb_results_summary2.query(\"inflection_from == inflection_to and model_label == 'Word'\")], axis=0)\n",
    "\n",
    "ff_results_summary2[\"base_inflection\"] = ff_results_summary2.inflection_from.str.replace(\"-FF\", \"\")\n",
    "\n",
    "ff_results_summary2 = ff_results_summary2[ff_results_summary2.base_inflection.isin(plot_inflections)]\n",
    "\n",
    "g = sns.FacetGrid(ff_results_summary2, col=\"base_inflection\", sharex=False, sharey=False)\n",
    "\n",
    "def mapfn(data, **kwargs):\n",
    "    ax = plt.gca()\n",
    "    sns.heatmap(data.set_index([\"inflection_from\", \"inflection_to\"]).correct.unstack(\"inflection_to\"),\n",
    "                vmin=main_plot_vmin, vmax=main_plot_vmax, annot=True, ax=ax)\n",
    "\n",
    "g.map_dataframe(mapfn)\n",
    "\n",
    "for i, ax in enumerate(g.axes.flat):\n",
    "    ax.set_title(ax.get_title().replace(\"base_inflection = \", \"\"))\n",
    "    if i > 0:\n",
    "        ax.set_ylabel(\"\")\n",
    "    if i < len(g.axes.flat) - 1:\n",
    "        ax.collections[0].colorbar.remove()\n",
    "\n",
    "g.fig.tight_layout()\n",
    "g.fig.savefig(f\"{output_dir}/ff_results.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_weak_ff_results = weak_ff_results.query(\"base_model_name == @focus_base_model and model_name == @focus_model and equivalence == @focus_equivalence\")\n",
    "\n",
    "weak_ff_results_summary2 = focus_weak_ff_results.groupby([\"inflection_from\", \"inflection_to\"]) \\\n",
    "    [[\"correct\", \"gt_label_rank\", \"gt_distance\"]].mean().reset_index()\n",
    "\n",
    "weak_ff_results_summary2[\"transfer_label\"] = weak_ff_results_summary2.inflection_from + \" -> \" + weak_ff_results_summary2.inflection_to\n",
    "\n",
    "# add in data for NNS->NNS and VBZ->VBZ\n",
    "weak_ff_results_summary2 = pd.concat([weak_ff_results_summary2, nnvb_results_summary2.query(\"inflection_from == inflection_to and model_label == 'Word'\")], axis=0)\n",
    "\n",
    "weak_ff_results_summary2[\"base_inflection\"] = weak_ff_results_summary2.inflection_from.str.replace(\"-FF\", \"\")\n",
    "\n",
    "weak_ff_results_summary2 = weak_ff_results_summary2[weak_ff_results_summary2.base_inflection.isin(plot_inflections)]\n",
    "\n",
    "g = sns.FacetGrid(weak_ff_results_summary2, col=\"base_inflection\", sharex=False, sharey=False)\n",
    "\n",
    "def mapfn(data, **kwargs):\n",
    "    ax = plt.gca()\n",
    "    sns.heatmap(data.set_index([\"inflection_from\", \"inflection_to\"]).correct.unstack(\"inflection_to\"),\n",
    "                vmin=main_plot_vmin, vmax=main_plot_vmax, annot=True, ax=ax)\n",
    "\n",
    "g.map_dataframe(mapfn)\n",
    "\n",
    "for i, ax in enumerate(g.axes.flat):\n",
    "    ax.set_title(ax.get_title().replace(\"base_inflection = \", \"\"))\n",
    "    if i > 0:\n",
    "        ax.set_ylabel(\"\")\n",
    "    if i < len(g.axes.flat) - 1:\n",
    "        ax.collections[0].colorbar.remove()\n",
    "\n",
    "g.fig.tight_layout()\n",
    "g.fig.savefig(f\"{output_dir}/ff_results_weak.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_ff_results[weak_ff_results.inflection_to == \"NNS-FF\"].groupby(\"base_to\").predicted_label.value_counts().to_csv(\"weak_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_ff_results.query(\"base_to == 'den'\")[[\"base_from\", \"allomorph_from\", \"predicted_label\", \"gt_label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=focus_ff_results.groupby([\"transfer_label\", \"base_to\"]).correct.mean().reset_index(),\n",
    "            x=\"transfer_label\", y=\"correct\", aspect=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_ff_results.allomorph_to.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_ff_results.query(\"base_to == 'to'\")[[\"gt_label\", \"allomorph_to\"]].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_ff_results.query(\"base_to == 'why'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_ff_results.query(\"transfer_label == 'NNS -> NNS-FF'\").groupby(\"base_to\").correct.mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=all_ff_results.query(\"base_model_name == 'w2v2_8'\").reset_index(),\n",
    "            x=\"from_freq_bin\", y=\"correct\", hue=\"model_name\",\n",
    "            col=\"transfer_label\", col_wrap=2, kind=\"point\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ff_results.query(\"base_model_name == 'w2v2_8' and model_name == 'ff_32' and transfer_label == 'NNS -> NNS-FF'\").groupby([\"to_freq_bin\", \"base_to\"]).correct.agg([\"count\", \"mean\"]).dropna().groupby(\"to_freq_bin\").apply(lambda xs: xs.sort_values(\"mean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=all_ff_results.query(\"base_model_name == 'w2v2_8'\").reset_index(),\n",
    "            x=\"to_freq_bin\", y=\"correct\", hue=\"model_name\",\n",
    "            col=\"transfer_label\", col_wrap=2, kind=\"point\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlled VBD analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vbd_results = all_results.query(\"experiment == 'regular' and inflection_from == 'VBD'\")\n",
    "all_vbd_results = pd.merge(all_vbd_results, mca.rename(columns={\"base\": \"base_from\", \"inflection\": \"inflection_from\", \"most_common_allomorph\": \"allomorph_from\"}),\n",
    "               on=[\"base_from\", \"inflection_from\"], how=\"left\")\n",
    "all_vbd_results = pd.merge(all_vbd_results, mca.rename(columns={\"base\": \"base_to\", \"inflection\": \"inflection_to\", \"most_common_allomorph\": \"allomorph_to\"}),\n",
    "               on=[\"base_to\", \"inflection_to\"], how=\"left\")\n",
    "all_vbd_results[[\"allomorph_from\", \"allomorph_to\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_vbd_allomorphs = all_vbd_results.allomorph_from.value_counts().head(3).index\n",
    "all_vbd_results = all_vbd_results[all_vbd_results.allomorph_from.isin(keep_vbd_allomorphs)\n",
    "                                  & all_vbd_results.allomorph_to.isin(keep_vbd_allomorphs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add frequency information\n",
    "\n",
    "all_vbd_results[\"inflected_from\"] = all_vbd_results.from_equiv_label.apply(lambda x: eval(x)[1])\n",
    "all_vbd_results[\"inflected_to\"] = all_vbd_results.to_equiv_label.apply(lambda x: eval(x)[1])\n",
    "\n",
    "all_vbd_results = pd.merge(all_vbd_results, word_freq_df[\"LogFreq\"].rename(\"from_base_freq\"),\n",
    "                           left_on=\"base_from\", right_index=True)\n",
    "all_vbd_results = pd.merge(all_vbd_results, word_freq_df[\"LogFreq\"].rename(\"to_base_freq\"),\n",
    "                           left_on=\"base_to\", right_index=True)\n",
    "all_vbd_results = pd.merge(all_vbd_results, word_freq_df[\"LogFreq\"].rename(\"from_inflected_freq\"),\n",
    "                            left_on=\"inflected_from\", right_index=True)\n",
    "all_vbd_results = pd.merge(all_vbd_results, word_freq_df[\"LogFreq\"].rename(\"to_inflected_freq\"),\n",
    "                            left_on=\"inflected_to\", right_index=True)\n",
    "\n",
    "all_vbd_results[\"from_freq\"] = all_vbd_results[[\"from_base_freq\", \"from_inflected_freq\"]].mean(axis=1)\n",
    "all_vbd_results[\"to_freq\"] = all_vbd_results[[\"to_base_freq\", \"to_inflected_freq\"]].mean(axis=1)\n",
    "\n",
    "_, vbd_frequency_bins = pd.qcut(pd.concat([all_vbd_results.to_freq, all_vbd_results.from_freq]), q=3, retbins=True)\n",
    "all_vbd_results[\"from_freq_bin\"] = pd.cut(all_vbd_results.from_freq, bins=vbd_frequency_bins, labels=[f\"Q{i}\" for i in range(1, 4)])\n",
    "all_vbd_results[\"to_freq_bin\"] = pd.cut(all_vbd_results.to_freq, bins=vbd_frequency_bins, labels=[f\"Q{i}\" for i in range(1, 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_vbd_run(rows):\n",
    "    rows[\"source_label\"] = rows.inflection_from + \" \" + rows.allomorph_from\n",
    "    rows[\"target_label\"] = rows.inflection_to + \" \" + rows.allomorph_to\n",
    "\n",
    "    rows[\"transfer_label\"] = rows.inflection_from + \" -> \" + rows.inflection_to\n",
    "    rows[\"phon_label\"] = rows.allomorph_from + \" -> \" + rows.allomorph_to\n",
    "\n",
    "    return rows\n",
    "\n",
    "summary_groupers = [\"inflection_from\", \"inflection_to\", \"allomorph_from\", \"allomorph_to\"]\n",
    "vbd_results_summary = all_vbd_results.groupby(run_groupers + summary_groupers) \\\n",
    "    .correct.agg([\"count\", \"mean\"]) \\\n",
    "    .reset_index(summary_groupers) \\\n",
    "    .groupby(run_groupers, group_keys=False) \\\n",
    "    .apply(summarize_vbd_run) \\\n",
    "    .reset_index()\n",
    "\n",
    "vbd_results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results = []\n",
    "for base_model_name, model_name, equivalence in plot_runs:\n",
    "    results_i = vbd_results_summary.query(\"base_model_name == @base_model_name and model_name == @model_name and equivalence == @equivalence\")\n",
    "    if len(results_i) > 0:\n",
    "        plot_results.append(results_i)\n",
    "num_plot_runs = len(plot_results)\n",
    "\n",
    "ncols = 2\n",
    "nrows = int(np.ceil(num_plot_runs / ncols))\n",
    "f, axs = plt.subplots(nrows, ncols, figsize=(ncols * 4, nrows * 4))\n",
    "\n",
    "for ax, results_i in zip(axs.flat, plot_results):\n",
    "    sns.heatmap(results_i.set_index([\"source_label\", \"target_label\"])[\"mean\"].unstack(),\n",
    "                vmin=0, vmax=1, ax=ax)\n",
    "    key_row = results_i.iloc[0]\n",
    "    ax.set_title(f\"{key_row.base_model_name} -> {key_row.model_name} ({key_row.equivalence})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focused plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_base_model, focus_model, focus_equivalence = main_plot_run\n",
    "foil_base_model, foil_model, foil_equivalence = \"w2v2_8\", \"id\", \"id\"\n",
    "\n",
    "vbd_focus = all_vbd_results.query(\"base_model_name == @focus_base_model and model_name == @focus_model and equivalence == @focus_equivalence\")\n",
    "vbd_foil = all_vbd_results.query(\"base_model_name == @foil_base_model and model_name == @foil_model and equivalence == @foil_equivalence\")\n",
    "vbd_focus[\"model_label\"] = \"Word\"\n",
    "vbd_foil[\"model_label\"] = \"Wav2Vec\"\n",
    "\n",
    "vbd_focus = pd.concat([vbd_focus, vbd_foil])\n",
    "\n",
    "allomorph_labels = {\"D\": \"d\", \"T\": \"t\", \"IH D\": \"ɪd\"}\n",
    "vbd_focus[\"allomorph_from\"] = vbd_focus.allomorph_from.map(allomorph_labels)\n",
    "vbd_focus[\"allomorph_to\"] = vbd_focus.allomorph_to.map(allomorph_labels)\n",
    "vbd_focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbd_results_summary = vbd_focus.groupby([\"model_label\", \"inflection_from\", \"inflection_to\",\n",
    "                                             \"allomorph_from\", \"allomorph_to\"]) \\\n",
    "    .correct.agg([\"count\", \"mean\"]) \\\n",
    "    .query(\"count >= 0\") \\\n",
    "    .reset_index()\n",
    "\n",
    "vbd_results_summary[\"source_label\"] = vbd_results_summary.inflection_from + \"\\n\" + vbd_results_summary.allomorph_from\n",
    "vbd_results_summary[\"target_label\"] = vbd_results_summary.inflection_to + \"\\n\" + vbd_results_summary.allomorph_to\n",
    "\n",
    "vbd_results_summary[\"transfer_label\"] = vbd_results_summary.inflection_from + \" -> \" + vbd_results_summary.inflection_to\n",
    "vbd_results_summary[\"phon_label\"] = vbd_results_summary.allomorph_from + \" \" + vbd_results_summary.allomorph_to\n",
    "\n",
    "# only retain cases where we have data in both transfer directions from source <-> target within inflection\n",
    "vbd_results_summary[\"complement_exists\"] = vbd_results_summary.apply(lambda row: len(vbd_results_summary.query(\"source_label == @row.target_label and target_label == @row.source_label\")), axis=1)\n",
    "vbd_results_summary = vbd_results_summary.query(\"complement_exists > 0\").drop(columns=[\"complement_exists\"])\n",
    "\n",
    "vbd_results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbd_focus_bar = vbd_focus.assign(source_label=lambda xs: xs.inflection_from + \" \" + xs.allomorph_from)\n",
    "order = vbd_focus_bar.groupby(\"source_label\").correct.mean().sort_values().index\n",
    "g = sns.catplot(data=vbd_focus_bar, x=\"allomorph_to\", hue=\"source_label\", y=\"correct\", col=\"model_label\", kind=\"bar\")\n",
    "g._legend.set_title(\"Train inflection\\nand allomorph\")\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_title(ax.get_title().split(\"=\")[1].strip())\n",
    "    ax.set_xlabel(\"Test inflection\")\n",
    "    ax.set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, ax = plt.subplots(1, 2, figsize=(7 * 2, 6))\n",
    "\n",
    "f, axs = plt.subplots(1, 3, figsize=(7 * 2, 6), gridspec_kw={'width_ratios': [1, 1, 0.04]})\n",
    "for i, (ax, (model_label, rows)) in enumerate(zip(axs, vbd_results_summary.groupby(\"model_label\"))):\n",
    "    cbar_ax = None\n",
    "    if i == 1:\n",
    "        cbar_ax = axs.flat[-1]\n",
    "\n",
    "    ax.set_title(model_label)\n",
    "    sns.heatmap(rows.set_index([\"source_label\", \"target_label\"]).sort_index()[\"mean\"].unstack(\"target_label\"),\n",
    "                vmin=main_plot_vmin, vmax=main_plot_vmax, annot=True, ax=ax,\n",
    "                cbar=i == 1, cbar_ax=cbar_ax)\n",
    "\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "    ax.set_ylabel(\"Train\")\n",
    "    ax.set_xlabel(\"Test\")\n",
    "\n",
    "f.tight_layout()\n",
    "f.savefig(f\"{output_dir}/vbd_allomorphs.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_inflections_v3 = [\"non\", \"agent\", \"comp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3_cross_instances = pd.read_parquet(\"outputs/analogy_v3/inputs/librispeech-train-clean-100/w2v2/all_cross_instances.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-compute metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prons = v3_cross_instances.groupby(\"base\").base_phones.value_counts().groupby(\"base\").head(1) \\\n",
    "    .reset_index().drop(columns=[\"count\"])\n",
    "\n",
    "from src.utils import syllabifier\n",
    "base_prons[\"num_syllables\"] = base_prons.base_phones.apply(\n",
    "    lambda phones: len(syllabifier.syllabify(syllabifier.English, phones)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_v3 = concat_csv_with_indices(\n",
    "        \"outputs/analogy_v3/runs/**/experiment_results.csv\",\n",
    "        [lambda p: p.parent.name, lambda p: p.parents[1].name,\n",
    "            lambda p: p.parents[2].name],\n",
    "        [\"equivalence\", \"model_name\", \"base_model_name\"]) \\\n",
    "    .droplevel(-1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_id_results_v3 = concat_csv_with_indices(\n",
    "        \"outputs/analogy_v3/runs_id/**/experiment_results.csv\",\n",
    "        [lambda p: p.parent.name],\n",
    "        [\"base_model_name\"]) \\\n",
    "    .droplevel(-1).reset_index()\n",
    "all_id_results_v3[\"model_name\"] = \"id\"\n",
    "all_id_results_v3[\"equivalence\"] = \"id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_v3 = pd.concat([all_results_v3, all_id_results_v3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_v3 = pd.merge(all_results_v3,\n",
    "         base_prons.rename(columns={\"base\": \"base_from\",\n",
    "                                    \"num_syllables\": \"base_from_num_syllables\"})\n",
    "                    .drop(columns=[\"base_phones\"]),\n",
    "         on=\"base_from\")\n",
    "\n",
    "all_results_v3 = pd.merge(all_results_v3,\n",
    "         base_prons.rename(columns={\"base\": \"base_to\",\n",
    "                                    \"num_syllables\": \"base_to_num_syllables\"})\n",
    "                    .drop(columns=[\"base_phones\"]),\n",
    "         on=\"base_to\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3_fixes = pd.read_csv(\"20250218 annot.csv\", index_col=0)\n",
    "v3_fixes[\"morph\"] = v3_fixes.morph.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_v3 = all_results_v3[all_results_v3.base_from.isin(v3_fixes.base) & all_results_v3.base_to.isin(v3_fixes.base)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_v3 = pd.merge(\n",
    "    all_results_v3, v3_fixes[[\"base\", \"morph\"]].rename(columns={\"base\": \"base_from\", \"morph\": \"morph_from\"}).fillna(\"non\"),\n",
    "    on=[\"base_from\"], how=\"inner\")\n",
    "all_results_v3 = pd.merge(\n",
    "    all_results_v3, v3_fixes[[\"base\", \"morph\"]].rename(columns={\"base\": \"base_to\", \"morph\": \"morph_to\"}).fillna(\"non\"),\n",
    "    on=[\"base_to\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_v3[\"inflection_from\"] = all_results_v3.morph_from\n",
    "all_results_v3[\"inflection_to\"] = all_results_v3.morph_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEV\n",
    "all_results_v3 = all_results_v3[((all_results_v3.base_from_num_syllables == 1) & (all_results_v3.base_to_num_syllables == 1))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_v3.loc[(all_results_v3.experiment == \"morph_related\") & (all_results_v3.group == \"(True,)\"), \"group_from\"] = \"morph\"\n",
    "all_results_v3.loc[(all_results_v3.experiment == \"morph_related\") & (all_results_v3.group == \"(True,)\"), \"group_to\"] = \"morph\"\n",
    "all_results_v3.loc[(all_results_v3.experiment == \"morph_related\") & (all_results_v3.group == \"(False,)\"), \"group_from\"] = \"non\"\n",
    "all_results_v3.loc[(all_results_v3.experiment == \"morph_related\") & (all_results_v3.group == \"(False,)\"), \"group_to\"] = \"non\"\n",
    "all_results_v3.loc[(all_results_v3.experiment == \"non_to_morph\"), \"group_from\"] = \"non\"\n",
    "all_results_v3.loc[(all_results_v3.experiment == \"non_to_morph\"), \"group_to\"] = \"morph\"\n",
    "all_results_v3.loc[(all_results_v3.experiment == \"morph_to_non\"), \"group_from\"] = \"morph\"\n",
    "all_results_v3.loc[(all_results_v3.experiment == \"morph_to_non\"), \"group_to\"] = \"non\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lw_v3 = all_results_v3\n",
    "plot_lw_v3 = plot_lw_v3.groupby(run_groupers + [\"inflection_from\", \"inflection_to\", \"base_to\"]).correct.mean().reset_index([\"inflection_from\", \"inflection_to\", \"base_to\"])\n",
    "# get just the relevant runs from plot_runs\n",
    "plot_lw_v3 = pd.concat([plot_lw_v3.loc[plot_run] for plot_run in plot_runs]).reset_index()\n",
    "plot_lw_v3[\"layer\"] = plot_lw_v3.base_model_name.str.extract(r\"_(\\d+)$\").astype(int)\n",
    "plot_lw_v3[\"label\"] = plot_lw_v3.inflection_from + \" -> \" + plot_lw_v3.inflection_to\n",
    "\n",
    "plot_lw_v3 = plot_lw_v3[plot_lw_v3.inflection_from.isin(plot_inflections_v3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=plot_lw_v3, x=\"layer\", y=\"correct\", row=\"model_name\", hue=\"label\",\n",
    "            kind=\"point\", errorbar=\"se\", height=3, aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_er_results = []\n",
    "\n",
    "for run, run_results in all_results_v3.groupby(run_groupers):\n",
    "    transfer_results = pd.Series(run_results.experiment[run_results.experiment.str.contains(\"_to_\")]).unique()\n",
    "    run_results = run_results.set_index(\"experiment\")\n",
    "\n",
    "    for expt in transfer_results:\n",
    "        inflection_from, inflection_to = re.findall(r\"(\\w+)_to_(\\w+)\", expt)[0]\n",
    "        expt_df = run_results.loc[expt].copy()\n",
    "\n",
    "        num_seen_words = min(len(expt_df.base_from.unique()), len(expt_df.base_to.unique()))\n",
    "        # DEV\n",
    "        # if num_seen_words < 10:\n",
    "        #     print(f\"Skipping {expt} due to only {num_seen_words} seen words\")\n",
    "        #     continue\n",
    "\n",
    "        # expt_df[\"inflection_from\"] = inflection_from\n",
    "        # expt_df[\"inflection_to\"] = inflection_to\n",
    "\n",
    "        all_er_results.append(expt_df)\n",
    "\n",
    "    all_er_results.append(run_results.loc[\"morph_related\"].copy())\n",
    "\n",
    "all_er_results = pd.concat(all_er_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_er_results[\"inflected_from\"] = all_er_results.from_equiv_label.apply(lambda x: eval(x)[1])\n",
    "all_er_results[\"inflected_to\"] = all_er_results.to_equiv_label.apply(lambda x: eval(x)[1])\n",
    "all_er_results[\"transfer_label\"] = all_er_results.inflection_from + \" -> \" + all_er_results.inflection_to\n",
    "\n",
    "all_er_results = pd.merge(all_er_results, word_freq_df.LogFreq.rename(\"from_base_freq\"),\n",
    "                            left_on=\"base_from\", right_index=True)\n",
    "all_er_results = pd.merge(all_er_results, word_freq_df.LogFreq.rename(\"from_inflected_freq\"),\n",
    "                            left_on=\"inflected_from\", right_index=True)\n",
    "all_er_results = pd.merge(all_er_results, word_freq_df.LogFreq.rename(\"to_base_freq\"),\n",
    "                              left_on=\"base_to\", right_index=True)\n",
    "all_er_results = pd.merge(all_er_results, word_freq_df.LogFreq.rename(\"to_inflected_freq\"),\n",
    "                            left_on=\"inflected_to\", right_index=True)\n",
    "\n",
    "all_er_results[\"from_freq\"] = all_er_results[[\"from_base_freq\", \"from_inflected_freq\"]].mean(axis=1)\n",
    "all_er_results[\"to_freq\"] = all_er_results[[\"to_base_freq\", \"to_inflected_freq\"]].mean(axis=1)\n",
    "\n",
    "_, er_frequency_bins = pd.qcut(pd.concat([all_er_results.to_freq, all_er_results.from_freq]), q=5, retbins=True)\n",
    "all_er_results[\"to_freq_bin\"] = pd.cut(all_er_results.to_freq, bins=frequency_bins, labels=[f\"Q{i}\" for i in range(1, 6)])\n",
    "all_er_results[\"from_freq_bin\"] = pd.cut(all_er_results.from_freq, bins=frequency_bins, labels=[f\"Q{i}\" for i in range(1, 6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEV\n",
    "focus_base_model, focus_model, focus_equivalence = \"w2v2_8\", \"id\", \"id\"\n",
    "# focus_base_model, focus_model, focus_equivalence = \"w2v2_8\", \"ff_32\", \"word_broad_10frames_fixedlen25\"\n",
    "\n",
    "focus_er_results = all_er_results.query(\"base_model_name == @focus_base_model and model_name == @focus_model and equivalence == @focus_equivalence\")\n",
    "\n",
    "focus_er_results_summary = focus_er_results.groupby([\"inflection_from\", \"inflection_to\"]) \\\n",
    "    [[\"correct\", \"gt_label_rank\", \"gt_distance\"]].mean().reset_index()\n",
    "\n",
    "g = sns.FacetGrid(focus_er_results_summary, sharex=False, sharey=False, height=4, aspect=1.25)\n",
    "\n",
    "def mapfn(data, **kwargs):\n",
    "    ax = plt.gca()\n",
    "    sns.heatmap(data.set_index([\"inflection_from\", \"inflection_to\"]).correct.unstack(\"inflection_to\"),\n",
    "                annot=True, ax=ax)\n",
    "\n",
    "g.map_dataframe(mapfn)\n",
    "\n",
    "for i, ax in enumerate(g.axes.flat):\n",
    "    ax.set_title(ax.get_title().replace(\"inflection_from = \", \"\"))\n",
    "    if i > 0:\n",
    "        ax.set_ylabel(\"\")\n",
    "    if i < len(g.axes.flat) - 1:\n",
    "        ax.collections[0].colorbar.remove()\n",
    "\n",
    "g.fig.tight_layout()\n",
    "g.fig.savefig(f\"{output_dir}/er_results.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_er_results.groupby([\"inflection_from\", \"inflection_to\"]).correct.agg([\"count\", \"mean\"]).sort_values(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_stats_df = pd.concat([\n",
    "    all_er_results[[\"inflection_from\", \"base_from\", \"from_freq\"]].reset_index(drop=True)\n",
    "        .rename(columns={\"inflection_from\": \"inflection\", \"base_from\": \"base\", \"from_freq\": \"freq\"}),\n",
    "    all_er_results[[\"inflection_to\", \"base_to\", \"to_freq\"]].reset_index(drop=True)\n",
    "        .rename(columns={\"inflection_to\": \"inflection\", \"base_to\": \"base\", \"to_freq\": \"freq\"})]) \\\n",
    ".drop_duplicates([\"inflection\", \"base\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.displot(data=freq_stats_df, x=\"freq\", row=\"inflection\", kind=\"hist\", bins=10, height=2, aspect=3, facet_kws={\"sharey\": False})\n",
    "\n",
    "# plot medians as vline\n",
    "for ax, row_name in zip(g.axes.flat, g.row_names):\n",
    "    ax.axvline(freq_stats_df.query(\"inflection == @row_name\").freq.median(), color=\"red\", linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "ttest_ind(freq_stats_df.query(\"inflection == 'comp'\").freq,\n",
    "          freq_stats_df.query(\"inflection == 'agent'\").freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=focus_er_results.reset_index(),\n",
    "            x=\"from_freq_bin\", y=\"correct\", hue=\"inflection_to\",\n",
    "            col=\"inflection_from\", col_wrap=2, kind=\"point\", errorbar=\"se\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=focus_er_results.reset_index(),\n",
    "            x=\"to_freq_bin\", y=\"correct\", hue=\"inflection_from\",\n",
    "            col=\"inflection_to\", col_wrap=2, kind=\"point\", errorbar=\"se\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_er_results.query(\"inflection_to == 'agent'\").groupby(\"base_to\").correct.mean().sort_values().head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_er_results.query(\"base_to == 'sin'\").predicted_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_er_results.query(\"inflection_to == 'comp'\").groupby(\"base_to\").correct.mean().sort_values().head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_er_results.query(\"base_to == 'sad'\").predicted_label.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
